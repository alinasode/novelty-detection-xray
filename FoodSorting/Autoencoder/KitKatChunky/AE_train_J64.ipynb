{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder: Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import axis, colorbar, imshow, show, figure, subplot\n",
    "from matplotlib import cm\n",
    "import matplotlib as mpl\n",
    "mpl.rc('axes', labelsize=18)\n",
    "mpl.rc('xtick', labelsize=16)\n",
    "mpl.rc('ytick', labelsize=16)\n",
    "%matplotlib inline\n",
    "\n",
    "import glob\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "import cv2\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "## ----- GPU ------------------------------------\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'    # use of GPU 0 (ERDA) (use before importing torch or tensorflow/keeas)\n",
    "## ----------------------------------------------\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.layers import Conv2D, Conv2DTranspose, Input, Flatten, Dense, Lambda, Reshape\n",
    "from keras.layers import BatchNormalization, ReLU, LeakyReLU\n",
    "from keras.models import Model\n",
    "from keras.losses import binary_crossentropy, mse\n",
    "from keras.activations import relu\n",
    "from keras.callbacks import Callback, ModelCheckpoint\n",
    "from keras import backend as K                         #contains calls for tensor manipulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n",
      "2.4.3\n"
     ]
    }
   ],
   "source": [
    "print (tf.__version__)\n",
    "print (keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NORMAL_TRAIN_AUG:       /home/jovyan/work/Speciale/FoodSorting/generated_dataset/KitKatChunky//DataAugmentation/NormalTrainAugmentations\n",
      "NORMAL_VALIDATION_AUG:  /home/jovyan/work/Speciale/FoodSorting/generated_dataset/KitKatChunky//DataAugmentation/NormalValidationAugmentations\n",
      "NORMAL_TEST_AUG:        /home/jovyan/work/Speciale/FoodSorting/generated_dataset/KitKatChunky//DataAugmentation/NormalTestAugmentations\n",
      "ANOMALY_AUG:            /home/jovyan/work/Speciale/FoodSorting/generated_dataset/KitKatChunky//DataAugmentation/AnomalyAugmentations\n"
     ]
    }
   ],
   "source": [
    "from utils_ae_kitkat_paths import *\n",
    "\n",
    "# Get work directions for augmentated data set:\n",
    "print (\"NORMAL_TRAIN_AUG:      \", NORMAL_TRAIN_AUG)\n",
    "print (\"NORMAL_VALIDATION_AUG: \", NORMAL_VAL_AUG)\n",
    "print (\"NORMAL_TEST_AUG:       \", NORMAL_TEST_AUG)\n",
    "print (\"ANOMALY_AUG:           \", ANOMALY_AUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Which Folder The Models Are Saved In: saved_models/latent64\n"
     ]
    }
   ],
   "source": [
    "# What latent dimension and filter size are we using?:\n",
    "latent_dim = 64\n",
    "filters    = 32\n",
    "\n",
    "# Get work direction for saving files:\n",
    "SAVE_FOLDER = f'saved_models/latent{latent_dim}'\n",
    "print (\"Which Folder The Models Are Saved In:\", SAVE_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] deleting existing files in folder...\n",
      "         done in 0.003 minutes\n"
     ]
    }
   ],
   "source": [
    "# Delete all existing files in folder where models are saved:\n",
    "print(\"[INFO] deleting existing files in folder...\")\n",
    "t0 = time()\n",
    "\n",
    "files = glob.glob(f'{SAVE_FOLDER}/*')\n",
    "for f in files:\n",
    "    os.remove(f)\n",
    "\n",
    "print (\"         done in %0.3f minutes\" % ((time() - t0)/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_VAE_AE import get_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Investigation of Ground Truths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of \"normal\" training images is:     1400\n",
      "Number of \"normal\" validation images is:   134\n",
      "Number of \"normal\" test images is:         266\n",
      "Number of \"anomaly\" images is:             600\n"
     ]
    }
   ],
   "source": [
    "# Glob the directories and get the lists of good and bad images\n",
    "good_train_fns = [f for f in os.listdir(NORMAL_TRAIN_AUG) if not f.startswith('.')]\n",
    "good_val_fns = [f for f in os.listdir(NORMAL_VAL_AUG) if not f.startswith('.')]\n",
    "good_test_fns = [f for f in os.listdir(NORMAL_TEST_AUG) if not f.startswith('.')]\n",
    "bad_fns = [f for f in os.listdir(ANOMALY_AUG) if not f.startswith('.')]\n",
    "\n",
    "print('Number of \"normal\" training images is:     {}'.format(len(good_train_fns)))\n",
    "print('Number of \"normal\" validation images is:   {}'.format(len(good_val_fns)))\n",
    "print('Number of \"normal\" test images is:         {}'.format(len(good_test_fns)))\n",
    "print('Number of \"anomaly\" images is:             {}'.format(len(bad_fns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Procentage of normal samples (ground truth):   75.00 %\n",
      "> Procentage of anomaly samples (ground truth):  25.00 %\n"
     ]
    }
   ],
   "source": [
    "# compute procentage of bad images vs good images:\n",
    "normal_fns = len(good_train_fns) + len(good_val_fns) + len(good_test_fns)\n",
    "anomaly_fns = len(bad_fns)\n",
    "print(f\"> Procentage of normal samples (ground truth):   {(normal_fns / (normal_fns + anomaly_fns)) * 100:.2f} %\")\n",
    "print(f\"> Procentage of anomaly samples (ground truth):  {(anomaly_fns / (normal_fns + anomaly_fns)) * 100:.2f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Normal Data\n",
    "\n",
    "Load normal samples in pre-splitted data sets: train, valdiation and test.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading normal training images...\n",
      "          done\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] loading normal training images...\")\n",
    "trainX = [cv2.imread(file, 0) for file in glob.glob(f\"{NORMAL_TRAIN_AUG}/*.png\")]  # read as grayscale\n",
    "trainX = np.array(trainX)\n",
    "print(\"          done\")\n",
    "\n",
    "# create a corresponding list of labels:\n",
    "trainY = get_labels(trainX, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading normal validation images...\n",
      "          done\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] loading normal validation images...\")\n",
    "x_normal_val = [cv2.imread(file, 0) for file in glob.glob(f\"{NORMAL_VAL_AUG}/*.png\")]  # read as grayscale\n",
    "x_normal_val = np.array(x_normal_val)\n",
    "print(\"          done\")\n",
    "\n",
    "# create a corresponding list of labels:\n",
    "y_normal_val = get_labels(x_normal_val, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading normal testing images...\n",
      "          done\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] loading normal testing images...\")\n",
    "x_normal_test = [cv2.imread(file, 0) for file in glob.glob(f\"{NORMAL_TEST_AUG}/*.png\")]  # read as grayscale\n",
    "x_normal_test = np.array(x_normal_test)\n",
    "print(\"          done\")\n",
    "\n",
    "# create a corresponding list of labels:\n",
    "y_normal_test = get_labels(x_normal_test, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Anomaly Data\n",
    "\n",
    "\n",
    "Load anomaly samples in its singular training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading anomaly images...\n",
      "          done\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] loading anomaly images...\")\n",
    "x_anomaly = [cv2.imread(file, 0) for file in glob.glob(f\"{ANOMALY_AUG}/*.png\")]  # read as grayscale\n",
    "x_anomaly = np.array(x_anomaly)\n",
    "print(\"          done\")\n",
    "\n",
    "# create a corresponding list of labels:\n",
    "y_anomaly = get_labels(x_anomaly, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine normal samples and anomaly samples and split them into training, validation and test sets\n",
    "\n",
    "`label 0` == Normal Samples\n",
    "\n",
    "`label 1` == Anomaly Samples\n",
    "\n",
    "Train and Validation sets only consists of `Normal Data`, aka. `label 0`.\n",
    "\n",
    "Testing sets consists of both  `Normal Data` (aka. `label 0`) and `Anomaly Data` (aka. `label 1`)\n",
    "\n",
    "________________\n",
    "\n",
    "Due to the very sparse original (preprossed) KitKat Chunky data, the validation set will be a mix of normal testing and validation data. The normal testing set will consists of all validation and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine pre-loaded validation and testing set into a new testing set:\n",
    "testvalX = np.vstack((x_normal_val, x_normal_test))\n",
    "testvalY = get_labels(testvalX, 0)\n",
    "\n",
    "# randomly split in order to make new validation set:\n",
    "NoUsageX, valX, NoUsageY, valY = train_test_split(testvalX, testvalY, test_size=0.25, random_state=4345672)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine pre-loaded validation and testing set into a new testing set:\n",
    "testNormalX = np.vstack((x_normal_val, x_normal_test))\n",
    "testNormalY = get_labels(testNormalX, 0)\n",
    "\n",
    "# anomaly class (only used for testing):\n",
    "testAnomalyX, testAnomalyY = x_anomaly, y_anomaly\n",
    "\n",
    "# Combine all testing data in one array:\n",
    "testAllX = np.vstack((testNormalX, testAnomalyX))\n",
    "testAllY = np.hstack((testNormalY, testAnomalyY))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Data Dimensions and Distributions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect data shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:\n",
      " > images: (1400, 128, 128)\n",
      " > labels: (1400,)\n",
      "\n",
      "Validation set:\n",
      " > images: (100, 128, 128)\n",
      " > labels: (100,)\n",
      "\n",
      "Test set:\n",
      " > images: (1000, 128, 128)\n",
      " > labels: (1000,)\n",
      "\t'Normal' test set:\n",
      "\t  > images: (400, 128, 128)\n",
      "\t  > labels: (400,)\n",
      "\t'Anomaly' test set:\n",
      "\t  > images: (600, 128, 128)\n",
      "\t  > labels: (600,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set:\")\n",
    "print(\" > images:\", trainX.shape)\n",
    "print(\" > labels:\", trainY.shape)\n",
    "print (\"\")\n",
    "\n",
    "print(\"Validation set:\")\n",
    "print(\" > images:\", valX.shape)\n",
    "print(\" > labels:\", valY.shape)\n",
    "print (\"\")\n",
    "\n",
    "print(\"Test set:\")\n",
    "print(\" > images:\", testAllX.shape)\n",
    "print(\" > labels:\", testAllY.shape)\n",
    "print(\"\\t'Normal' test set:\")\n",
    "print(\"\\t  > images:\", testNormalX.shape)\n",
    "print(\"\\t  > labels:\", testNormalY.shape)\n",
    "print(\"\\t'Anomaly' test set:\")\n",
    "print(\"\\t  > images:\", testAnomalyX.shape)\n",
    "print(\"\\t  > labels:\", testAnomalyY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procentage of training samples: 73.68 %\n",
      "Procentage of validation samples: 5.26 %\n",
      "Procentage of (only normal) testing samples: 21.05 %\n"
     ]
    }
   ],
   "source": [
    "# compute procentage between training, validation and test data sets (only normal data):\n",
    "print(f\"Procentage of training samples: {(len(trainX) / (len(trainX) + len(valX) + len(testNormalX))) * 100:.2f} %\")\n",
    "print(f\"Procentage of validation samples: {(len(valX) / (len(trainX) + len(valX) + len(testNormalX))) * 100:.2f} %\")\n",
    "print(f\"Procentage of (only normal) testing samples: {(len(testNormalX) / (len(trainX) + len(valX) + len(testNormalX))) * 100:.2f} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procentage of training samples: 56.00 %\n",
      "Procentage of validation samples: 4.00 %\n",
      "Procentage of (total) testing samples: 40.00 %\n"
     ]
    }
   ],
   "source": [
    "# compute procentage between training, validation and test data sets:\n",
    "print(f\"Procentage of training samples: {(len(trainX) / (len(trainX) + len(valX) + len(testAllX))) * 100:.2f} %\")\n",
    "print(f\"Procentage of validation samples: {(len(valX) / (len(trainX) + len(valX) + len(testAllX))) * 100:.2f} %\")\n",
    "print(f\"Procentage of (total) testing samples: {(len(testAllX) / (len(trainX) + len(valX) + len(testAllX))) * 100:.2f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for (un)balanced data\n",
    "\n",
    "In an ideal world the data should be balanced. However in the real world we expect there being around ~$99 \\%$ good samples and only ~$1 \\%$ bad samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels = ['0: normal', '1: anomaly']\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(16,4))\n",
    "ax[0].hist(trainY, align=\"left\"); ax[0].set_title('train', fontsize=18); ax[0].set_xticks([0,1]); ax[0].set_xticklabels(labels); ax[0].tick_params(axis='both', which='major', labelsize=16); ax[0].set_xlim(-0.5, 1.5);\n",
    "ax[1].hist(valY, align=\"left\"); ax[1].set_title('validation', fontsize=18); ax[1].set_xticks([0,1]); ax[1].set_xticklabels(labels); ax[1].tick_params(axis='both', which='major', labelsize=16); ax[1].set_xlim(-0.5, 1.5);\n",
    "ax[2].hist(testAllY, align=\"left\"); ax[2].set_title('test', fontsize=18); ax[2].set_xticks([0,1]); ax[2].set_xticklabels(labels); ax[2].tick_params(axis='both', which='major', labelsize=16); ax[2].set_xlim(-0.5, 1.5);\n",
    "\n",
    "ax[0].set_ylabel(\"Number of images\", fontsize=18)\n",
    "ax[1].set_xlabel(\"Class Labels\", fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procentage of normal test samples: 40.00 %\n",
      "Procentage of total anomaly test samples: 60.00 %\n"
     ]
    }
   ],
   "source": [
    "# compute procentage of normal test images vs total anomaly test images:\n",
    "print(f\"Procentage of normal test samples: {(len(testNormalX) / (len(testNormalX) + len(testAnomalyX))) * 100:.2f} %\")\n",
    "print(f\"Procentage of total anomaly test samples: {(len(testAnomalyX) / (len(testNormalX) + len(testAnomalyX))) * 100:.2f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling\n",
    "\n",
    "Only normalization has been used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data configuration\n",
    "img_width, img_height = trainX.shape[1], trainX.shape[2]      # input image dimensions\n",
    "num_channels = 1                                              # gray-channel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization\n",
    "\n",
    "Neural Networks learns faster with normalized data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize to be in  the [0, 1] range:\n",
    "trainX = trainX.astype('float32') / 255.\n",
    "valX = valX.astype('float32') / 255.\n",
    "testAllX = testAllX.astype('float32') / 255.\n",
    "\n",
    "# reshape data to keras inputs --> (n_samples, img_rows, img_cols, n_channels):\n",
    "trainX = trainX.reshape(trainX.shape[0], img_height, img_width, num_channels)\n",
    "valX = valX.reshape(valX.shape[0], img_height, img_width, num_channels)\n",
    "testAllX = testAllX.reshape(testAllX.shape[0], img_height, img_width, num_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the AE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture and Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import architecture of VAE model and its hyperparameters:\n",
    "from J64_AE_model import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Total AE Model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"ae\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder_input (InputLayer)   [(None, 128, 128, 1)]     0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 64, 64, 32)        160       \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 64, 64, 32)        128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 32)        4128      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 16, 16, 32)        4128      \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 16, 16, 32)        128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 8, 8, 32)          4128      \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 8, 8, 32)          128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 4, 4, 32)          4128      \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 4, 4, 32)          128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 4, 4, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "latent (Dense)               (None, 64)                32832     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               33280     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "re_lu (ReLU)                 (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 4, 4, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose (Conv2DTran (None, 8, 8, 32)          4128      \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 8, 8, 32)          128       \n",
      "_________________________________________________________________\n",
      "re_lu_1 (ReLU)               (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 16, 16, 32)        4128      \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 16, 16, 32)        128       \n",
      "_________________________________________________________________\n",
      "re_lu_2 (ReLU)               (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTr (None, 32, 32, 32)        4128      \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "re_lu_3 (ReLU)               (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTr (None, 64, 64, 32)        4128      \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 64, 64, 32)        128       \n",
      "_________________________________________________________________\n",
      "re_lu_4 (ReLU)               (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_4 (Conv2DTr (None, 128, 128, 32)      4128      \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 128, 128, 32)      128       \n",
      "_________________________________________________________________\n",
      "re_lu_5 (ReLU)               (None, 128, 128, 32)      0         \n",
      "_________________________________________________________________\n",
      "decoder_output (Conv2DTransp (None, 128, 128, 1)       129       \n",
      "=================================================================\n",
      "Total params: 106,881\n",
      "Trainable params: 105,217\n",
      "Non-trainable params: 1,664\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "ae.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile and train model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Hyperparameters \"\"\"\n",
    "batch_size  = 256\n",
    "epochs      = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Checkpoint callback options: save model at 1'st epoch (inital model)\n",
    "\n",
    "https://stackoverflow.com/questions/54323960/save-keras-model-at-specific-epochs\n",
    "\"\"\"\n",
    "\n",
    "class CustomSaver(keras.callbacks.Callback):\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        if epoch == 1:\n",
    "            self.model.save_weights(f\"{SAVE_FOLDER}/cp-0001.h5\")\n",
    "            \n",
    "# create and use callback:\n",
    "initial_checkpoint = CustomSaver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Checkpoint callback options: save model at every k'te epochs\n",
    "\"\"\"\n",
    "# Include the epoch in the file name (uses `str.format`):\n",
    "checkpoint_path = \"saved_models/latent64/cp-{epoch:04d}.h5\"\n",
    "\n",
    "# Create a callback that saves the model's weights every k'te epochs:\n",
    "checkpoint = ModelCheckpoint(filepath = checkpoint_path,\n",
    "                             monitor='loss',           # name of the metrics to monitor\n",
    "                             verbose=1, \n",
    "                             #save_best_only=False,     # if False, the best model will not be overridden.\n",
    "                             save_weights_only=True,   # if True, only the weights of the models will be saved.\n",
    "                                                        # if False, the whole models will be saved.\n",
    "                             #mode='auto', \n",
    "                             period=200                  # save after every k'te epochs\n",
    "                            )\n",
    "\n",
    "# Save the weights using the `checkpoint_path` format\n",
    "ae.save_weights(checkpoint_path.format(epoch=0))          # save for zero epoch (inital)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Checkpoint callback options: Early stopping\n",
    "https://www.tensorflow.org/guide/keras/custom_callback\n",
    "\"\"\"\n",
    "\n",
    "class EarlyStoppingAtMinLoss(keras.callbacks.Callback):\n",
    "    \"\"\"Stop training when the loss is at its min, i.e. the loss stops decreasing.\n",
    "\n",
    "  Arguments:\n",
    "      patience: Number of epochs to wait after min has been hit. After this\n",
    "      number of no improvement, training stops.\n",
    "  \"\"\"\n",
    "\n",
    "    def __init__(self, patience=50):\n",
    "        super(EarlyStoppingAtMinLoss, self).__init__()\n",
    "        self.patience = patience\n",
    "        # best_weights to store the weights at which the minimum loss occurs.\n",
    "        self.best_weights = None\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        # The number of epoch it has waited when loss is no longer minimum.\n",
    "        self.wait = 0\n",
    "        # The epoch the training stops at.\n",
    "        self.stopped_epoch = 0\n",
    "        # Initialize the best as infinity.\n",
    "        self.best = np.Inf\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        current = logs.get(\"loss\")\n",
    "        if np.less(current, self.best):\n",
    "            self.best = current\n",
    "            self.wait = 0\n",
    "            # Record the best weights if current results is better (less).\n",
    "            self.best_weights = self.model.get_weights()\n",
    "        else:\n",
    "            self.wait += 1\n",
    "            if self.wait >= self.patience:\n",
    "                self.stopped_epoch = epoch\n",
    "                self.model.stop_training = True\n",
    "                print(\"Restoring model weights from the end of the best epoch.\")\n",
    "                self.model.set_weights(self.best_weights)\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        if self.stopped_epoch > 0:\n",
    "            print(\"Epoch %05d: early stopping\" % (self.stopped_epoch + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Create and Compile Model\"\"\"\n",
    "# import architecture of VAE model and its hyperparameters. learning rate choosen from graph above.\n",
    "ae = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "6/6 [==============================] - 1s 166ms/step - loss: 0.2914 - val_loss: 0.2154\n",
      "Epoch 2/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.2016 - val_loss: 0.2097\n",
      "Epoch 3/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.1440 - val_loss: 0.2023\n",
      "Epoch 4/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0976 - val_loss: 0.1935\n",
      "Epoch 5/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0684 - val_loss: 0.1837\n",
      "Epoch 6/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0531 - val_loss: 0.1730\n",
      "Epoch 7/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0442 - val_loss: 0.1623\n",
      "Epoch 8/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0388 - val_loss: 0.1519\n",
      "Epoch 9/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0347 - val_loss: 0.1416\n",
      "Epoch 10/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0313 - val_loss: 0.1315\n",
      "Epoch 11/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0282 - val_loss: 0.1217\n",
      "Epoch 12/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0255 - val_loss: 0.1125\n",
      "Epoch 13/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0230 - val_loss: 0.1039\n",
      "Epoch 14/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0206 - val_loss: 0.0959\n",
      "Epoch 15/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0186 - val_loss: 0.0886\n",
      "Epoch 16/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0169 - val_loss: 0.0823\n",
      "Epoch 17/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0153 - val_loss: 0.0768\n",
      "Epoch 18/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0136 - val_loss: 0.0720\n",
      "Epoch 19/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0124 - val_loss: 0.0677\n",
      "Epoch 20/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0115 - val_loss: 0.0642\n",
      "Epoch 21/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0110 - val_loss: 0.0612\n",
      "Epoch 22/1000\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 0.0099 - val_loss: 0.0586\n",
      "Epoch 23/1000\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 0.0091 - val_loss: 0.0566\n",
      "Epoch 24/1000\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 0.0087 - val_loss: 0.0549\n",
      "Epoch 25/1000\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 0.0084 - val_loss: 0.0536\n",
      "Epoch 26/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0078 - val_loss: 0.0524\n",
      "Epoch 27/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0071 - val_loss: 0.0515\n",
      "Epoch 28/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0068 - val_loss: 0.0507\n",
      "Epoch 29/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0066 - val_loss: 0.0502\n",
      "Epoch 30/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0064 - val_loss: 0.0498\n",
      "Epoch 31/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0060 - val_loss: 0.0495\n",
      "Epoch 32/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0057 - val_loss: 0.0493\n",
      "Epoch 33/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0055 - val_loss: 0.0490\n",
      "Epoch 34/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0053 - val_loss: 0.0490\n",
      "Epoch 35/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0051 - val_loss: 0.0487\n",
      "Epoch 36/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0051 - val_loss: 0.0486\n",
      "Epoch 37/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0049 - val_loss: 0.0485\n",
      "Epoch 38/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0049 - val_loss: 0.0484\n",
      "Epoch 39/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0046 - val_loss: 0.0481\n",
      "Epoch 40/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0046 - val_loss: 0.0477\n",
      "Epoch 41/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0043 - val_loss: 0.0476\n",
      "Epoch 42/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0044 - val_loss: 0.0471\n",
      "Epoch 43/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0042 - val_loss: 0.0468\n",
      "Epoch 44/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0041 - val_loss: 0.0464\n",
      "Epoch 45/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0040 - val_loss: 0.0461\n",
      "Epoch 46/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0038 - val_loss: 0.0456\n",
      "Epoch 47/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0037 - val_loss: 0.0443\n",
      "Epoch 48/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0036 - val_loss: 0.0443\n",
      "Epoch 49/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0035 - val_loss: 0.0429\n",
      "Epoch 50/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0035 - val_loss: 0.0433\n",
      "Epoch 51/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0036 - val_loss: 0.0410\n",
      "Epoch 52/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0033 - val_loss: 0.0413\n",
      "Epoch 53/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0037 - val_loss: 0.0393\n",
      "Epoch 54/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0036 - val_loss: 0.0394\n",
      "Epoch 55/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0031 - val_loss: 0.0372\n",
      "Epoch 56/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0031 - val_loss: 0.0368\n",
      "Epoch 57/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0031 - val_loss: 0.0350\n",
      "Epoch 58/1000\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 0.0029 - val_loss: 0.0344\n",
      "Epoch 59/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0030 - val_loss: 0.0333\n",
      "Epoch 60/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0028 - val_loss: 0.0319\n",
      "Epoch 61/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0029 - val_loss: 0.0310\n",
      "Epoch 62/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0029 - val_loss: 0.0279\n",
      "Epoch 63/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0028 - val_loss: 0.0277\n",
      "Epoch 64/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0030 - val_loss: 0.0262\n",
      "Epoch 65/1000\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 0.0027 - val_loss: 0.0244\n",
      "Epoch 66/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0027 - val_loss: 0.0231\n",
      "Epoch 67/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0027 - val_loss: 0.0222\n",
      "Epoch 68/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0025 - val_loss: 0.0211\n",
      "Epoch 69/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0026 - val_loss: 0.0190\n",
      "Epoch 70/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0024 - val_loss: 0.0194\n",
      "Epoch 71/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0025 - val_loss: 0.0189\n",
      "Epoch 72/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0025 - val_loss: 0.0172\n",
      "Epoch 73/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0026 - val_loss: 0.0183\n",
      "Epoch 74/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0025 - val_loss: 0.0164\n",
      "Epoch 75/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0024 - val_loss: 0.0139\n",
      "Epoch 76/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0024 - val_loss: 0.0125\n",
      "Epoch 77/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0025 - val_loss: 0.0108\n",
      "Epoch 78/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0023 - val_loss: 0.0101\n",
      "Epoch 79/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0023 - val_loss: 0.0096\n",
      "Epoch 80/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0022 - val_loss: 0.0088\n",
      "Epoch 81/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0022 - val_loss: 0.0109\n",
      "Epoch 82/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0020 - val_loss: 0.0080\n",
      "Epoch 83/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0022 - val_loss: 0.0080\n",
      "Epoch 84/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0021 - val_loss: 0.0066\n",
      "Epoch 85/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0020 - val_loss: 0.0066\n",
      "Epoch 86/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0021 - val_loss: 0.0074\n",
      "Epoch 87/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0021 - val_loss: 0.0052\n",
      "Epoch 88/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0020 - val_loss: 0.0051\n",
      "Epoch 89/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0020 - val_loss: 0.0041\n",
      "Epoch 90/1000\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.0019 - val_loss: 0.0034\n",
      "Epoch 91/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0020 - val_loss: 0.0034\n",
      "Epoch 92/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0020 - val_loss: 0.0031\n",
      "Epoch 93/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0018 - val_loss: 0.0032\n",
      "Epoch 94/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0019 - val_loss: 0.0030\n",
      "Epoch 95/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0021 - val_loss: 0.0033\n",
      "Epoch 96/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0019 - val_loss: 0.0032\n",
      "Epoch 97/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0020 - val_loss: 0.0033\n",
      "Epoch 98/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0018 - val_loss: 0.0032\n",
      "Epoch 99/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0019 - val_loss: 0.0031\n",
      "Epoch 100/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0018 - val_loss: 0.0033\n",
      "Epoch 101/1000\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 0.0018 - val_loss: 0.0032\n",
      "Epoch 102/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0017 - val_loss: 0.0029\n",
      "Epoch 103/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0017 - val_loss: 0.0027\n",
      "Epoch 104/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0017 - val_loss: 0.0027\n",
      "Epoch 105/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0018 - val_loss: 0.0028\n",
      "Epoch 106/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0017 - val_loss: 0.0026\n",
      "Epoch 107/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0017 - val_loss: 0.0025\n",
      "Epoch 108/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0017 - val_loss: 0.0026\n",
      "Epoch 109/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0016 - val_loss: 0.0024\n",
      "Epoch 110/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0017 - val_loss: 0.0027\n",
      "Epoch 111/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0016 - val_loss: 0.0027\n",
      "Epoch 112/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0017 - val_loss: 0.0028\n",
      "Epoch 113/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0016 - val_loss: 0.0028\n",
      "Epoch 114/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0017 - val_loss: 0.0026\n",
      "Epoch 115/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0016 - val_loss: 0.0025\n",
      "Epoch 116/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0017 - val_loss: 0.0022\n",
      "Epoch 117/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0018 - val_loss: 0.0025\n",
      "Epoch 118/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0016 - val_loss: 0.0025\n",
      "Epoch 119/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0016 - val_loss: 0.0020\n",
      "Epoch 120/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0016 - val_loss: 0.0022\n",
      "Epoch 121/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0017 - val_loss: 0.0025\n",
      "Epoch 122/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0016 - val_loss: 0.0025\n",
      "Epoch 123/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0016 - val_loss: 0.0028\n",
      "Epoch 124/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0016 - val_loss: 0.0027\n",
      "Epoch 125/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0014 - val_loss: 0.0025\n",
      "Epoch 126/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0014 - val_loss: 0.0026\n",
      "Epoch 127/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0017 - val_loss: 0.0025\n",
      "Epoch 128/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0015 - val_loss: 0.0025\n",
      "Epoch 129/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0016 - val_loss: 0.0025\n",
      "Epoch 130/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0016 - val_loss: 0.0027\n",
      "Epoch 131/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0016 - val_loss: 0.0027\n",
      "Epoch 132/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0016 - val_loss: 0.0029\n",
      "Epoch 133/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0016 - val_loss: 0.0027\n",
      "Epoch 134/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0014 - val_loss: 0.0025\n",
      "Epoch 135/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0015 - val_loss: 0.0027\n",
      "Epoch 136/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0015 - val_loss: 0.0026\n",
      "Epoch 137/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0014 - val_loss: 0.0021\n",
      "Epoch 138/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0016 - val_loss: 0.0022\n",
      "Epoch 139/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0015 - val_loss: 0.0022\n",
      "Epoch 140/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0015 - val_loss: 0.0031\n",
      "Epoch 141/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0014 - val_loss: 0.0036\n",
      "Epoch 142/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0013 - val_loss: 0.0031\n",
      "Epoch 143/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0014 - val_loss: 0.0032\n",
      "Epoch 144/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0015 - val_loss: 0.0032\n",
      "Epoch 145/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0013 - val_loss: 0.0026\n",
      "Epoch 146/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0014 - val_loss: 0.0023\n",
      "Epoch 147/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0013 - val_loss: 0.0025\n",
      "Epoch 148/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0014 - val_loss: 0.0023\n",
      "Epoch 149/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0013 - val_loss: 0.0022\n",
      "Epoch 150/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0013 - val_loss: 0.0019\n",
      "Epoch 151/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0012 - val_loss: 0.0021\n",
      "Epoch 152/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0014 - val_loss: 0.0021\n",
      "Epoch 153/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0013 - val_loss: 0.0020\n",
      "Epoch 154/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0013 - val_loss: 0.0018\n",
      "Epoch 155/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0014 - val_loss: 0.0018\n",
      "Epoch 156/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0014 - val_loss: 0.0021\n",
      "Epoch 157/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0014 - val_loss: 0.0016\n",
      "Epoch 158/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0014 - val_loss: 0.0017\n",
      "Epoch 159/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0013 - val_loss: 0.0020\n",
      "Epoch 160/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0013 - val_loss: 0.0019\n",
      "Epoch 161/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0014 - val_loss: 0.0017\n",
      "Epoch 162/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0012 - val_loss: 0.0017\n",
      "Epoch 163/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0015 - val_loss: 0.0018\n",
      "Epoch 164/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0014 - val_loss: 0.0018\n",
      "Epoch 165/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0014 - val_loss: 0.0018\n",
      "Epoch 166/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0013 - val_loss: 0.0019\n",
      "Epoch 167/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0013 - val_loss: 0.0017\n",
      "Epoch 168/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0013 - val_loss: 0.0015\n",
      "Epoch 169/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 170/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0013 - val_loss: 0.0020\n",
      "Epoch 171/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0012 - val_loss: 0.0015\n",
      "Epoch 172/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0011 - val_loss: 0.0014\n",
      "Epoch 173/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0012 - val_loss: 0.0016\n",
      "Epoch 174/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0012 - val_loss: 0.0015\n",
      "Epoch 175/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0012 - val_loss: 0.0015\n",
      "Epoch 176/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0012 - val_loss: 0.0016\n",
      "Epoch 177/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0013 - val_loss: 0.0016\n",
      "Epoch 178/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0013 - val_loss: 0.0017\n",
      "Epoch 179/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0012 - val_loss: 0.0016\n",
      "Epoch 180/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0013 - val_loss: 0.0019\n",
      "Epoch 181/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0013 - val_loss: 0.0015\n",
      "Epoch 182/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0013 - val_loss: 0.0019\n",
      "Epoch 183/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0012 - val_loss: 0.0016\n",
      "Epoch 184/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0012 - val_loss: 0.0017\n",
      "Epoch 185/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0013 - val_loss: 0.0016\n",
      "Epoch 186/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0012 - val_loss: 0.0018\n",
      "Epoch 187/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0012 - val_loss: 0.0018\n",
      "Epoch 188/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0012 - val_loss: 0.0016\n",
      "Epoch 189/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0012 - val_loss: 0.0019\n",
      "Epoch 190/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0013 - val_loss: 0.0020\n",
      "Epoch 191/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0012 - val_loss: 0.0017\n",
      "Epoch 192/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0013 - val_loss: 0.0017\n",
      "Epoch 193/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0012 - val_loss: 0.0017\n",
      "Epoch 194/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0012 - val_loss: 0.0016\n",
      "Epoch 195/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0011 - val_loss: 0.0014\n",
      "Epoch 196/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0011 - val_loss: 0.0015\n",
      "Epoch 197/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0011 - val_loss: 0.0015\n",
      "Epoch 198/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0011 - val_loss: 0.0016\n",
      "Epoch 199/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0012 - val_loss: 0.0014\n",
      "Epoch 200/1000\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.0011\n",
      "Epoch 00200: saving model to saved_models/latent64/cp-0200.h5\n",
      "6/6 [==============================] - 1s 203ms/step - loss: 0.0012 - val_loss: 0.0017\n",
      "Epoch 201/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0011 - val_loss: 0.0017\n",
      "Epoch 202/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0012 - val_loss: 0.0014\n",
      "Epoch 203/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0011 - val_loss: 0.0016\n",
      "Epoch 204/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0012 - val_loss: 0.0017\n",
      "Epoch 205/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0012 - val_loss: 0.0016\n",
      "Epoch 206/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0011 - val_loss: 0.0013\n",
      "Epoch 207/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0012 - val_loss: 0.0014\n",
      "Epoch 208/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0011 - val_loss: 0.0014\n",
      "Epoch 209/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0011 - val_loss: 0.0015\n",
      "Epoch 210/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 211/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0011 - val_loss: 0.0015\n",
      "Epoch 212/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0012 - val_loss: 0.0015\n",
      "Epoch 213/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0012 - val_loss: 0.0016\n",
      "Epoch 214/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0011 - val_loss: 0.0016\n",
      "Epoch 215/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0011 - val_loss: 0.0013\n",
      "Epoch 216/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0011 - val_loss: 0.0016\n",
      "Epoch 217/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0011 - val_loss: 0.0016\n",
      "Epoch 218/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0011 - val_loss: 0.0013\n",
      "Epoch 219/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0010 - val_loss: 0.0013\n",
      "Epoch 220/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0011 - val_loss: 0.0014\n",
      "Epoch 221/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0012 - val_loss: 0.0013\n",
      "Epoch 222/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0011 - val_loss: 0.0013\n",
      "Epoch 223/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0010 - val_loss: 0.0012\n",
      "Epoch 224/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0010 - val_loss: 0.0013\n",
      "Epoch 225/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0011 - val_loss: 0.0015\n",
      "Epoch 226/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0011 - val_loss: 0.0013\n",
      "Epoch 227/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0011 - val_loss: 0.0017\n",
      "Epoch 228/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0010 - val_loss: 0.0014\n",
      "Epoch 229/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0011 - val_loss: 0.0013\n",
      "Epoch 230/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0011 - val_loss: 0.0016\n",
      "Epoch 231/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0010 - val_loss: 0.0016\n",
      "Epoch 232/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0011 - val_loss: 0.0016\n",
      "Epoch 233/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0010 - val_loss: 0.0014\n",
      "Epoch 234/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 9.6425e-04 - val_loss: 0.0016\n",
      "Epoch 235/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0011 - val_loss: 0.0015\n",
      "Epoch 236/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0011 - val_loss: 0.0013\n",
      "Epoch 237/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0011 - val_loss: 0.0015\n",
      "Epoch 238/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0011 - val_loss: 0.0017\n",
      "Epoch 239/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0011 - val_loss: 0.0014\n",
      "Epoch 240/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0011 - val_loss: 0.0015\n",
      "Epoch 241/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0012 - val_loss: 0.0016\n",
      "Epoch 242/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0011 - val_loss: 0.0016\n",
      "Epoch 243/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0011 - val_loss: 0.0015\n",
      "Epoch 244/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0010 - val_loss: 0.0015\n",
      "Epoch 245/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0010 - val_loss: 0.0013\n",
      "Epoch 246/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0011 - val_loss: 0.0014\n",
      "Epoch 247/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0010 - val_loss: 0.0014\n",
      "Epoch 248/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 9.8397e-04 - val_loss: 0.0015\n",
      "Epoch 249/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0011 - val_loss: 0.0017\n",
      "Epoch 250/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0011 - val_loss: 0.0015\n",
      "Epoch 251/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0011 - val_loss: 0.0016\n",
      "Epoch 252/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0010 - val_loss: 0.0013\n",
      "Epoch 253/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0010 - val_loss: 0.0014\n",
      "Epoch 254/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 9.8481e-04 - val_loss: 0.0014\n",
      "Epoch 255/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0010 - val_loss: 0.0016\n",
      "Epoch 256/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 9.2656e-04 - val_loss: 0.0014\n",
      "Epoch 257/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 9.4552e-04 - val_loss: 0.0015\n",
      "Epoch 258/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0011 - val_loss: 0.0015\n",
      "Epoch 259/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0010 - val_loss: 0.0013\n",
      "Epoch 260/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0011 - val_loss: 0.0014\n",
      "Epoch 261/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 9.4222e-04 - val_loss: 0.0015\n",
      "Epoch 262/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 9.5073e-04 - val_loss: 0.0015\n",
      "Epoch 263/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 9.3113e-04 - val_loss: 0.0015\n",
      "Epoch 264/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0010 - val_loss: 0.0015\n",
      "Epoch 265/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 9.9578e-04 - val_loss: 0.0015\n",
      "Epoch 266/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0010 - val_loss: 0.0016\n",
      "Epoch 267/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 9.4016e-04 - val_loss: 0.0016\n",
      "Epoch 268/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 9.9546e-04 - val_loss: 0.0015\n",
      "Epoch 269/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 9.4874e-04 - val_loss: 0.0015\n",
      "Epoch 270/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0011 - val_loss: 0.0013\n",
      "Epoch 271/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0010 - val_loss: 0.0013\n",
      "Epoch 272/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 9.1715e-04 - val_loss: 0.0013\n",
      "Epoch 273/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 9.3728e-04 - val_loss: 0.0011\n",
      "Epoch 274/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 9.1028e-04 - val_loss: 0.0012\n",
      "Epoch 275/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 9.9350e-04 - val_loss: 0.0011\n",
      "Epoch 276/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 8.8403e-04 - val_loss: 0.0011\n",
      "Epoch 277/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 8.5055e-04 - val_loss: 0.0011\n",
      "Epoch 278/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 9.8845e-04 - val_loss: 0.0012\n",
      "Epoch 279/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 9.2761e-04 - val_loss: 0.0012\n",
      "Epoch 280/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0010 - val_loss: 0.0013\n",
      "Epoch 281/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 9.5121e-04 - val_loss: 0.0011\n",
      "Epoch 282/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 9.3695e-04 - val_loss: 0.0013\n",
      "Epoch 283/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 9.4564e-04 - val_loss: 0.0013\n",
      "Epoch 284/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.8403e-04 - val_loss: 0.0014\n",
      "Epoch 285/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0010 - val_loss: 0.0012\n",
      "Epoch 286/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 9.6555e-04 - val_loss: 0.0013\n",
      "Epoch 287/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0010 - val_loss: 0.0013\n",
      "Epoch 288/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 9.1953e-04 - val_loss: 0.0013\n",
      "Epoch 289/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 9.6122e-04 - val_loss: 0.0012\n",
      "Epoch 290/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 9.3276e-04 - val_loss: 0.0012\n",
      "Epoch 291/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 9.7336e-04 - val_loss: 0.0014\n",
      "Epoch 292/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 293/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 9.9441e-04 - val_loss: 0.0013\n",
      "Epoch 294/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 9.5856e-04 - val_loss: 0.0011\n",
      "Epoch 295/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0010 - val_loss: 0.0011\n",
      "Epoch 296/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 9.3862e-04 - val_loss: 0.0011\n",
      "Epoch 297/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.7786e-04 - val_loss: 0.0012\n",
      "Epoch 298/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 9.2493e-04 - val_loss: 0.0012\n",
      "Epoch 299/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 8.9862e-04 - val_loss: 0.0013\n",
      "Epoch 300/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 9.1647e-04 - val_loss: 0.0012\n",
      "Epoch 301/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.8824e-04 - val_loss: 0.0013\n",
      "Epoch 302/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.8613e-04 - val_loss: 0.0013\n",
      "Epoch 303/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 9.3442e-04 - val_loss: 0.0013\n",
      "Epoch 304/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0011 - val_loss: 0.0013\n",
      "Epoch 305/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0010 - val_loss: 0.0014\n",
      "Epoch 306/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.9194e-04 - val_loss: 0.0013\n",
      "Epoch 307/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 9.0029e-04 - val_loss: 0.0014\n",
      "Epoch 308/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 9.3733e-04 - val_loss: 0.0016\n",
      "Epoch 309/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 9.1203e-04 - val_loss: 0.0013\n",
      "Epoch 310/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.7002e-04 - val_loss: 0.0013\n",
      "Epoch 311/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 9.0131e-04 - val_loss: 0.0012\n",
      "Epoch 312/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.5325e-04 - val_loss: 0.0012\n",
      "Epoch 313/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 9.2112e-04 - val_loss: 0.0011\n",
      "Epoch 314/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 9.0238e-04 - val_loss: 0.0012\n",
      "Epoch 315/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 9.9003e-04 - val_loss: 0.0011\n",
      "Epoch 316/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.8498e-04 - val_loss: 0.0010\n",
      "Epoch 317/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 8.1939e-04 - val_loss: 0.0011\n",
      "Epoch 318/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 7.9524e-04 - val_loss: 0.0011\n",
      "Epoch 319/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 8.4265e-04 - val_loss: 0.0011\n",
      "Epoch 320/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.5835e-04 - val_loss: 0.0012\n",
      "Epoch 321/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.2902e-04 - val_loss: 0.0011\n",
      "Epoch 322/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 8.8527e-04 - val_loss: 9.7249e-04\n",
      "Epoch 323/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 8.1997e-04 - val_loss: 0.0011\n",
      "Epoch 324/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 7.9332e-04 - val_loss: 0.0011\n",
      "Epoch 325/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.3138e-04 - val_loss: 0.0010\n",
      "Epoch 326/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.8208e-04 - val_loss: 0.0011\n",
      "Epoch 327/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.8317e-04 - val_loss: 9.9502e-04\n",
      "Epoch 328/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.7932e-04 - val_loss: 9.4460e-04\n",
      "Epoch 329/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.0807e-04 - val_loss: 0.0011\n",
      "Epoch 330/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.3195e-04 - val_loss: 0.0010\n",
      "Epoch 331/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.3730e-04 - val_loss: 0.0012\n",
      "Epoch 332/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 8.7757e-04 - val_loss: 0.0011\n",
      "Epoch 333/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.8892e-04 - val_loss: 0.0011\n",
      "Epoch 334/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 9.1015e-04 - val_loss: 0.0011\n",
      "Epoch 335/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.9570e-04 - val_loss: 0.0013\n",
      "Epoch 336/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.7105e-04 - val_loss: 0.0010\n",
      "Epoch 337/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.6596e-04 - val_loss: 0.0010\n",
      "Epoch 338/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 9.7308e-04 - val_loss: 0.0010\n",
      "Epoch 339/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.8149e-04 - val_loss: 0.0010\n",
      "Epoch 340/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.5273e-04 - val_loss: 0.0010\n",
      "Epoch 341/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.8848e-04 - val_loss: 0.0012\n",
      "Epoch 342/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.6032e-04 - val_loss: 0.0012\n",
      "Epoch 343/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 9.3100e-04 - val_loss: 0.0011\n",
      "Epoch 344/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 9.0832e-04 - val_loss: 0.0010\n",
      "Epoch 345/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.9509e-04 - val_loss: 0.0010\n",
      "Epoch 346/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.8088e-04 - val_loss: 0.0010\n",
      "Epoch 347/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 9.5155e-04 - val_loss: 0.0010\n",
      "Epoch 348/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.8811e-04 - val_loss: 9.7706e-04\n",
      "Epoch 349/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 9.5391e-04 - val_loss: 0.0011\n",
      "Epoch 350/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0010 - val_loss: 0.0011\n",
      "Epoch 351/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 9.0481e-04 - val_loss: 0.0012\n",
      "Epoch 352/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.7132e-04 - val_loss: 0.0012\n",
      "Epoch 353/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.2720e-04 - val_loss: 0.0010\n",
      "Epoch 354/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 7.9709e-04 - val_loss: 0.0010\n",
      "Epoch 355/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 7.9842e-04 - val_loss: 0.0010\n",
      "Epoch 356/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.1185e-04 - val_loss: 0.0010\n",
      "Epoch 357/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 8.2678e-04 - val_loss: 9.8638e-04\n",
      "Epoch 358/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.4967e-04 - val_loss: 0.0011\n",
      "Epoch 359/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.8714e-04 - val_loss: 0.0011\n",
      "Epoch 360/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.3723e-04 - val_loss: 0.0012\n",
      "Epoch 361/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.3278e-04 - val_loss: 0.0011\n",
      "Epoch 362/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.0411e-04 - val_loss: 0.0010\n",
      "Epoch 363/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 8.2547e-04 - val_loss: 9.5458e-04\n",
      "Epoch 364/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 7.6738e-04 - val_loss: 0.0011\n",
      "Epoch 365/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 8.2828e-04 - val_loss: 0.0011\n",
      "Epoch 366/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.7450e-04 - val_loss: 9.3266e-04\n",
      "Epoch 367/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 7.8032e-04 - val_loss: 0.0010\n",
      "Epoch 368/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.0750e-04 - val_loss: 0.0011\n",
      "Epoch 369/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.2049e-04 - val_loss: 0.0011\n",
      "Epoch 370/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.3598e-04 - val_loss: 0.0010\n",
      "Epoch 371/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 7.9715e-04 - val_loss: 0.0010\n",
      "Epoch 372/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.1916e-04 - val_loss: 9.8597e-04\n",
      "Epoch 373/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.1320e-04 - val_loss: 0.0011\n",
      "Epoch 374/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.3924e-04 - val_loss: 0.0011\n",
      "Epoch 375/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.0833e-04 - val_loss: 9.7253e-04\n",
      "Epoch 376/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 8.1687e-04 - val_loss: 0.0010\n",
      "Epoch 377/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 8.0515e-04 - val_loss: 0.0010\n",
      "Epoch 378/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.7423e-04 - val_loss: 0.0011\n",
      "Epoch 379/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.6480e-04 - val_loss: 9.1980e-04\n",
      "Epoch 380/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.5248e-04 - val_loss: 9.9187e-04\n",
      "Epoch 381/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.4811e-04 - val_loss: 9.6804e-04\n",
      "Epoch 382/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 9.5890e-04 - val_loss: 8.4460e-04\n",
      "Epoch 383/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.1269e-04 - val_loss: 9.3244e-04\n",
      "Epoch 384/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.2086e-04 - val_loss: 0.0010\n",
      "Epoch 385/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 7.6089e-04 - val_loss: 0.0010\n",
      "Epoch 386/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.0932e-04 - val_loss: 9.1636e-04\n",
      "Epoch 387/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 7.7518e-04 - val_loss: 9.4662e-04\n",
      "Epoch 388/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 7.9364e-04 - val_loss: 9.2054e-04\n",
      "Epoch 389/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 7.9714e-04 - val_loss: 0.0010\n",
      "Epoch 390/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 7.7294e-04 - val_loss: 9.2339e-04\n",
      "Epoch 391/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.2666e-04 - val_loss: 9.5467e-04\n",
      "Epoch 392/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 7.6271e-04 - val_loss: 8.9492e-04\n",
      "Epoch 393/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 7.7205e-04 - val_loss: 8.9367e-04\n",
      "Epoch 394/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 7.8594e-04 - val_loss: 9.7097e-04\n",
      "Epoch 395/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.0729e-04 - val_loss: 9.2543e-04\n",
      "Epoch 396/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.1312e-04 - val_loss: 9.1815e-04\n",
      "Epoch 397/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 9.0177e-04 - val_loss: 0.0010\n",
      "Epoch 398/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.1503e-04 - val_loss: 9.6781e-04\n",
      "Epoch 399/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 7.9533e-04 - val_loss: 9.8249e-04\n",
      "Epoch 400/1000\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 8.7051e-04\n",
      "Epoch 00400: saving model to saved_models/latent64/cp-0400.h5\n",
      "6/6 [==============================] - 1s 95ms/step - loss: 8.5938e-04 - val_loss: 8.7706e-04\n",
      "Epoch 401/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.3987e-04 - val_loss: 8.8121e-04\n",
      "Epoch 402/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 7.9186e-04 - val_loss: 8.9244e-04\n",
      "Epoch 403/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 8.4254e-04 - val_loss: 0.0012\n",
      "Epoch 404/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 8.2987e-04 - val_loss: 8.5406e-04\n",
      "Epoch 405/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 8.3703e-04 - val_loss: 9.6960e-04\n",
      "Epoch 406/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.1216e-04 - val_loss: 9.5531e-04\n",
      "Epoch 407/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 7.4394e-04 - val_loss: 9.3242e-04\n",
      "Epoch 408/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.3120e-04 - val_loss: 9.0551e-04\n",
      "Epoch 409/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.1345e-04 - val_loss: 8.5117e-04\n",
      "Epoch 410/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.0204e-04 - val_loss: 0.0011\n",
      "Epoch 411/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 7.8532e-04 - val_loss: 9.3761e-04\n",
      "Epoch 412/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 7.5240e-04 - val_loss: 9.0514e-04\n",
      "Epoch 413/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 7.5644e-04 - val_loss: 8.4426e-04\n",
      "Epoch 414/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 7.2186e-04 - val_loss: 8.8630e-04\n",
      "Epoch 415/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.0433e-04 - val_loss: 8.8777e-04\n",
      "Epoch 416/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 7.4485e-04 - val_loss: 9.9099e-04\n",
      "Epoch 417/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.0931e-04 - val_loss: 9.1787e-04\n",
      "Epoch 418/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 7.9965e-04 - val_loss: 9.2999e-04\n",
      "Epoch 419/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 7.8848e-04 - val_loss: 9.2608e-04\n",
      "Epoch 420/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 7.9151e-04 - val_loss: 8.3935e-04\n",
      "Epoch 421/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.1045e-04 - val_loss: 9.5275e-04\n",
      "Epoch 422/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 7.1762e-04 - val_loss: 8.5448e-04\n",
      "Epoch 423/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 7.4056e-04 - val_loss: 8.0633e-04\n",
      "Epoch 424/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 7.2202e-04 - val_loss: 8.2874e-04\n",
      "Epoch 425/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 7.8824e-04 - val_loss: 8.4488e-04\n",
      "Epoch 426/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 7.2305e-04 - val_loss: 8.9756e-04\n",
      "Epoch 427/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 7.7999e-04 - val_loss: 8.6576e-04\n",
      "Epoch 428/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 7.4393e-04 - val_loss: 9.4977e-04\n",
      "Epoch 429/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.0803e-04 - val_loss: 9.1461e-04\n",
      "Epoch 430/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 7.5988e-04 - val_loss: 8.6455e-04\n",
      "Epoch 431/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 7.8596e-04 - val_loss: 8.9881e-04\n",
      "Epoch 432/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.0524e-04 - val_loss: 8.2306e-04\n",
      "Epoch 433/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 7.3721e-04 - val_loss: 8.3731e-04\n",
      "Epoch 434/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 7.7841e-04 - val_loss: 8.4224e-04\n",
      "Epoch 435/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 7.1054e-04 - val_loss: 8.4153e-04\n",
      "Epoch 436/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 7.6898e-04 - val_loss: 8.1786e-04\n",
      "Epoch 437/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 7.3504e-04 - val_loss: 9.3264e-04\n",
      "Epoch 438/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 7.6400e-04 - val_loss: 8.3768e-04\n",
      "Epoch 439/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 6.9952e-04 - val_loss: 9.4569e-04\n",
      "Epoch 440/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 7.2386e-04 - val_loss: 8.7180e-04\n",
      "Epoch 441/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 7.8647e-04 - val_loss: 8.7545e-04\n",
      "Epoch 442/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 7.3118e-04 - val_loss: 8.8255e-04\n",
      "Epoch 443/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 7.3155e-04 - val_loss: 8.6763e-04\n",
      "Epoch 444/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 6.9756e-04 - val_loss: 8.8449e-04\n",
      "Epoch 445/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 7.2995e-04 - val_loss: 8.4517e-04\n",
      "Epoch 446/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 7.5238e-04 - val_loss: 8.5041e-04\n",
      "Epoch 447/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 7.1553e-04 - val_loss: 8.2035e-04\n",
      "Epoch 448/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 7.4682e-04 - val_loss: 8.3778e-04\n",
      "Epoch 449/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 7.6391e-04 - val_loss: 8.2338e-04\n",
      "Epoch 450/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.0201e-04 - val_loss: 8.8398e-04\n",
      "Epoch 451/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 7.5666e-04 - val_loss: 8.9723e-04\n",
      "Epoch 452/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 7.7082e-04 - val_loss: 8.4206e-04\n",
      "Epoch 453/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 7.5771e-04 - val_loss: 9.8529e-04\n",
      "Epoch 454/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 8.0937e-04 - val_loss: 9.3661e-04\n",
      "Epoch 455/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 7.9263e-04 - val_loss: 9.5796e-04\n",
      "Epoch 456/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 7.6122e-04 - val_loss: 0.0011\n",
      "Epoch 457/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 7.6234e-04 - val_loss: 9.0006e-04\n",
      "Epoch 458/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 7.7762e-04 - val_loss: 0.0011\n",
      "Epoch 459/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.6763e-04 - val_loss: 9.5017e-04\n",
      "Epoch 460/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 7.6951e-04 - val_loss: 9.8992e-04\n",
      "Epoch 461/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 7.7723e-04 - val_loss: 9.3268e-04\n",
      "Epoch 462/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 7.7478e-04 - val_loss: 8.9701e-04\n",
      "Epoch 463/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 7.4551e-04 - val_loss: 8.4048e-04\n",
      "Epoch 464/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 7.6023e-04 - val_loss: 8.6919e-04\n",
      "Epoch 465/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 6.7653e-04 - val_loss: 8.7646e-04\n",
      "Epoch 466/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 7.5662e-04 - val_loss: 8.9516e-04\n",
      "Epoch 467/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 7.7764e-04 - val_loss: 8.5365e-04\n",
      "Epoch 468/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.3565e-04 - val_loss: 8.8617e-04\n",
      "Epoch 469/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 7.9049e-04 - val_loss: 9.0111e-04\n",
      "Epoch 470/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 7.2470e-04 - val_loss: 8.8167e-04\n",
      "Epoch 471/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.4050e-04 - val_loss: 8.6630e-04\n",
      "Epoch 472/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 7.5237e-04 - val_loss: 9.0244e-04\n",
      "Epoch 473/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 7.0333e-04 - val_loss: 8.5077e-04\n",
      "Epoch 474/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 7.6122e-04 - val_loss: 9.7005e-04\n",
      "Epoch 475/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 7.6062e-04 - val_loss: 9.3645e-04\n",
      "Epoch 476/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 8.0252e-04 - val_loss: 9.0743e-04\n",
      "Epoch 477/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 6.9697e-04 - val_loss: 9.5490e-04\n",
      "Epoch 478/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 7.5642e-04 - val_loss: 9.4896e-04\n",
      "Epoch 479/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 7.5071e-04 - val_loss: 9.1627e-04\n",
      "Epoch 480/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 7.1649e-04 - val_loss: 9.2574e-04\n",
      "Epoch 481/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 7.0180e-04 - val_loss: 8.9481e-04\n",
      "Epoch 482/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 7.2022e-04 - val_loss: 8.9806e-04\n",
      "Epoch 483/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 7.7477e-04 - val_loss: 8.5394e-04\n",
      "Epoch 484/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 7.3090e-04 - val_loss: 9.5253e-04\n",
      "Epoch 485/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 7.4761e-04 - val_loss: 8.6238e-04\n",
      "Epoch 486/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 7.4520e-04 - val_loss: 8.0293e-04\n",
      "Epoch 487/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 7.2947e-04 - val_loss: 8.1136e-04\n",
      "Epoch 488/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 7.2668e-04 - val_loss: 8.6071e-04\n",
      "Epoch 489/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 6.9200e-04 - val_loss: 7.9740e-04\n",
      "Epoch 490/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 7.2086e-04 - val_loss: 8.5739e-04\n",
      "Epoch 491/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 7.1876e-04 - val_loss: 7.8635e-04\n",
      "Epoch 492/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 6.9032e-04 - val_loss: 8.0359e-04\n",
      "Epoch 493/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 7.4564e-04 - val_loss: 7.9494e-04\n",
      "Epoch 494/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 7.3581e-04 - val_loss: 8.6367e-04\n",
      "Epoch 495/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 8.0430e-04 - val_loss: 7.7175e-04\n",
      "Epoch 496/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 7.5437e-04 - val_loss: 8.0206e-04\n",
      "Epoch 497/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 6.8384e-04 - val_loss: 7.8452e-04\n",
      "Epoch 498/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 6.8027e-04 - val_loss: 8.6007e-04\n",
      "Epoch 499/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 7.4854e-04 - val_loss: 8.9734e-04\n",
      "Epoch 500/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 6.6802e-04 - val_loss: 8.0772e-04\n",
      "Epoch 501/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 6.8438e-04 - val_loss: 8.2383e-04\n",
      "Epoch 502/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 6.8597e-04 - val_loss: 8.5512e-04\n",
      "Epoch 503/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 7.1410e-04 - val_loss: 8.2615e-04\n",
      "Epoch 504/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 7.8599e-04 - val_loss: 8.3336e-04\n",
      "Epoch 505/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 6.9526e-04 - val_loss: 7.7224e-04\n",
      "Epoch 506/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 6.6756e-04 - val_loss: 8.4374e-04\n",
      "Epoch 507/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 7.0871e-04 - val_loss: 7.8395e-04\n",
      "Epoch 508/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 7.2701e-04 - val_loss: 8.5398e-04\n",
      "Epoch 509/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 7.6242e-04 - val_loss: 8.5289e-04\n",
      "Epoch 510/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 7.3278e-04 - val_loss: 8.4609e-04\n",
      "Epoch 511/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 7.4418e-04 - val_loss: 9.0431e-04\n",
      "Epoch 512/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 7.0825e-04 - val_loss: 8.7689e-04\n",
      "Epoch 513/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 7.2782e-04 - val_loss: 8.5984e-04\n",
      "Epoch 514/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 7.5639e-04 - val_loss: 8.8313e-04\n",
      "Epoch 515/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 6.7118e-04 - val_loss: 7.9408e-04\n",
      "Epoch 516/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 6.9275e-04 - val_loss: 8.8254e-04\n",
      "Epoch 517/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 6.9312e-04 - val_loss: 7.4737e-04\n",
      "Epoch 518/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 6.7729e-04 - val_loss: 8.6025e-04\n",
      "Epoch 519/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 6.4470e-04 - val_loss: 8.3336e-04\n",
      "Epoch 520/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 6.9712e-04 - val_loss: 9.0442e-04\n",
      "Epoch 521/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 7.2986e-04 - val_loss: 8.7189e-04\n",
      "Epoch 522/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 6.8628e-04 - val_loss: 8.2237e-04\n",
      "Epoch 523/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 6.7603e-04 - val_loss: 8.1283e-04\n",
      "Epoch 524/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 6.3440e-04 - val_loss: 7.6718e-04\n",
      "Epoch 525/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 6.8992e-04 - val_loss: 8.0841e-04\n",
      "Epoch 526/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 6.6006e-04 - val_loss: 8.6334e-04\n",
      "Epoch 527/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 6.7353e-04 - val_loss: 8.6305e-04\n",
      "Epoch 528/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 7.3555e-04 - val_loss: 8.4691e-04\n",
      "Epoch 529/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 7.3501e-04 - val_loss: 8.0671e-04\n",
      "Epoch 530/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 6.9395e-04 - val_loss: 9.1453e-04\n",
      "Epoch 531/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 7.5297e-04 - val_loss: 9.4758e-04\n",
      "Epoch 532/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 7.5117e-04 - val_loss: 9.3227e-04\n",
      "Epoch 533/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 6.8346e-04 - val_loss: 8.4093e-04\n",
      "Epoch 534/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 7.1665e-04 - val_loss: 7.8568e-04\n",
      "Epoch 535/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 7.1972e-04 - val_loss: 9.1737e-04\n",
      "Epoch 536/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 6.8870e-04 - val_loss: 8.7094e-04\n",
      "Epoch 537/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 7.1061e-04 - val_loss: 7.6056e-04\n",
      "Epoch 538/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 6.6164e-04 - val_loss: 8.6500e-04\n",
      "Epoch 539/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 6.6442e-04 - val_loss: 7.5233e-04\n",
      "Epoch 540/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 6.8343e-04 - val_loss: 7.8802e-04\n",
      "Epoch 541/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 7.0783e-04 - val_loss: 7.6200e-04\n",
      "Epoch 542/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 6.5757e-04 - val_loss: 7.5477e-04\n",
      "Epoch 543/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 6.4761e-04 - val_loss: 7.9938e-04\n",
      "Epoch 544/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 6.6974e-04 - val_loss: 7.9257e-04\n",
      "Epoch 545/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 6.7405e-04 - val_loss: 7.6057e-04\n",
      "Epoch 546/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 6.7520e-04 - val_loss: 7.7106e-04\n",
      "Epoch 547/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 6.6609e-04 - val_loss: 8.0390e-04\n",
      "Epoch 548/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 6.8863e-04 - val_loss: 8.1159e-04\n",
      "Epoch 549/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 6.8123e-04 - val_loss: 8.1549e-04\n",
      "Epoch 550/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 7.6707e-04 - val_loss: 9.9836e-04\n",
      "Epoch 551/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 7.1738e-04 - val_loss: 8.0472e-04\n",
      "Epoch 552/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 6.6324e-04 - val_loss: 8.5306e-04\n",
      "Epoch 553/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 7.0320e-04 - val_loss: 7.6108e-04\n",
      "Epoch 554/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 7.2628e-04 - val_loss: 8.0524e-04\n",
      "Epoch 555/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 6.7875e-04 - val_loss: 8.0064e-04\n",
      "Epoch 556/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 6.6284e-04 - val_loss: 7.6984e-04\n",
      "Epoch 557/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 7.0702e-04 - val_loss: 7.8458e-04\n",
      "Epoch 558/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 7.0398e-04 - val_loss: 9.0790e-04\n",
      "Epoch 559/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 6.7176e-04 - val_loss: 9.3075e-04\n",
      "Epoch 560/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 7.0339e-04 - val_loss: 7.7537e-04\n",
      "Epoch 561/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 7.0167e-04 - val_loss: 7.7152e-04\n",
      "Epoch 562/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 6.9953e-04 - val_loss: 7.6657e-04\n",
      "Epoch 563/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 6.8151e-04 - val_loss: 7.6041e-04\n",
      "Epoch 564/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 6.8768e-04 - val_loss: 7.9414e-04\n",
      "Epoch 565/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 7.2551e-04 - val_loss: 7.4441e-04\n",
      "Epoch 566/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 6.5036e-04 - val_loss: 8.3415e-04\n",
      "Epoch 567/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 6.2298e-04 - val_loss: 8.0549e-04\n",
      "Epoch 568/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 6.8667e-04 - val_loss: 0.0010\n",
      "Epoch 569/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 6.6691e-04 - val_loss: 7.8521e-04\n",
      "Epoch 570/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 6.9020e-04 - val_loss: 8.3488e-04\n",
      "Epoch 571/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 7.4064e-04 - val_loss: 7.3947e-04\n",
      "Epoch 572/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 6.7773e-04 - val_loss: 7.9348e-04\n",
      "Epoch 573/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 7.2249e-04 - val_loss: 7.9847e-04\n",
      "Epoch 574/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 6.6430e-04 - val_loss: 7.5970e-04\n",
      "Epoch 575/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 6.9640e-04 - val_loss: 7.7948e-04\n",
      "Epoch 576/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 6.3358e-04 - val_loss: 7.4740e-04\n",
      "Epoch 577/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 6.6400e-04 - val_loss: 7.7566e-04\n",
      "Epoch 578/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 6.1671e-04 - val_loss: 7.4107e-04\n",
      "Epoch 579/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 6.8692e-04 - val_loss: 7.1790e-04\n",
      "Epoch 580/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 6.5569e-04 - val_loss: 8.2076e-04\n",
      "Epoch 581/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 6.4696e-04 - val_loss: 8.0058e-04\n",
      "Epoch 582/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 6.6026e-04 - val_loss: 8.0831e-04\n",
      "Epoch 583/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 6.3217e-04 - val_loss: 7.3439e-04\n",
      "Epoch 584/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 6.3958e-04 - val_loss: 7.4310e-04\n",
      "Epoch 585/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 6.4832e-04 - val_loss: 7.6106e-04\n",
      "Epoch 586/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 6.9271e-04 - val_loss: 7.9190e-04\n",
      "Epoch 587/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 6.7760e-04 - val_loss: 7.5656e-04\n",
      "Epoch 588/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 6.4989e-04 - val_loss: 7.7071e-04\n",
      "Epoch 589/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 7.0438e-04 - val_loss: 8.3939e-04\n",
      "Epoch 590/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 7.0955e-04 - val_loss: 9.4515e-04\n",
      "Epoch 591/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 6.6978e-04 - val_loss: 0.0010\n",
      "Epoch 592/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 7.3212e-04 - val_loss: 7.9426e-04\n",
      "Epoch 593/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 6.8089e-04 - val_loss: 7.7636e-04\n",
      "Epoch 594/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 6.3701e-04 - val_loss: 6.9446e-04\n",
      "Epoch 595/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 6.5468e-04 - val_loss: 7.9174e-04\n",
      "Epoch 596/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 6.6350e-04 - val_loss: 8.1905e-04\n",
      "Epoch 597/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 6.7998e-04 - val_loss: 8.4302e-04\n",
      "Epoch 598/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 6.6380e-04 - val_loss: 7.3892e-04\n",
      "Epoch 599/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 6.4150e-04 - val_loss: 7.8527e-04\n",
      "Epoch 600/1000\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 6.4183e-04\n",
      "Epoch 00600: saving model to saved_models/latent64/cp-0600.h5\n",
      "6/6 [==============================] - 1s 204ms/step - loss: 6.5668e-04 - val_loss: 7.9995e-04\n",
      "Epoch 601/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 6.9190e-04 - val_loss: 7.6211e-04\n",
      "Epoch 602/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 6.7189e-04 - val_loss: 7.4566e-04\n",
      "Epoch 603/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 6.4394e-04 - val_loss: 7.8916e-04\n",
      "Epoch 604/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 6.7592e-04 - val_loss: 7.7480e-04\n",
      "Epoch 605/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 6.8020e-04 - val_loss: 7.6259e-04\n",
      "Epoch 606/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 6.9932e-04 - val_loss: 7.3211e-04\n",
      "Epoch 607/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 6.8044e-04 - val_loss: 7.7511e-04\n",
      "Epoch 608/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 7.1055e-04 - val_loss: 8.5890e-04\n",
      "Epoch 609/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 7.0761e-04 - val_loss: 8.1580e-04\n",
      "Epoch 610/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 7.3957e-04 - val_loss: 8.3384e-04\n",
      "Epoch 611/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 6.5349e-04 - val_loss: 8.7460e-04\n",
      "Epoch 612/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 7.3644e-04 - val_loss: 7.7173e-04\n",
      "Epoch 613/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 6.2651e-04 - val_loss: 8.3682e-04\n",
      "Epoch 614/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 6.4387e-04 - val_loss: 8.0626e-04\n",
      "Epoch 615/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 6.4978e-04 - val_loss: 7.1836e-04\n",
      "Epoch 616/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 6.4735e-04 - val_loss: 7.7189e-04\n",
      "Epoch 617/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 6.7456e-04 - val_loss: 7.3571e-04\n",
      "Epoch 618/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 5.8425e-04 - val_loss: 8.4970e-04\n",
      "Epoch 619/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 6.0042e-04 - val_loss: 8.0471e-04\n",
      "Epoch 620/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 6.6108e-04 - val_loss: 7.5826e-04\n",
      "Epoch 621/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 6.5661e-04 - val_loss: 7.4209e-04\n",
      "Epoch 622/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 6.3729e-04 - val_loss: 7.8918e-04\n",
      "Epoch 623/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 6.3411e-04 - val_loss: 8.3807e-04\n",
      "Epoch 624/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 5.9849e-04 - val_loss: 8.6495e-04\n",
      "Epoch 625/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 6.4729e-04 - val_loss: 7.3465e-04\n",
      "Epoch 626/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 6.5910e-04 - val_loss: 7.3153e-04\n",
      "Epoch 627/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 5.9823e-04 - val_loss: 7.2727e-04\n",
      "Epoch 628/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 6.0129e-04 - val_loss: 7.4419e-04\n",
      "Epoch 629/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 6.3301e-04 - val_loss: 8.6166e-04\n",
      "Epoch 630/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 6.8792e-04 - val_loss: 9.2869e-04\n",
      "Epoch 631/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 6.9234e-04 - val_loss: 7.2694e-04\n",
      "Epoch 632/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 6.1803e-04 - val_loss: 8.0588e-04\n",
      "Epoch 633/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 6.4911e-04 - val_loss: 7.2834e-04\n",
      "Epoch 634/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 6.2918e-04 - val_loss: 7.4928e-04\n",
      "Epoch 635/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 6.7982e-04 - val_loss: 8.4961e-04\n",
      "Epoch 636/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 6.9604e-04 - val_loss: 8.3604e-04\n",
      "Epoch 637/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 6.3122e-04 - val_loss: 8.6093e-04\n",
      "Epoch 638/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 6.8792e-04 - val_loss: 8.2944e-04\n",
      "Epoch 639/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 6.6355e-04 - val_loss: 7.7673e-04\n",
      "Epoch 640/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 6.5485e-04 - val_loss: 8.5647e-04\n",
      "Epoch 641/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 6.5790e-04 - val_loss: 8.9821e-04\n",
      "Epoch 642/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 6.3824e-04 - val_loss: 7.4492e-04\n",
      "Epoch 643/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 6.4079e-04 - val_loss: 7.8363e-04\n",
      "Epoch 644/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 6.0558e-04 - val_loss: 8.1839e-04\n",
      "Epoch 645/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 6.6276e-04 - val_loss: 7.6697e-04\n",
      "Epoch 646/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 6.1701e-04 - val_loss: 8.2268e-04\n",
      "Epoch 647/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 6.0396e-04 - val_loss: 7.1372e-04\n",
      "Epoch 648/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 6.5199e-04 - val_loss: 8.1671e-04\n",
      "Epoch 649/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 6.1412e-04 - val_loss: 7.5365e-04\n",
      "Epoch 650/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 6.3031e-04 - val_loss: 7.4013e-04\n",
      "Epoch 651/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 6.4719e-04 - val_loss: 7.8204e-04\n",
      "Epoch 652/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 6.3583e-04 - val_loss: 7.8330e-04\n",
      "Epoch 653/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 6.6846e-04 - val_loss: 7.5964e-04\n",
      "Epoch 654/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 6.5622e-04 - val_loss: 8.6977e-04\n",
      "Epoch 655/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 6.2201e-04 - val_loss: 7.6941e-04\n",
      "Epoch 656/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 6.0306e-04 - val_loss: 7.4959e-04\n",
      "Epoch 657/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 6.5215e-04 - val_loss: 8.3564e-04\n",
      "Epoch 658/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 6.7365e-04 - val_loss: 7.9355e-04\n",
      "Epoch 659/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 6.4475e-04 - val_loss: 7.6415e-04\n",
      "Epoch 660/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 6.5982e-04 - val_loss: 8.3243e-04\n",
      "Epoch 661/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 6.5233e-04 - val_loss: 8.1908e-04\n",
      "Epoch 662/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 6.3461e-04 - val_loss: 8.7347e-04\n",
      "Epoch 663/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 6.1864e-04 - val_loss: 8.8766e-04\n",
      "Epoch 664/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 6.4259e-04 - val_loss: 8.4322e-04\n",
      "Epoch 665/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 6.1653e-04 - val_loss: 8.2888e-04\n",
      "Epoch 666/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 6.4455e-04 - val_loss: 9.1683e-04\n",
      "Epoch 667/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 6.3422e-04 - val_loss: 7.9287e-04\n",
      "Epoch 668/1000\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 6.0827e-04Restoring model weights from the end of the best epoch.\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 6.2624e-04 - val_loss: 7.6770e-04\n",
      "Epoch 00668: early stopping\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Train VAE model with callbacks \"\"\"\n",
    "history = ae.fit(trainX, trainX, \n",
    "                  batch_size = batch_size, \n",
    "                  epochs = epochs, \n",
    "                  callbacks=[initial_checkpoint, checkpoint, EarlyStoppingAtMinLoss()],\n",
    "                  #callbacks=[initial_checkpoint, checkpoint],\n",
    "                  shuffle=True,\n",
    "                  validation_data=(valX, valX)\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step size: 6.0\n"
     ]
    }
   ],
   "source": [
    "# In this GPU implementation, it shows the number of batches/iterations for one epoch (and not the training samples), which is:\n",
    "print (\"Step size:\", np.ceil(trainX.shape[0]/batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Save the entire model \"\"\"\n",
    "# Save notes about hyperparameters used:\n",
    "with open(f'{SAVE_FOLDER}/notes.txt', 'w') as f:\n",
    "    f.write(f'Epochs: {epochs} \\n')\n",
    "    f.write(f'Batch size: {batch_size} \\n')\n",
    "    f.write(f'Latent dimension: {latent_dim} \\n')\n",
    "    f.write(f'Filter sizes: {filters} \\n')\n",
    "    f.write(f'img_width: {img_width} \\n')\n",
    "    f.write(f'img_height: {img_height} \\n')\n",
    "    f.write(f'num_channels: {num_channels}')\n",
    "    f.close()\n",
    "\n",
    "# Save encoder weights:\n",
    "encoder.save_weights(f'{SAVE_FOLDER}/ENCODER_WEIGHTS.h5')\n",
    "\n",
    "# Save the total AE model, i.e. its weights:\n",
    "ae.save_weights(f'{SAVE_FOLDER}/AE_WEIGHTS.h5')\n",
    "\n",
    "# Save the history at each epochs (cost of train and validation sets):\n",
    "np.save(f'{SAVE_FOLDER}/HISTORY.npy', history.history)\n",
    "\n",
    "# Save exact training, validation and testing data set (as numpy arrays):\n",
    "np.save(f'{SAVE_FOLDER}/trainX.npy', trainX)\n",
    "np.save(f'{SAVE_FOLDER}/valX.npy', valX)\n",
    "np.save(f'{SAVE_FOLDER}/testAllX.npy', testAllX)\n",
    "np.save(f'{SAVE_FOLDER}/testAllY.npy', testAllY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate training process\n",
    "\n",
    "Now that the training process is complete, lets evaluate the average loss of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 936x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set_style(\"darkgrid\")\n",
    "plt.figure(figsize=(13, 6))\n",
    "\n",
    "# summarize history for loss:\n",
    "plt.plot(history.history['loss'], color='blue', label='train')\n",
    "plt.plot(history.history['val_loss'], color='blue', alpha=0.4, label='validation')\n",
    "plt.xlabel('Epoch', fontsize=16)\n",
    "plt.ylabel('Cost', fontsize=16)\n",
    "plt.tick_params(axis='both', which='major', labelsize=14)\n",
    "plt.title('Average Loss During Training', fontsize=18)\n",
    "#plt.xticks(range(0,epochs))\n",
    "plt.legend(loc='upper right', fontsize=16)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
