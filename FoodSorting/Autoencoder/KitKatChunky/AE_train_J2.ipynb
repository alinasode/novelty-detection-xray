{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder: Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import axis, colorbar, imshow, show, figure, subplot\n",
    "from matplotlib import cm\n",
    "import matplotlib as mpl\n",
    "mpl.rc('axes', labelsize=18)\n",
    "mpl.rc('xtick', labelsize=16)\n",
    "mpl.rc('ytick', labelsize=16)\n",
    "%matplotlib inline\n",
    "\n",
    "import glob\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "import cv2\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "## ----- GPU ------------------------------------\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'    # use of GPU 0 (ERDA) (use before importing torch or tensorflow/keeas)\n",
    "## ----------------------------------------------\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.layers import Conv2D, Conv2DTranspose, Input, Flatten, Dense, Lambda, Reshape\n",
    "from keras.layers import BatchNormalization, ReLU, LeakyReLU\n",
    "from keras.models import Model\n",
    "from keras.losses import binary_crossentropy, mse\n",
    "from keras.activations import relu\n",
    "from keras.callbacks import Callback, ModelCheckpoint\n",
    "from keras import backend as K                         #contains calls for tensor manipulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n",
      "2.4.3\n"
     ]
    }
   ],
   "source": [
    "print (tf.__version__)\n",
    "print (keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NORMAL_TRAIN_AUG:       /home/jovyan/work/Speciale/FoodSorting/generated_dataset/KitKatChunky//DataAugmentation/NormalTrainAugmentations\n",
      "NORMAL_VALIDATION_AUG:  /home/jovyan/work/Speciale/FoodSorting/generated_dataset/KitKatChunky//DataAugmentation/NormalValidationAugmentations\n",
      "NORMAL_TEST_AUG:        /home/jovyan/work/Speciale/FoodSorting/generated_dataset/KitKatChunky//DataAugmentation/NormalTestAugmentations\n",
      "ANOMALY_AUG:            /home/jovyan/work/Speciale/FoodSorting/generated_dataset/KitKatChunky//DataAugmentation/AnomalyAugmentations\n"
     ]
    }
   ],
   "source": [
    "from utils_ae_kitkat_paths import *\n",
    "\n",
    "# Get work directions for augmentated data set:\n",
    "print (\"NORMAL_TRAIN_AUG:      \", NORMAL_TRAIN_AUG)\n",
    "print (\"NORMAL_VALIDATION_AUG: \", NORMAL_VAL_AUG)\n",
    "print (\"NORMAL_TEST_AUG:       \", NORMAL_TEST_AUG)\n",
    "print (\"ANOMALY_AUG:           \", ANOMALY_AUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Which Folder The Models Are Saved In: saved_models/latent2\n"
     ]
    }
   ],
   "source": [
    "# What latent dimension and filter size are we using?:\n",
    "latent_dim = 2\n",
    "filters    = 32\n",
    "\n",
    "# Get work direction for saving files:\n",
    "SAVE_FOLDER = f'saved_models/latent{latent_dim}'\n",
    "print (\"Which Folder The Models Are Saved In:\", SAVE_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] deleting existing files in folder...\n",
      "         done in 0.020 minutes\n"
     ]
    }
   ],
   "source": [
    "# Delete all existing files in folder where models are saved:\n",
    "print(\"[INFO] deleting existing files in folder...\")\n",
    "t0 = time()\n",
    "\n",
    "files = glob.glob(f'{SAVE_FOLDER}/*')\n",
    "for f in files:\n",
    "    os.remove(f)\n",
    "\n",
    "print (\"         done in %0.3f minutes\" % ((time() - t0)/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_VAE_AE import get_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Investigation of Ground Truths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of \"normal\" training images is:     1400\n",
      "Number of \"normal\" validation images is:   134\n",
      "Number of \"normal\" test images is:         266\n",
      "Number of \"anomaly\" images is:             600\n"
     ]
    }
   ],
   "source": [
    "# Glob the directories and get the lists of good and bad images\n",
    "good_train_fns = [f for f in os.listdir(NORMAL_TRAIN_AUG) if not f.startswith('.')]\n",
    "good_val_fns = [f for f in os.listdir(NORMAL_VAL_AUG) if not f.startswith('.')]\n",
    "good_test_fns = [f for f in os.listdir(NORMAL_TEST_AUG) if not f.startswith('.')]\n",
    "bad_fns = [f for f in os.listdir(ANOMALY_AUG) if not f.startswith('.')]\n",
    "\n",
    "print('Number of \"normal\" training images is:     {}'.format(len(good_train_fns)))\n",
    "print('Number of \"normal\" validation images is:   {}'.format(len(good_val_fns)))\n",
    "print('Number of \"normal\" test images is:         {}'.format(len(good_test_fns)))\n",
    "print('Number of \"anomaly\" images is:             {}'.format(len(bad_fns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Procentage of normal samples (ground truth):   75.00 %\n",
      "> Procentage of anomaly samples (ground truth):  25.00 %\n"
     ]
    }
   ],
   "source": [
    "# compute procentage of bad images vs good images:\n",
    "normal_fns = len(good_train_fns) + len(good_val_fns) + len(good_test_fns)\n",
    "anomaly_fns = len(bad_fns)\n",
    "print(f\"> Procentage of normal samples (ground truth):   {(normal_fns / (normal_fns + anomaly_fns)) * 100:.2f} %\")\n",
    "print(f\"> Procentage of anomaly samples (ground truth):  {(anomaly_fns / (normal_fns + anomaly_fns)) * 100:.2f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Normal Data\n",
    "\n",
    "Load normal samples in pre-splitted data sets: train, valdiation and test.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading normal training images...\n",
      "          done\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] loading normal training images...\")\n",
    "trainX = [cv2.imread(file, 0) for file in glob.glob(f\"{NORMAL_TRAIN_AUG}/*.png\")]  # read as grayscale\n",
    "trainX = np.array(trainX)\n",
    "print(\"          done\")\n",
    "\n",
    "# create a corresponding list of labels:\n",
    "trainY = get_labels(trainX, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading normal validation images...\n",
      "          done\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] loading normal validation images...\")\n",
    "x_normal_val = [cv2.imread(file, 0) for file in glob.glob(f\"{NORMAL_VAL_AUG}/*.png\")]  # read as grayscale\n",
    "x_normal_val = np.array(x_normal_val)\n",
    "print(\"          done\")\n",
    "\n",
    "# create a corresponding list of labels:\n",
    "y_normal_val = get_labels(x_normal_val, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading normal testing images...\n",
      "          done\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] loading normal testing images...\")\n",
    "x_normal_test = [cv2.imread(file, 0) for file in glob.glob(f\"{NORMAL_TEST_AUG}/*.png\")]  # read as grayscale\n",
    "x_normal_test = np.array(x_normal_test)\n",
    "print(\"          done\")\n",
    "\n",
    "# create a corresponding list of labels:\n",
    "y_normal_test = get_labels(x_normal_test, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Anomaly Data\n",
    "\n",
    "\n",
    "Load anomaly samples in its singular training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading anomaly images...\n",
      "          done\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] loading anomaly images...\")\n",
    "x_anomaly = [cv2.imread(file, 0) for file in glob.glob(f\"{ANOMALY_AUG}/*.png\")]  # read as grayscale\n",
    "x_anomaly = np.array(x_anomaly)\n",
    "print(\"          done\")\n",
    "\n",
    "# create a corresponding list of labels:\n",
    "y_anomaly = get_labels(x_anomaly, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine normal samples and anomaly samples and split them into training, validation and test sets\n",
    "\n",
    "`label 0` == Normal Samples\n",
    "\n",
    "`label 1` == Anomaly Samples\n",
    "\n",
    "Train and Validation sets only consists of `Normal Data`, aka. `label 0`.\n",
    "\n",
    "Testing sets consists of both  `Normal Data` (aka. `label 0`) and `Anomaly Data` (aka. `label 1`)\n",
    "\n",
    "________________\n",
    "\n",
    "Due to the very sparse original (preprossed) KitKat Chunky data, the validation set will be a mix of normal testing and validation data. The normal testing set will consists of all validation and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine pre-loaded validation and testing set into a new testing set:\n",
    "testvalX = np.vstack((x_normal_val, x_normal_test))\n",
    "testvalY = get_labels(testvalX, 0)\n",
    "\n",
    "# randomly split in order to make new validation set:\n",
    "NoUsageX, valX, NoUsageY, valY = train_test_split(testvalX, testvalY, test_size=0.25, random_state=4345672)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine pre-loaded validation and testing set into a new testing set:\n",
    "testNormalX = np.vstack((x_normal_val, x_normal_test))\n",
    "testNormalY = get_labels(testNormalX, 0)\n",
    "\n",
    "# anomaly class (only used for testing):\n",
    "testAnomalyX, testAnomalyY = x_anomaly, y_anomaly\n",
    "\n",
    "# Combine all testing data in one array:\n",
    "testAllX = np.vstack((testNormalX, testAnomalyX))\n",
    "testAllY = np.hstack((testNormalY, testAnomalyY))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Data Dimensions and Distributions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect data shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:\n",
      " > images: (1400, 128, 128)\n",
      " > labels: (1400,)\n",
      "\n",
      "Validation set:\n",
      " > images: (100, 128, 128)\n",
      " > labels: (100,)\n",
      "\n",
      "Test set:\n",
      " > images: (1000, 128, 128)\n",
      " > labels: (1000,)\n",
      "\t'Normal' test set:\n",
      "\t  > images: (400, 128, 128)\n",
      "\t  > labels: (400,)\n",
      "\t'Anomaly' test set:\n",
      "\t  > images: (600, 128, 128)\n",
      "\t  > labels: (600,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set:\")\n",
    "print(\" > images:\", trainX.shape)\n",
    "print(\" > labels:\", trainY.shape)\n",
    "print (\"\")\n",
    "\n",
    "print(\"Validation set:\")\n",
    "print(\" > images:\", valX.shape)\n",
    "print(\" > labels:\", valY.shape)\n",
    "print (\"\")\n",
    "\n",
    "print(\"Test set:\")\n",
    "print(\" > images:\", testAllX.shape)\n",
    "print(\" > labels:\", testAllY.shape)\n",
    "print(\"\\t'Normal' test set:\")\n",
    "print(\"\\t  > images:\", testNormalX.shape)\n",
    "print(\"\\t  > labels:\", testNormalY.shape)\n",
    "print(\"\\t'Anomaly' test set:\")\n",
    "print(\"\\t  > images:\", testAnomalyX.shape)\n",
    "print(\"\\t  > labels:\", testAnomalyY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procentage of training samples: 73.68 %\n",
      "Procentage of validation samples: 5.26 %\n",
      "Procentage of (only normal) testing samples: 21.05 %\n"
     ]
    }
   ],
   "source": [
    "# compute procentage between training, validation and test data sets (only normal data):\n",
    "print(f\"Procentage of training samples: {(len(trainX) / (len(trainX) + len(valX) + len(testNormalX))) * 100:.2f} %\")\n",
    "print(f\"Procentage of validation samples: {(len(valX) / (len(trainX) + len(valX) + len(testNormalX))) * 100:.2f} %\")\n",
    "print(f\"Procentage of (only normal) testing samples: {(len(testNormalX) / (len(trainX) + len(valX) + len(testNormalX))) * 100:.2f} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procentage of training samples: 56.00 %\n",
      "Procentage of validation samples: 4.00 %\n",
      "Procentage of (total) testing samples: 40.00 %\n"
     ]
    }
   ],
   "source": [
    "# compute procentage between training, validation and test data sets:\n",
    "print(f\"Procentage of training samples: {(len(trainX) / (len(trainX) + len(valX) + len(testAllX))) * 100:.2f} %\")\n",
    "print(f\"Procentage of validation samples: {(len(valX) / (len(trainX) + len(valX) + len(testAllX))) * 100:.2f} %\")\n",
    "print(f\"Procentage of (total) testing samples: {(len(testAllX) / (len(trainX) + len(valX) + len(testAllX))) * 100:.2f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for (un)balanced data\n",
    "\n",
    "In an ideal world the data should be balanced. However in the real world we expect there being around ~$99 \\%$ good samples and only ~$1 \\%$ bad samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels = ['0: normal', '1: anomaly']\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(16,4))\n",
    "ax[0].hist(trainY, align=\"left\"); ax[0].set_title('train', fontsize=18); ax[0].set_xticks([0,1]); ax[0].set_xticklabels(labels); ax[0].tick_params(axis='both', which='major', labelsize=16); ax[0].set_xlim(-0.5, 1.5);\n",
    "ax[1].hist(valY, align=\"left\"); ax[1].set_title('validation', fontsize=18); ax[1].set_xticks([0,1]); ax[1].set_xticklabels(labels); ax[1].tick_params(axis='both', which='major', labelsize=16); ax[1].set_xlim(-0.5, 1.5);\n",
    "ax[2].hist(testAllY, align=\"left\"); ax[2].set_title('test', fontsize=18); ax[2].set_xticks([0,1]); ax[2].set_xticklabels(labels); ax[2].tick_params(axis='both', which='major', labelsize=16); ax[2].set_xlim(-0.5, 1.5);\n",
    "\n",
    "ax[0].set_ylabel(\"Number of images\", fontsize=18)\n",
    "ax[1].set_xlabel(\"Class Labels\", fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procentage of normal test samples: 40.00 %\n",
      "Procentage of total anomaly test samples: 60.00 %\n"
     ]
    }
   ],
   "source": [
    "# compute procentage of normal test images vs total anomaly test images:\n",
    "print(f\"Procentage of normal test samples: {(len(testNormalX) / (len(testNormalX) + len(testAnomalyX))) * 100:.2f} %\")\n",
    "print(f\"Procentage of total anomaly test samples: {(len(testAnomalyX) / (len(testNormalX) + len(testAnomalyX))) * 100:.2f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling\n",
    "\n",
    "Only normalization has been used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data configuration\n",
    "img_width, img_height = trainX.shape[1], trainX.shape[2]      # input image dimensions\n",
    "num_channels = 1                                              # gray-channel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization\n",
    "\n",
    "Neural Networks learns faster with normalized data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize to be in  the [0, 1] range:\n",
    "trainX = trainX.astype('float32') / 255.\n",
    "valX = valX.astype('float32') / 255.\n",
    "testAllX = testAllX.astype('float32') / 255.\n",
    "\n",
    "# reshape data to keras inputs --> (n_samples, img_rows, img_cols, n_channels):\n",
    "trainX = trainX.reshape(trainX.shape[0], img_height, img_width, num_channels)\n",
    "valX = valX.reshape(valX.shape[0], img_height, img_width, num_channels)\n",
    "testAllX = testAllX.reshape(testAllX.shape[0], img_height, img_width, num_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the AE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture and Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import architecture of VAE model and its hyperparameters:\n",
    "from J2_AE_model import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Total AE Model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"ae\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder_input (InputLayer)   [(None, 128, 128, 1)]     0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 64, 64, 32)        160       \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 64, 64, 32)        128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 32)        4128      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 16, 16, 32)        4128      \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 16, 16, 32)        128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 8, 8, 32)          4128      \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 8, 8, 32)          128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 4, 4, 32)          4128      \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 4, 4, 32)          128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 4, 4, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "latent (Dense)               (None, 2)                 1026      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               1536      \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "re_lu (ReLU)                 (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 4, 4, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose (Conv2DTran (None, 8, 8, 32)          4128      \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 8, 8, 32)          128       \n",
      "_________________________________________________________________\n",
      "re_lu_1 (ReLU)               (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 16, 16, 32)        4128      \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 16, 16, 32)        128       \n",
      "_________________________________________________________________\n",
      "re_lu_2 (ReLU)               (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTr (None, 32, 32, 32)        4128      \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "re_lu_3 (ReLU)               (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTr (None, 64, 64, 32)        4128      \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 64, 64, 32)        128       \n",
      "_________________________________________________________________\n",
      "re_lu_4 (ReLU)               (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_4 (Conv2DTr (None, 128, 128, 32)      4128      \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 128, 128, 32)      128       \n",
      "_________________________________________________________________\n",
      "re_lu_5 (ReLU)               (None, 128, 128, 32)      0         \n",
      "_________________________________________________________________\n",
      "decoder_output (Conv2DTransp (None, 128, 128, 1)       129       \n",
      "=================================================================\n",
      "Total params: 43,331\n",
      "Trainable params: 41,667\n",
      "Non-trainable params: 1,664\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "ae.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile and train model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Hyperparameters \"\"\"\n",
    "batch_size  = 256\n",
    "epochs      = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Checkpoint callback options: save model at 1'st epoch (inital model)\n",
    "\n",
    "https://stackoverflow.com/questions/54323960/save-keras-model-at-specific-epochs\n",
    "\"\"\"\n",
    "\n",
    "class CustomSaver(keras.callbacks.Callback):\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        if epoch == 1:\n",
    "            self.model.save_weights(f\"{SAVE_FOLDER}/cp-0001.h5\")\n",
    "            \n",
    "# create and use callback:\n",
    "initial_checkpoint = CustomSaver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Checkpoint callback options: save model at every k'te epochs\n",
    "\"\"\"\n",
    "# Include the epoch in the file name (uses `str.format`):\n",
    "checkpoint_path = \"saved_models/latent2/cp-{epoch:04d}.h5\"\n",
    "\n",
    "# Create a callback that saves the model's weights every k'te epochs:\n",
    "checkpoint = ModelCheckpoint(filepath = checkpoint_path,\n",
    "                             monitor='loss',           # name of the metrics to monitor\n",
    "                             verbose=1, \n",
    "                             #save_best_only=False,     # if False, the best model will not be overridden.\n",
    "                             save_weights_only=True,   # if True, only the weights of the models will be saved.\n",
    "                                                        # if False, the whole models will be saved.\n",
    "                             #mode='auto', \n",
    "                             period=200                  # save after every k'te epochs\n",
    "                            )\n",
    "\n",
    "# Save the weights using the `checkpoint_path` format\n",
    "ae.save_weights(checkpoint_path.format(epoch=0))          # save for zero epoch (inital)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Checkpoint callback options: Early stopping\n",
    "https://www.tensorflow.org/guide/keras/custom_callback\n",
    "\"\"\"\n",
    "\n",
    "class EarlyStoppingAtMinLoss(keras.callbacks.Callback):\n",
    "    \"\"\"Stop training when the loss is at its min, i.e. the loss stops decreasing.\n",
    "\n",
    "  Arguments:\n",
    "      patience: Number of epochs to wait after min has been hit. After this\n",
    "      number of no improvement, training stops.\n",
    "  \"\"\"\n",
    "\n",
    "    def __init__(self, patience=50):\n",
    "        super(EarlyStoppingAtMinLoss, self).__init__()\n",
    "        self.patience = patience\n",
    "        # best_weights to store the weights at which the minimum loss occurs.\n",
    "        self.best_weights = None\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        # The number of epoch it has waited when loss is no longer minimum.\n",
    "        self.wait = 0\n",
    "        # The epoch the training stops at.\n",
    "        self.stopped_epoch = 0\n",
    "        # Initialize the best as infinity.\n",
    "        self.best = np.Inf\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        current = logs.get(\"loss\")\n",
    "        if np.less(current, self.best):\n",
    "            self.best = current\n",
    "            self.wait = 0\n",
    "            # Record the best weights if current results is better (less).\n",
    "            self.best_weights = self.model.get_weights()\n",
    "        else:\n",
    "            self.wait += 1\n",
    "            if self.wait >= self.patience:\n",
    "                self.stopped_epoch = epoch\n",
    "                self.model.stop_training = True\n",
    "                print(\"Restoring model weights from the end of the best epoch.\")\n",
    "                self.model.set_weights(self.best_weights)\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        if self.stopped_epoch > 0:\n",
    "            print(\"Epoch %05d: early stopping\" % (self.stopped_epoch + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Create and Compile Model\"\"\"\n",
    "# import architecture of VAE model and its hyperparameters. learning rate choosen from graph above.\n",
    "ae = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "6/6 [==============================] - 1s 168ms/step - loss: 0.2756 - val_loss: 0.2153\n",
      "Epoch 2/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.1952 - val_loss: 0.2095\n",
      "Epoch 3/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.1452 - val_loss: 0.2021\n",
      "Epoch 4/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.1036 - val_loss: 0.1933\n",
      "Epoch 5/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0744 - val_loss: 0.1838\n",
      "Epoch 6/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0594 - val_loss: 0.1736\n",
      "Epoch 7/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0519 - val_loss: 0.1637\n",
      "Epoch 8/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0481 - val_loss: 0.1549\n",
      "Epoch 9/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0458 - val_loss: 0.1469\n",
      "Epoch 10/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0441 - val_loss: 0.1394\n",
      "Epoch 11/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0428 - val_loss: 0.1320\n",
      "Epoch 12/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0415 - val_loss: 0.1244\n",
      "Epoch 13/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0403 - val_loss: 0.1168\n",
      "Epoch 14/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0392 - val_loss: 0.1091\n",
      "Epoch 15/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0377 - val_loss: 0.1017\n",
      "Epoch 16/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0368 - val_loss: 0.0949\n",
      "Epoch 17/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0353 - val_loss: 0.0884\n",
      "Epoch 18/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0333 - val_loss: 0.0825\n",
      "Epoch 19/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0312 - val_loss: 0.0769\n",
      "Epoch 20/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0292 - val_loss: 0.0719\n",
      "Epoch 21/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0273 - val_loss: 0.0672\n",
      "Epoch 22/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0252 - val_loss: 0.0633\n",
      "Epoch 23/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0237 - val_loss: 0.0600\n",
      "Epoch 24/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0221 - val_loss: 0.0573\n",
      "Epoch 25/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0212 - val_loss: 0.0551\n",
      "Epoch 26/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0196 - val_loss: 0.0533\n",
      "Epoch 27/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0184 - val_loss: 0.0519\n",
      "Epoch 28/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0178 - val_loss: 0.0508\n",
      "Epoch 29/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0170 - val_loss: 0.0501\n",
      "Epoch 30/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0162 - val_loss: 0.0496\n",
      "Epoch 31/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0154 - val_loss: 0.0493\n",
      "Epoch 32/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0150 - val_loss: 0.0490\n",
      "Epoch 33/1000\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 0.0149 - val_loss: 0.0489\n",
      "Epoch 34/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0142 - val_loss: 0.0488\n",
      "Epoch 35/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0141 - val_loss: 0.0488\n",
      "Epoch 36/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0138 - val_loss: 0.0486\n",
      "Epoch 37/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0132 - val_loss: 0.0488\n",
      "Epoch 38/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0128 - val_loss: 0.0488\n",
      "Epoch 39/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0125 - val_loss: 0.0490\n",
      "Epoch 40/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0122 - val_loss: 0.0489\n",
      "Epoch 41/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0119 - val_loss: 0.0493\n",
      "Epoch 42/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0119 - val_loss: 0.0492\n",
      "Epoch 43/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0114 - val_loss: 0.0493\n",
      "Epoch 44/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0111 - val_loss: 0.0492\n",
      "Epoch 45/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0111 - val_loss: 0.0487\n",
      "Epoch 46/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0106 - val_loss: 0.0485\n",
      "Epoch 47/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0102 - val_loss: 0.0484\n",
      "Epoch 48/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0102 - val_loss: 0.0480\n",
      "Epoch 49/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0100 - val_loss: 0.0476\n",
      "Epoch 50/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0103 - val_loss: 0.0472\n",
      "Epoch 51/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0097 - val_loss: 0.0461\n",
      "Epoch 52/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0096 - val_loss: 0.0466\n",
      "Epoch 53/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0107 - val_loss: 0.0446\n",
      "Epoch 54/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0098 - val_loss: 0.0451\n",
      "Epoch 55/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0092 - val_loss: 0.0434\n",
      "Epoch 56/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0087 - val_loss: 0.0426\n",
      "Epoch 57/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0092 - val_loss: 0.0419\n",
      "Epoch 58/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0090 - val_loss: 0.0406\n",
      "Epoch 59/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0096 - val_loss: 0.0402\n",
      "Epoch 60/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0086 - val_loss: 0.0395\n",
      "Epoch 61/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0088 - val_loss: 0.0388\n",
      "Epoch 62/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0087 - val_loss: 0.0384\n",
      "Epoch 63/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0084 - val_loss: 0.0364\n",
      "Epoch 64/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0086 - val_loss: 0.0364\n",
      "Epoch 65/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0080 - val_loss: 0.0361\n",
      "Epoch 66/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0082 - val_loss: 0.0348\n",
      "Epoch 67/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0079 - val_loss: 0.0344\n",
      "Epoch 68/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0075 - val_loss: 0.0330\n",
      "Epoch 69/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0081 - val_loss: 0.0330\n",
      "Epoch 70/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0075 - val_loss: 0.0313\n",
      "Epoch 71/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0079 - val_loss: 0.0317\n",
      "Epoch 72/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0078 - val_loss: 0.0298\n",
      "Epoch 73/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0076 - val_loss: 0.0303\n",
      "Epoch 74/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0078 - val_loss: 0.0279\n",
      "Epoch 75/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0080 - val_loss: 0.0291\n",
      "Epoch 76/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0079 - val_loss: 0.0272\n",
      "Epoch 77/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0075 - val_loss: 0.0274\n",
      "Epoch 78/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0073 - val_loss: 0.0259\n",
      "Epoch 79/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0078 - val_loss: 0.0254\n",
      "Epoch 80/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0072 - val_loss: 0.0250\n",
      "Epoch 81/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0071 - val_loss: 0.0243\n",
      "Epoch 82/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0070 - val_loss: 0.0245\n",
      "Epoch 83/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0074 - val_loss: 0.0238\n",
      "Epoch 84/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0076 - val_loss: 0.0213\n",
      "Epoch 85/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0074 - val_loss: 0.0221\n",
      "Epoch 86/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0070 - val_loss: 0.0214\n",
      "Epoch 87/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0076 - val_loss: 0.0197\n",
      "Epoch 88/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0069 - val_loss: 0.0204\n",
      "Epoch 89/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0069 - val_loss: 0.0192\n",
      "Epoch 90/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0066 - val_loss: 0.0187\n",
      "Epoch 91/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0067 - val_loss: 0.0181\n",
      "Epoch 92/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0066 - val_loss: 0.0170\n",
      "Epoch 93/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0067 - val_loss: 0.0168\n",
      "Epoch 94/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0069 - val_loss: 0.0167\n",
      "Epoch 95/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0073 - val_loss: 0.0162\n",
      "Epoch 96/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0068 - val_loss: 0.0147\n",
      "Epoch 97/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0069 - val_loss: 0.0158\n",
      "Epoch 98/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0068 - val_loss: 0.0145\n",
      "Epoch 99/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0072 - val_loss: 0.0144\n",
      "Epoch 100/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0065 - val_loss: 0.0138\n",
      "Epoch 101/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0064 - val_loss: 0.0138\n",
      "Epoch 102/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0062 - val_loss: 0.0133\n",
      "Epoch 103/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0062 - val_loss: 0.0125\n",
      "Epoch 104/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0061 - val_loss: 0.0118\n",
      "Epoch 105/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0063 - val_loss: 0.0119\n",
      "Epoch 106/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0069 - val_loss: 0.0114\n",
      "Epoch 107/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0063 - val_loss: 0.0108\n",
      "Epoch 108/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0064 - val_loss: 0.0114\n",
      "Epoch 109/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0063 - val_loss: 0.0108\n",
      "Epoch 110/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0061 - val_loss: 0.0100\n",
      "Epoch 111/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0062 - val_loss: 0.0103\n",
      "Epoch 112/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0065 - val_loss: 0.0097\n",
      "Epoch 113/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0062 - val_loss: 0.0097\n",
      "Epoch 114/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0063 - val_loss: 0.0095\n",
      "Epoch 115/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0060 - val_loss: 0.0084\n",
      "Epoch 116/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0061 - val_loss: 0.0090\n",
      "Epoch 117/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0064 - val_loss: 0.0090\n",
      "Epoch 118/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0059 - val_loss: 0.0091\n",
      "Epoch 119/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0059 - val_loss: 0.0091\n",
      "Epoch 120/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0062 - val_loss: 0.0096\n",
      "Epoch 121/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0064 - val_loss: 0.0084\n",
      "Epoch 122/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0062 - val_loss: 0.0085\n",
      "Epoch 123/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0060 - val_loss: 0.0081\n",
      "Epoch 124/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0069 - val_loss: 0.0080\n",
      "Epoch 125/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0059 - val_loss: 0.0090\n",
      "Epoch 126/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0061 - val_loss: 0.0075\n",
      "Epoch 127/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0064 - val_loss: 0.0083\n",
      "Epoch 128/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0062 - val_loss: 0.0086\n",
      "Epoch 129/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0064 - val_loss: 0.0079\n",
      "Epoch 130/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0062 - val_loss: 0.0084\n",
      "Epoch 131/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0063 - val_loss: 0.0089\n",
      "Epoch 132/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0065 - val_loss: 0.0075\n",
      "Epoch 133/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0059 - val_loss: 0.0071\n",
      "Epoch 134/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0061 - val_loss: 0.0075\n",
      "Epoch 135/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0057 - val_loss: 0.0086\n",
      "Epoch 136/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0062 - val_loss: 0.0088\n",
      "Epoch 137/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0059 - val_loss: 0.0086\n",
      "Epoch 138/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0063 - val_loss: 0.0088\n",
      "Epoch 139/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0067 - val_loss: 0.0087\n",
      "Epoch 140/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0063 - val_loss: 0.0092\n",
      "Epoch 141/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0061 - val_loss: 0.0083\n",
      "Epoch 142/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0060 - val_loss: 0.0083\n",
      "Epoch 143/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0063 - val_loss: 0.0077\n",
      "Epoch 144/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0062 - val_loss: 0.0070\n",
      "Epoch 145/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0059 - val_loss: 0.0073\n",
      "Epoch 146/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0058 - val_loss: 0.0069\n",
      "Epoch 147/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0057 - val_loss: 0.0068\n",
      "Epoch 148/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0057 - val_loss: 0.0068\n",
      "Epoch 149/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0055 - val_loss: 0.0065\n",
      "Epoch 150/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0058 - val_loss: 0.0067\n",
      "Epoch 151/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0053 - val_loss: 0.0067\n",
      "Epoch 152/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0057 - val_loss: 0.0075\n",
      "Epoch 153/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0053 - val_loss: 0.0075\n",
      "Epoch 154/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0056 - val_loss: 0.0067\n",
      "Epoch 155/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0057 - val_loss: 0.0069\n",
      "Epoch 156/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0067 - val_loss: 0.0068\n",
      "Epoch 157/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0055 - val_loss: 0.0066\n",
      "Epoch 158/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0054 - val_loss: 0.0063\n",
      "Epoch 159/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0056 - val_loss: 0.0065\n",
      "Epoch 160/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0054 - val_loss: 0.0066\n",
      "Epoch 161/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0058 - val_loss: 0.0068\n",
      "Epoch 162/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0054 - val_loss: 0.0066\n",
      "Epoch 163/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0059 - val_loss: 0.0081\n",
      "Epoch 164/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0060 - val_loss: 0.0077\n",
      "Epoch 165/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0059 - val_loss: 0.0088\n",
      "Epoch 166/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0059 - val_loss: 0.0098\n",
      "Epoch 167/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0057 - val_loss: 0.0096\n",
      "Epoch 168/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0058 - val_loss: 0.0121\n",
      "Epoch 169/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0053 - val_loss: 0.0130\n",
      "Epoch 170/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0056 - val_loss: 0.0100\n",
      "Epoch 171/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0053 - val_loss: 0.0077\n",
      "Epoch 172/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0051 - val_loss: 0.0074\n",
      "Epoch 173/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0054 - val_loss: 0.0090\n",
      "Epoch 174/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0055 - val_loss: 0.0079\n",
      "Epoch 175/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0055 - val_loss: 0.0075\n",
      "Epoch 176/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0054 - val_loss: 0.0067\n",
      "Epoch 177/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0052 - val_loss: 0.0081\n",
      "Epoch 178/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0052 - val_loss: 0.0067\n",
      "Epoch 179/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0051 - val_loss: 0.0072\n",
      "Epoch 180/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0057 - val_loss: 0.0068\n",
      "Epoch 181/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0057 - val_loss: 0.0064\n",
      "Epoch 182/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0062 - val_loss: 0.0071\n",
      "Epoch 183/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0052 - val_loss: 0.0067\n",
      "Epoch 184/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0051 - val_loss: 0.0069\n",
      "Epoch 185/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0055 - val_loss: 0.0060\n",
      "Epoch 186/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0054 - val_loss: 0.0064\n",
      "Epoch 187/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0058 - val_loss: 0.0068\n",
      "Epoch 188/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0055 - val_loss: 0.0090\n",
      "Epoch 189/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0059 - val_loss: 0.0075\n",
      "Epoch 190/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0069 - val_loss: 0.0069\n",
      "Epoch 191/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0054 - val_loss: 0.0065\n",
      "Epoch 192/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0059 - val_loss: 0.0062\n",
      "Epoch 193/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0052 - val_loss: 0.0065\n",
      "Epoch 194/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0054 - val_loss: 0.0060\n",
      "Epoch 195/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0051 - val_loss: 0.0061\n",
      "Epoch 196/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0057 - val_loss: 0.0061\n",
      "Epoch 197/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0055 - val_loss: 0.0063\n",
      "Epoch 198/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0054 - val_loss: 0.0062\n",
      "Epoch 199/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0056 - val_loss: 0.0068\n",
      "Epoch 200/1000\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.0052\n",
      "Epoch 00200: saving model to saved_models/latent2/cp-0200.h5\n",
      "6/6 [==============================] - 1s 162ms/step - loss: 0.0052 - val_loss: 0.0064\n",
      "Epoch 201/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0053 - val_loss: 0.0083\n",
      "Epoch 202/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0059 - val_loss: 0.0067\n",
      "Epoch 203/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0055 - val_loss: 0.0063\n",
      "Epoch 204/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0061 - val_loss: 0.0071\n",
      "Epoch 205/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0055 - val_loss: 0.0062\n",
      "Epoch 206/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0053 - val_loss: 0.0068\n",
      "Epoch 207/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0057 - val_loss: 0.0071\n",
      "Epoch 208/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0056 - val_loss: 0.0066\n",
      "Epoch 209/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0059 - val_loss: 0.0066\n",
      "Epoch 210/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0061 - val_loss: 0.0080\n",
      "Epoch 211/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0052 - val_loss: 0.0065\n",
      "Epoch 212/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0062 - val_loss: 0.0072\n",
      "Epoch 213/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0057 - val_loss: 0.0116\n",
      "Epoch 214/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0052 - val_loss: 0.0092\n",
      "Epoch 215/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0056 - val_loss: 0.0097\n",
      "Epoch 216/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0051 - val_loss: 0.0070\n",
      "Epoch 217/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0060 - val_loss: 0.0088\n",
      "Epoch 218/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0058 - val_loss: 0.0071\n",
      "Epoch 219/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0050 - val_loss: 0.0064\n",
      "Epoch 220/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0056 - val_loss: 0.0105\n",
      "Epoch 221/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0056 - val_loss: 0.0090\n",
      "Epoch 222/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0052 - val_loss: 0.0094\n",
      "Epoch 223/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0050 - val_loss: 0.0091\n",
      "Epoch 224/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0052 - val_loss: 0.0092\n",
      "Epoch 225/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0055 - val_loss: 0.0061\n",
      "Epoch 226/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0054 - val_loss: 0.0062\n",
      "Epoch 227/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0052 - val_loss: 0.0061\n",
      "Epoch 228/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0050 - val_loss: 0.0062\n",
      "Epoch 229/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0057 - val_loss: 0.0064\n",
      "Epoch 230/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0052 - val_loss: 0.0067\n",
      "Epoch 231/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0052 - val_loss: 0.0063\n",
      "Epoch 232/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0053 - val_loss: 0.0072\n",
      "Epoch 233/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0053 - val_loss: 0.0069\n",
      "Epoch 234/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0051 - val_loss: 0.0084\n",
      "Epoch 235/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0050 - val_loss: 0.0088\n",
      "Epoch 236/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0054 - val_loss: 0.0065\n",
      "Epoch 237/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0050 - val_loss: 0.0072\n",
      "Epoch 238/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0052 - val_loss: 0.0058\n",
      "Epoch 239/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0052 - val_loss: 0.0074\n",
      "Epoch 240/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0054 - val_loss: 0.0062\n",
      "Epoch 241/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0056 - val_loss: 0.0064\n",
      "Epoch 242/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0056 - val_loss: 0.0062\n",
      "Epoch 243/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0051 - val_loss: 0.0061\n",
      "Epoch 244/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0050 - val_loss: 0.0062\n",
      "Epoch 245/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0050 - val_loss: 0.0065\n",
      "Epoch 246/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0055 - val_loss: 0.0060\n",
      "Epoch 247/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0050 - val_loss: 0.0063\n",
      "Epoch 248/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0050 - val_loss: 0.0062\n",
      "Epoch 249/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0056 - val_loss: 0.0064\n",
      "Epoch 250/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0053 - val_loss: 0.0061\n",
      "Epoch 251/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0060 - val_loss: 0.0063\n",
      "Epoch 252/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0060 - val_loss: 0.0080\n",
      "Epoch 253/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0055 - val_loss: 0.0079\n",
      "Epoch 254/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0057 - val_loss: 0.0088\n",
      "Epoch 255/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0053 - val_loss: 0.0064\n",
      "Epoch 256/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0051 - val_loss: 0.0175\n",
      "Epoch 257/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0052 - val_loss: 0.0104\n",
      "Epoch 258/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0055 - val_loss: 0.0126\n",
      "Epoch 259/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0050 - val_loss: 0.0174\n",
      "Epoch 260/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0059 - val_loss: 0.0216\n",
      "Epoch 261/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0053 - val_loss: 0.0086\n",
      "Epoch 262/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0055 - val_loss: 0.0072\n",
      "Epoch 263/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0054 - val_loss: 0.0063\n",
      "Epoch 264/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0051 - val_loss: 0.0066\n",
      "Epoch 265/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0051 - val_loss: 0.0064\n",
      "Epoch 266/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0057 - val_loss: 0.0069\n",
      "Epoch 267/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0050 - val_loss: 0.0060\n",
      "Epoch 268/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0052 - val_loss: 0.0062\n",
      "Epoch 269/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0051 - val_loss: 0.0063\n",
      "Epoch 270/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0062 - val_loss: 0.0070\n",
      "Epoch 271/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0055 - val_loss: 0.0062\n",
      "Epoch 272/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0048 - val_loss: 0.0059\n",
      "Epoch 273/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0049 - val_loss: 0.0060\n",
      "Epoch 274/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0047 - val_loss: 0.0061\n",
      "Epoch 275/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0051 - val_loss: 0.0058\n",
      "Epoch 276/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0054 - val_loss: 0.0058\n",
      "Epoch 277/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0047 - val_loss: 0.0067\n",
      "Epoch 278/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0050 - val_loss: 0.0060\n",
      "Epoch 279/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0048 - val_loss: 0.0061\n",
      "Epoch 280/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0054 - val_loss: 0.0058\n",
      "Epoch 281/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0054 - val_loss: 0.0061\n",
      "Epoch 282/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0050 - val_loss: 0.0065\n",
      "Epoch 283/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0052 - val_loss: 0.0064\n",
      "Epoch 284/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0049 - val_loss: 0.0061\n",
      "Epoch 285/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0055 - val_loss: 0.0059\n",
      "Epoch 286/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0057 - val_loss: 0.0061\n",
      "Epoch 287/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0055 - val_loss: 0.0067\n",
      "Epoch 288/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0051 - val_loss: 0.0062\n",
      "Epoch 289/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0057 - val_loss: 0.0077\n",
      "Epoch 290/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0047 - val_loss: 0.0064\n",
      "Epoch 291/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0051 - val_loss: 0.0059\n",
      "Epoch 292/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0056 - val_loss: 0.0063\n",
      "Epoch 293/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0052 - val_loss: 0.0063\n",
      "Epoch 294/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0052 - val_loss: 0.0061\n",
      "Epoch 295/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0059 - val_loss: 0.0074\n",
      "Epoch 296/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0049 - val_loss: 0.0065\n",
      "Epoch 297/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0049 - val_loss: 0.0061\n",
      "Epoch 298/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0054 - val_loss: 0.0065\n",
      "Epoch 299/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0047 - val_loss: 0.0059\n",
      "Epoch 300/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0049 - val_loss: 0.0060\n",
      "Epoch 301/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0046 - val_loss: 0.0070\n",
      "Epoch 302/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0049 - val_loss: 0.0060\n",
      "Epoch 303/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0048 - val_loss: 0.0059\n",
      "Epoch 304/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0052 - val_loss: 0.0059\n",
      "Epoch 305/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0054 - val_loss: 0.0060\n",
      "Epoch 306/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0048 - val_loss: 0.0062\n",
      "Epoch 307/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0050 - val_loss: 0.0060\n",
      "Epoch 308/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0049 - val_loss: 0.0058\n",
      "Epoch 309/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0048 - val_loss: 0.0058\n",
      "Epoch 310/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0047 - val_loss: 0.0113\n",
      "Epoch 311/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0050 - val_loss: 0.0059\n",
      "Epoch 312/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0049 - val_loss: 0.0060\n",
      "Epoch 313/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0050 - val_loss: 0.0089\n",
      "Epoch 314/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0053 - val_loss: 0.0065\n",
      "Epoch 315/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0048 - val_loss: 0.0071\n",
      "Epoch 316/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0054 - val_loss: 0.0062\n",
      "Epoch 317/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0045 - val_loss: 0.0061\n",
      "Epoch 318/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0045 - val_loss: 0.0061\n",
      "Epoch 319/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0046 - val_loss: 0.0062\n",
      "Epoch 320/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0050 - val_loss: 0.0075\n",
      "Epoch 321/1000\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.0044 - val_loss: 0.0064\n",
      "Epoch 322/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0050 - val_loss: 0.0073\n",
      "Epoch 323/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0050 - val_loss: 0.0058\n",
      "Epoch 324/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0045 - val_loss: 0.0057\n",
      "Epoch 325/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0047 - val_loss: 0.0056\n",
      "Epoch 326/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0052 - val_loss: 0.0056\n",
      "Epoch 327/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0048 - val_loss: 0.0056\n",
      "Epoch 328/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0054 - val_loss: 0.0060\n",
      "Epoch 329/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0046 - val_loss: 0.0068\n",
      "Epoch 330/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0048 - val_loss: 0.0091\n",
      "Epoch 331/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0048 - val_loss: 0.0106\n",
      "Epoch 332/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0049 - val_loss: 0.0096\n",
      "Epoch 333/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0055 - val_loss: 0.0071\n",
      "Epoch 334/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0056 - val_loss: 0.0090\n",
      "Epoch 335/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0056 - val_loss: 0.0070\n",
      "Epoch 336/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0054 - val_loss: 0.0059\n",
      "Epoch 337/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0054 - val_loss: 0.0061\n",
      "Epoch 338/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0053 - val_loss: 0.0065\n",
      "Epoch 339/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0051 - val_loss: 0.0067\n",
      "Epoch 340/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0046 - val_loss: 0.0068\n",
      "Epoch 341/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0047 - val_loss: 0.0066\n",
      "Epoch 342/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0044 - val_loss: 0.0072\n",
      "Epoch 343/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0050 - val_loss: 0.0077\n",
      "Epoch 344/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0049 - val_loss: 0.0080\n",
      "Epoch 345/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0049 - val_loss: 0.0086\n",
      "Epoch 346/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0047 - val_loss: 0.0074\n",
      "Epoch 347/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0056 - val_loss: 0.0063\n",
      "Epoch 348/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0051 - val_loss: 0.0060\n",
      "Epoch 349/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0059 - val_loss: 0.0063\n",
      "Epoch 350/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0051 - val_loss: 0.0099\n",
      "Epoch 351/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0055 - val_loss: 0.0066\n",
      "Epoch 352/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0051 - val_loss: 0.0057\n",
      "Epoch 353/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0048 - val_loss: 0.0063\n",
      "Epoch 354/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0054 - val_loss: 0.0080\n",
      "Epoch 355/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0051 - val_loss: 0.0078\n",
      "Epoch 356/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0052 - val_loss: 0.0074\n",
      "Epoch 357/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0044 - val_loss: 0.0066\n",
      "Epoch 358/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0048 - val_loss: 0.0062\n",
      "Epoch 359/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0052 - val_loss: 0.0060\n",
      "Epoch 360/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0045 - val_loss: 0.0060\n",
      "Epoch 361/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0047 - val_loss: 0.0057\n",
      "Epoch 362/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0046 - val_loss: 0.0062\n",
      "Epoch 363/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0047 - val_loss: 0.0062\n",
      "Epoch 364/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0045 - val_loss: 0.0062\n",
      "Epoch 365/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0051 - val_loss: 0.0057\n",
      "Epoch 366/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0051 - val_loss: 0.0058\n",
      "Epoch 367/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0044 - val_loss: 0.0058\n",
      "Epoch 368/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0045 - val_loss: 0.0059\n",
      "Epoch 369/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0045 - val_loss: 0.0057\n",
      "Epoch 370/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0049 - val_loss: 0.0061\n",
      "Epoch 371/1000\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0044 - val_loss: 0.0057\n",
      "Epoch 372/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0046 - val_loss: 0.0061\n",
      "Epoch 373/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0050 - val_loss: 0.0057\n",
      "Epoch 374/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0047 - val_loss: 0.0056\n",
      "Epoch 375/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0050 - val_loss: 0.0064\n",
      "Epoch 376/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0048 - val_loss: 0.0060\n",
      "Epoch 377/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0047 - val_loss: 0.0070\n",
      "Epoch 378/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0051 - val_loss: 0.0069\n",
      "Epoch 379/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0052 - val_loss: 0.0060\n",
      "Epoch 380/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0047 - val_loss: 0.0058\n",
      "Epoch 381/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0051 - val_loss: 0.0057\n",
      "Epoch 382/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0053 - val_loss: 0.0061\n",
      "Epoch 383/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0048 - val_loss: 0.0069\n",
      "Epoch 384/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0054 - val_loss: 0.0072\n",
      "Epoch 385/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0047 - val_loss: 0.0087\n",
      "Epoch 386/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0046 - val_loss: 0.0069\n",
      "Epoch 387/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0046 - val_loss: 0.0087\n",
      "Epoch 388/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0045 - val_loss: 0.0066\n",
      "Epoch 389/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0046 - val_loss: 0.0107\n",
      "Epoch 390/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0047 - val_loss: 0.0129\n",
      "Epoch 391/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0052 - val_loss: 0.0069\n",
      "Epoch 392/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0046 - val_loss: 0.0056\n",
      "Epoch 393/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0049 - val_loss: 0.0062\n",
      "Epoch 394/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0055 - val_loss: 0.0065\n",
      "Epoch 395/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0054 - val_loss: 0.0055\n",
      "Epoch 396/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0053 - val_loss: 0.0057\n",
      "Epoch 397/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0049 - val_loss: 0.0064\n",
      "Epoch 398/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0048 - val_loss: 0.0063\n",
      "Epoch 399/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0047 - val_loss: 0.0066\n",
      "Epoch 400/1000\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.0047\n",
      "Epoch 00400: saving model to saved_models/latent2/cp-0400.h5\n",
      "6/6 [==============================] - 1s 96ms/step - loss: 0.0047 - val_loss: 0.0070\n",
      "Epoch 401/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0048 - val_loss: 0.0066\n",
      "Epoch 402/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0049 - val_loss: 0.0059\n",
      "Epoch 403/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0051 - val_loss: 0.0059\n",
      "Epoch 404/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0046 - val_loss: 0.0055\n",
      "Epoch 405/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0051 - val_loss: 0.0058\n",
      "Epoch 406/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0049 - val_loss: 0.0063\n",
      "Epoch 407/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0045 - val_loss: 0.0063\n",
      "Epoch 408/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0052 - val_loss: 0.0058\n",
      "Epoch 409/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0049 - val_loss: 0.0062\n",
      "Epoch 410/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0053 - val_loss: 0.0059\n",
      "Epoch 411/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0052 - val_loss: 0.0066\n",
      "Epoch 412/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0047 - val_loss: 0.0056\n",
      "Epoch 413/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0048 - val_loss: 0.0059\n",
      "Epoch 414/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0046 - val_loss: 0.0101\n",
      "Epoch 415/1000\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.0053 - val_loss: 0.0071\n",
      "Epoch 416/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0046 - val_loss: 0.0068\n",
      "Epoch 417/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0048 - val_loss: 0.0058\n",
      "Epoch 418/1000\n",
      "6/6 [==============================] - 0s 69ms/step - loss: 0.0047 - val_loss: 0.0056\n",
      "Epoch 419/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0045 - val_loss: 0.0059\n",
      "Epoch 420/1000\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.0047 - val_loss: 0.0060\n",
      "Epoch 421/1000\n",
      "5/6 [========================>.....] - ETA: 0s - loss: 0.0047Restoring model weights from the end of the best epoch.\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.0047 - val_loss: 0.0068\n",
      "Epoch 00421: early stopping\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Train VAE model with callbacks \"\"\"\n",
    "history = ae.fit(trainX, trainX, \n",
    "                  batch_size = batch_size, \n",
    "                  epochs = epochs, \n",
    "                  callbacks=[initial_checkpoint, checkpoint, EarlyStoppingAtMinLoss()],\n",
    "                  #callbacks=[initial_checkpoint, checkpoint],\n",
    "                  shuffle=True,\n",
    "                  validation_data=(valX, valX)\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step size: 6.0\n"
     ]
    }
   ],
   "source": [
    "# In this GPU implementation, it shows the number of batches/iterations for one epoch (and not the training samples), which is:\n",
    "print (\"Step size:\", np.ceil(trainX.shape[0]/batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Save the entire model \"\"\"\n",
    "# Save notes about hyperparameters used:\n",
    "with open(f'{SAVE_FOLDER}/notes.txt', 'w') as f:\n",
    "    f.write(f'Epochs: {epochs} \\n')\n",
    "    f.write(f'Batch size: {batch_size} \\n')\n",
    "    f.write(f'Latent dimension: {latent_dim} \\n')\n",
    "    f.write(f'Filter sizes: {filters} \\n')\n",
    "    f.write(f'img_width: {img_width} \\n')\n",
    "    f.write(f'img_height: {img_height} \\n')\n",
    "    f.write(f'num_channels: {num_channels}')\n",
    "    f.close()\n",
    "\n",
    "# Save encoder weights:\n",
    "encoder.save_weights(f'{SAVE_FOLDER}/ENCODER_WEIGHTS.h5')\n",
    "\n",
    "# Save the total AE model, i.e. its weights:\n",
    "ae.save_weights(f'{SAVE_FOLDER}/AE_WEIGHTS.h5')\n",
    "\n",
    "# Save the history at each epochs (cost of train and validation sets):\n",
    "np.save(f'{SAVE_FOLDER}/HISTORY.npy', history.history)\n",
    "\n",
    "# Save exact training, validation and testing data set (as numpy arrays):\n",
    "np.save(f'{SAVE_FOLDER}/trainX.npy', trainX)\n",
    "np.save(f'{SAVE_FOLDER}/valX.npy', valX)\n",
    "np.save(f'{SAVE_FOLDER}/testAllX.npy', testAllX)\n",
    "np.save(f'{SAVE_FOLDER}/testAllY.npy', testAllY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate training process\n",
    "\n",
    "Now that the training process is complete, lets evaluate the average loss of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 936x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set_style(\"darkgrid\")\n",
    "plt.figure(figsize=(13, 6))\n",
    "\n",
    "# summarize history for loss:\n",
    "plt.plot(history.history['loss'], color='blue', label='train')\n",
    "plt.plot(history.history['val_loss'], color='blue', alpha=0.4, label='validation')\n",
    "plt.xlabel('Epoch', fontsize=16)\n",
    "plt.ylabel('Cost', fontsize=16)\n",
    "plt.tick_params(axis='both', which='major', labelsize=14)\n",
    "plt.title('Average Loss During Training', fontsize=18)\n",
    "#plt.xticks(range(0,epochs))\n",
    "plt.legend(loc='upper right', fontsize=16)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
