{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Autoencoder: Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import axis, colorbar, imshow, show, figure, subplot\n",
    "from matplotlib import cm\n",
    "import matplotlib as mpl\n",
    "mpl.rc('axes', labelsize=18)\n",
    "mpl.rc('xtick', labelsize=16)\n",
    "mpl.rc('ytick', labelsize=16)\n",
    "%matplotlib inline\n",
    "\n",
    "import glob\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "#import cv2\n",
    "from PIL import Image\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "## ----- GPU ------------------------------------\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'    # use of GPU 0 (ERDA) (use before importing torch or tensorflow/keeas)\n",
    "## ----------------------------------------------\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.layers import Conv2D, Conv2DTranspose, Input, Flatten, Dense, Lambda, Reshape\n",
    "from keras.layers import BatchNormalization, ReLU, LeakyReLU\n",
    "from keras.models import Model\n",
    "from keras.losses import binary_crossentropy, mse\n",
    "from keras.activations import relu\n",
    "from keras.callbacks import Callback, ModelCheckpoint\n",
    "from keras import backend as K                         #contains calls for tensor manipulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n",
      "2.4.3\n"
     ]
    }
   ],
   "source": [
    "print (tf.__version__)\n",
    "print (keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NORMAL_TRAIN_AUG:       /home/jovyan/work/Speciale/FoodSorting/generated_dataset/PotatoNew//DataAugmentation/NormalTrainAugmentations\n",
      "NORMAL_VALIDATION_AUG:  /home/jovyan/work/Speciale/FoodSorting/generated_dataset/PotatoNew//DataAugmentation/NormalValidationAugmentations\n",
      "NORMAL_TEST_AUG:        /home/jovyan/work/Speciale/FoodSorting/generated_dataset/PotatoNew//DataAugmentation/NormalTestAugmentations\n",
      "ANOMALY_AUG:            /home/jovyan/work/Speciale/FoodSorting/generated_dataset/PotatoNew//DataAugmentation/AnomalyAugmentations\n"
     ]
    }
   ],
   "source": [
    "from utils_vae_potatonew_paths import *\n",
    "\n",
    "# Get work directions for augmentated data set:\n",
    "print (\"NORMAL_TRAIN_AUG:      \", NORMAL_TRAIN_AUG)\n",
    "print (\"NORMAL_VALIDATION_AUG: \", NORMAL_VAL_AUG)\n",
    "print (\"NORMAL_TEST_AUG:       \", NORMAL_TEST_AUG)\n",
    "print (\"ANOMALY_AUG:           \", ANOMALY_AUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Which Folder The Models Are Saved In: saved_models/latent32\n"
     ]
    }
   ],
   "source": [
    "# What latent dimension and filter size are we using?:\n",
    "latent_dim = 32\n",
    "filters    = 32\n",
    "\n",
    "# Get work direction for saving files:\n",
    "SAVE_FOLDER = f'saved_models/latent{latent_dim}'\n",
    "print (\"Which Folder The Models Are Saved In:\", SAVE_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] deleting existing files in folder...\n",
      "         done in 0.009 minutes\n"
     ]
    }
   ],
   "source": [
    "# Delete all existing files in folder where models are saved:\n",
    "print(\"[INFO] deleting existing files in folder...\")\n",
    "t0 = time()\n",
    "\n",
    "files = glob.glob(f'{SAVE_FOLDER}/*')\n",
    "for f in files:\n",
    "    os.remove(f)\n",
    "\n",
    "print (\"         done in %0.3f minutes\" % ((time() - t0)/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_VAE_AE import get_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Investigation of Ground Truths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of \"normal\" training images is:     1683\n",
      "Number of \"normal\" validation images is:   82\n",
      "Number of \"normal\" test images is:         410\n",
      "Number of \"anomaly\" images is:             462\n"
     ]
    }
   ],
   "source": [
    "# Glob the directories and get the lists of good and bad images\n",
    "good_train_fns = [f for f in os.listdir(NORMAL_TRAIN_AUG) if not f.startswith('.')]\n",
    "good_val_fns = [f for f in os.listdir(NORMAL_VAL_AUG) if not f.startswith('.')]\n",
    "good_test_fns = [f for f in os.listdir(NORMAL_TEST_AUG) if not f.startswith('.')]\n",
    "bad_fns = [f for f in os.listdir(ANOMALY_AUG) if not f.startswith('.')]\n",
    "\n",
    "print('Number of \"normal\" training images is:     {}'.format(len(good_train_fns)))\n",
    "print('Number of \"normal\" validation images is:   {}'.format(len(good_val_fns)))\n",
    "print('Number of \"normal\" test images is:         {}'.format(len(good_test_fns)))\n",
    "print('Number of \"anomaly\" images is:             {}'.format(len(bad_fns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Procentage of normal samples (ground truth):   82.48 %\n",
      "> Procentage of anomaly samples (ground truth):  17.52 %\n"
     ]
    }
   ],
   "source": [
    "# compute procentage of bad images vs good images:\n",
    "normal_fns = len(good_train_fns) + len(good_val_fns) + len(good_test_fns)\n",
    "anomaly_fns = len(bad_fns)\n",
    "print(f\"> Procentage of normal samples (ground truth):   {(normal_fns / (normal_fns + anomaly_fns)) * 100:.2f} %\")\n",
    "print(f\"> Procentage of anomaly samples (ground truth):  {(anomaly_fns / (normal_fns + anomaly_fns)) * 100:.2f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Normal Data\n",
    "\n",
    "Load normal samples in pre-splitted data sets: train, valdiation and test.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading normal training images...\n",
      "          done\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] loading normal training images...\")\n",
    "#trainX = [cv2.imread(file, 0) for file in glob.glob(f\"{NORMAL_TRAIN_AUG}/*.png\")]  # read as grayscale\n",
    "trainX = [np.asarray(Image.open(file)) for file in glob.glob(f\"{NORMAL_TRAIN_AUG}/*.png\")]\n",
    "trainX = np.array(trainX)\n",
    "print(\"          done\")\n",
    "\n",
    "# create a corresponding list of labels:\n",
    "trainY = get_labels(trainX, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading normal validation images...\n",
      "          done\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] loading normal validation images...\")\n",
    "#x_normal_val = [cv2.imread(file, 0) for file in glob.glob(f\"{NORMAL_VAL_AUG}/*.png\")]  # read as grayscale\n",
    "x_normal_val = [np.asarray(Image.open(file)) for file in glob.glob(f\"{NORMAL_VAL_AUG}/*.png\")]\n",
    "x_normal_val = np.array(x_normal_val)\n",
    "print(\"          done\")\n",
    "\n",
    "# create a corresponding list of labels:\n",
    "y_normal_val = get_labels(x_normal_val, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading normal testing images...\n",
      "          done\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] loading normal testing images...\")\n",
    "#x_normal_test = [cv2.imread(file, 0) for file in glob.glob(f\"{NORMAL_TEST_AUG}/*.png\")]  # read as grayscale\n",
    "x_normal_test = [np.asarray(Image.open(file)) for file in glob.glob(f\"{NORMAL_TEST_AUG}/*.png\")]\n",
    "x_normal_test = np.array(x_normal_test)\n",
    "print(\"          done\")\n",
    "\n",
    "# create a corresponding list of labels:\n",
    "y_normal_test = get_labels(x_normal_test, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Anomaly Data\n",
    "\n",
    "\n",
    "Load anomaly samples in its singular training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading anomaly images...\n",
      "          done\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] loading anomaly images...\")\n",
    "#x_anomaly = [cv2.imread(file, 0) for file in glob.glob(f\"{ANOMALY_AUG}/*.png\")]  # read as grayscale\n",
    "x_anomaly = [np.asarray(Image.open(file)) for file in glob.glob(f\"{ANOMALY_AUG}/*.png\")]\n",
    "x_anomaly = np.array(x_anomaly)\n",
    "print(\"          done\")\n",
    "\n",
    "# create a corresponding list of labels:\n",
    "y_anomaly = get_labels(x_anomaly, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine normal samples and anomaly samples and split them into training, validation and test sets\n",
    "\n",
    "`label 0` == Normal Samples\n",
    "\n",
    "`label 1` == Anomaly Samples\n",
    "\n",
    "Train and Validation sets only consists of `Normal Data`, aka. `label 0`.\n",
    "\n",
    "Testing sets consists of both  `Normal Data` (aka. `label 0`) and `Anomaly Data` (aka. `label 1`)\n",
    "\n",
    "________________\n",
    "\n",
    "Due to the very sparse original (preprossed) KitKat Chunky data, the validation set will be a mix of normal testing and validation data. The normal testing set will consists of all validation and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine pre-loaded validation and testing set into a new testing set:\n",
    "testvalX = np.vstack((x_normal_val, x_normal_test))\n",
    "testvalY = get_labels(testvalX, 0)\n",
    "\n",
    "# randomly split in order to make new validation set:\n",
    "NoUsageX, valX, NoUsageY, valY = train_test_split(testvalX, testvalY, test_size=0.35, random_state=4345672)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine pre-loaded validation and testing set into a new testing set:\n",
    "testNormalX = np.vstack((x_normal_val, x_normal_test))\n",
    "testNormalY = get_labels(testNormalX, 0)\n",
    "\n",
    "# anomaly class (only used for testing):\n",
    "testAnomalyX, testAnomalyY = x_anomaly, y_anomaly\n",
    "\n",
    "# Combine all testing data in one array:\n",
    "testAllX = np.vstack((testNormalX, testAnomalyX))\n",
    "testAllY = np.hstack((testNormalY, testAnomalyY))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Data Dimensions and Distributions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect data shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:\n",
      " > images: (1683, 128, 128)\n",
      " > labels: (1683,)\n",
      "\n",
      "Validation set:\n",
      " > images: (173, 128, 128)\n",
      " > labels: (173,)\n",
      "\n",
      "Test set:\n",
      " > images: (954, 128, 128)\n",
      " > labels: (954,)\n",
      "\t'Normal' test set:\n",
      "\t  > images: (492, 128, 128)\n",
      "\t  > labels: (492,)\n",
      "\t'Anomaly' test set:\n",
      "\t  > images: (462, 128, 128)\n",
      "\t  > labels: (462,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set:\")\n",
    "print(\" > images:\", trainX.shape)\n",
    "print(\" > labels:\", trainY.shape)\n",
    "print (\"\")\n",
    "\n",
    "print(\"Validation set:\")\n",
    "print(\" > images:\", valX.shape)\n",
    "print(\" > labels:\", valY.shape)\n",
    "print (\"\")\n",
    "\n",
    "print(\"Test set:\")\n",
    "print(\" > images:\", testAllX.shape)\n",
    "print(\" > labels:\", testAllY.shape)\n",
    "print(\"\\t'Normal' test set:\")\n",
    "print(\"\\t  > images:\", testNormalX.shape)\n",
    "print(\"\\t  > labels:\", testNormalY.shape)\n",
    "print(\"\\t'Anomaly' test set:\")\n",
    "print(\"\\t  > images:\", testAnomalyX.shape)\n",
    "print(\"\\t  > labels:\", testAnomalyY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procentage of training samples: 71.68 %\n",
      "Procentage of validation samples: 7.37 %\n",
      "Procentage of (only normal) testing samples: 20.95 %\n"
     ]
    }
   ],
   "source": [
    "# compute procentage between training, validation and test data sets (only normal data):\n",
    "print(f\"Procentage of training samples: {(len(trainX) / (len(trainX) + len(valX) + len(testNormalX))) * 100:.2f} %\")\n",
    "print(f\"Procentage of validation samples: {(len(valX) / (len(trainX) + len(valX) + len(testNormalX))) * 100:.2f} %\")\n",
    "print(f\"Procentage of (only normal) testing samples: {(len(testNormalX) / (len(trainX) + len(valX) + len(testNormalX))) * 100:.2f} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procentage of training samples: 59.89 %\n",
      "Procentage of validation samples: 6.16 %\n",
      "Procentage of (total) testing samples: 33.95 %\n"
     ]
    }
   ],
   "source": [
    "# compute procentage between training, validation and test data sets:\n",
    "print(f\"Procentage of training samples: {(len(trainX) / (len(trainX) + len(valX) + len(testAllX))) * 100:.2f} %\")\n",
    "print(f\"Procentage of validation samples: {(len(valX) / (len(trainX) + len(valX) + len(testAllX))) * 100:.2f} %\")\n",
    "print(f\"Procentage of (total) testing samples: {(len(testAllX) / (len(trainX) + len(valX) + len(testAllX))) * 100:.2f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for (un)balanced data\n",
    "\n",
    "In an ideal world the data should be balanced. However in the real world we expect there being around ~$99 \\%$ good samples and only ~$1 \\%$ bad samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x288 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels = ['0: normal', '1: anomaly']\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(16,4))\n",
    "ax[0].hist(trainY, align=\"left\"); ax[0].set_title('train', fontsize=18); ax[0].set_xticks([0,1]); ax[0].set_xticklabels(labels); ax[0].tick_params(axis='both', which='major', labelsize=16); ax[0].set_xlim(-0.5, 1.5);\n",
    "ax[1].hist(valY, align=\"left\"); ax[1].set_title('validation', fontsize=18); ax[1].set_xticks([0,1]); ax[1].set_xticklabels(labels); ax[1].tick_params(axis='both', which='major', labelsize=16); ax[1].set_xlim(-0.5, 1.5);\n",
    "ax[2].hist(testAllY, align=\"left\"); ax[2].set_title('test', fontsize=18); ax[2].set_xticks([0,1]); ax[2].set_xticklabels(labels); ax[2].tick_params(axis='both', which='major', labelsize=16); ax[2].set_xlim(-0.5, 1.5);\n",
    "\n",
    "ax[0].set_ylabel(\"Number of images\", fontsize=18)\n",
    "ax[1].set_xlabel(\"Class Labels\", fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procentage of normal test samples: 51.57 %\n",
      "Procentage of total anomaly test samples: 48.43 %\n"
     ]
    }
   ],
   "source": [
    "# compute procentage of normal test images vs total anomaly test images:\n",
    "print(f\"Procentage of normal test samples: {(len(testNormalX) / (len(testNormalX) + len(testAnomalyX))) * 100:.2f} %\")\n",
    "print(f\"Procentage of total anomaly test samples: {(len(testAnomalyX) / (len(testNormalX) + len(testAnomalyX))) * 100:.2f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling\n",
    "\n",
    "Only normalization has been used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data configuration\n",
    "img_width, img_height = trainX.shape[1], trainX.shape[2]      # input image dimensions\n",
    "num_channels = 1                                              # gray-channel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization\n",
    "\n",
    "Neural Networks learns faster with normalized data."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def normalization(originals):\n",
    "    \"\"\"   \n",
    "    Function used to normalize images stored in an array to domain [0,255]\n",
    "    \"\"\"\n",
    "    normalized = []\n",
    "    for x in range(len(originals)):\n",
    "        normalized.append(cv2.normalize(originals[x], None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8U))\n",
    "    return np.array(normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize to be in the [0, 255] range (in the augmentation stage, the normalization was partly ruined..):\n",
    "#trainX   = normalization(trainX)\n",
    "#valX     = normalization(valX)\n",
    "#testAllX = normalization(testAllX)\n",
    "\n",
    "# Normalize to be in the [0, 1] range:\n",
    "trainX = trainX.astype('float32') / 255.\n",
    "valX = valX.astype('float32') / 255.\n",
    "testAllX = testAllX.astype('float32') / 255.\n",
    "\n",
    "# reshape data to keras inputs --> (n_samples, img_rows, img_cols, n_channels):\n",
    "trainX = trainX.reshape(trainX.shape[0], img_height, img_width, num_channels)\n",
    "valX = valX.reshape(valX.shape[0], img_height, img_width, num_channels)\n",
    "testAllX = testAllX.reshape(testAllX.shape[0], img_height, img_width, num_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture and Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import architecture of VAE model and its hyperparameters:\n",
    "from J32_VAE_model import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Encoder Part*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_input (InputLayer)      [(None, 128, 128, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 64, 64, 32)   320         encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)         (None, 64, 64, 32)   0           conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 32, 32, 32)   9248        leaky_re_lu[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)       (None, 32, 32, 32)   0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 16, 16, 32)   9248        leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)       (None, 16, 16, 32)   0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 8, 8, 32)     9248        leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)       (None, 8, 8, 32)     0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 4, 4, 32)     9248        leaky_re_lu_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)       (None, 4, 4, 32)     0           conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 512)          0           leaky_re_lu_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "latent (Dense)                  (None, 200)          102600      flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)       (None, 200)          0           latent[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "z_mean (Dense)                  (None, 32)           6432        leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "z_log_mean (Dense)              (None, 32)           6432        leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "z (Lambda)                      (None, 32)           0           z_mean[0][0]                     \n",
      "                                                                 z_log_mean[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 152,776\n",
      "Trainable params: 152,776\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Decoder Part*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "decoder_input (InputLayer)   [(None, 32)]              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               16896     \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 4, 4, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose (Conv2DTran (None, 8, 8, 32)          9248      \n",
      "_________________________________________________________________\n",
      "re_lu (ReLU)                 (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 16, 16, 32)        9248      \n",
      "_________________________________________________________________\n",
      "re_lu_1 (ReLU)               (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTr (None, 32, 32, 32)        9248      \n",
      "_________________________________________________________________\n",
      "re_lu_2 (ReLU)               (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTr (None, 64, 64, 32)        9248      \n",
      "_________________________________________________________________\n",
      "re_lu_3 (ReLU)               (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_4 (Conv2DTr (None, 128, 128, 32)      9248      \n",
      "_________________________________________________________________\n",
      "re_lu_4 (ReLU)               (None, 128, 128, 32)      0         \n",
      "_________________________________________________________________\n",
      "decoder_output (Conv2DTransp (None, 128, 128, 1)       289       \n",
      "=================================================================\n",
      "Total params: 63,425\n",
      "Trainable params: 63,425\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Total VAE Model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vae\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder_input (InputLayer)   [(None, 128, 128, 1)]     0         \n",
      "_________________________________________________________________\n",
      "encoder (Functional)         [(None, 32), (None, 32),  152776    \n",
      "_________________________________________________________________\n",
      "decoder (Functional)         (None, 128, 128, 1)       63425     \n",
      "=================================================================\n",
      "Total params: 216,201\n",
      "Trainable params: 216,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vae.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile and train model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Hyperparameters \"\"\"\n",
    "batch_size  = 256\n",
    "epochs      = 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Checkpoint callback options: save model at 1'st epoch (inital model)\n",
    "\n",
    "https://stackoverflow.com/questions/54323960/save-keras-model-at-specific-epochs\n",
    "\"\"\"\n",
    "\n",
    "class CustomSaver(keras.callbacks.Callback):\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        if epoch == 1:\n",
    "            self.model.save_weights(f\"{SAVE_FOLDER}/cp-0001.h5\")\n",
    "            \n",
    "# create and use callback:\n",
    "initial_checkpoint = CustomSaver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Checkpoint callback options: save model at every k'te epochs\n",
    "\"\"\"\n",
    "# Include the epoch in the file name (uses `str.format`):\n",
    "checkpoint_path = \"saved_models/latent32/cp-{epoch:04d}.h5\"\n",
    "\n",
    "# Create a callback that saves the model's weights every k'te epochs:\n",
    "checkpoint = ModelCheckpoint(filepath = checkpoint_path,\n",
    "                             monitor='loss',           # name of the metrics to monitor\n",
    "                             verbose=1, \n",
    "                             #save_best_only=False,     # if False, the best model will not be overridden.\n",
    "                             save_weights_only=True,   # if True, only the weights of the models will be saved.\n",
    "                                                        # if False, the whole models will be saved.\n",
    "                             #mode='auto', \n",
    "                             period=200                  # save after every k'te epochs\n",
    "                            )\n",
    "\n",
    "# Save the weights using the `checkpoint_path` format\n",
    "vae.save_weights(checkpoint_path.format(epoch=0))          # save for zero epoch (inital)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Checkpoint callback options: Early stopping\n",
    "https://www.tensorflow.org/guide/keras/custom_callback\n",
    "\"\"\"\n",
    "\n",
    "class EarlyStoppingAtMinLoss(keras.callbacks.Callback):\n",
    "    \"\"\"Stop training when the loss is at its min, i.e. the loss stops decreasing.\n",
    "\n",
    "  Arguments:\n",
    "      patience: Number of epochs to wait after min has been hit. After this\n",
    "      number of no improvement, training stops.\n",
    "  \"\"\"\n",
    "\n",
    "    def __init__(self, patience=25):\n",
    "        super(EarlyStoppingAtMinLoss, self).__init__()\n",
    "        self.patience = patience\n",
    "        # best_weights to store the weights at which the minimum loss occurs.\n",
    "        self.best_weights = None\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        # The number of epoch it has waited when loss is no longer minimum.\n",
    "        self.wait = 0\n",
    "        # The epoch the training stops at.\n",
    "        self.stopped_epoch = 0\n",
    "        # Initialize the best as infinity.\n",
    "        self.best = np.Inf\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        current = logs.get(\"loss\")\n",
    "        if np.less(current, self.best):\n",
    "            self.best = current\n",
    "            self.wait = 0\n",
    "            # Record the best weights if current results is better (less).\n",
    "            self.best_weights = self.model.get_weights()\n",
    "        else:\n",
    "            self.wait += 1\n",
    "            if self.wait >= self.patience:\n",
    "                self.stopped_epoch = epoch\n",
    "                self.model.stop_training = True\n",
    "                print(\"Restoring model weights from the end of the best epoch.\")\n",
    "                self.model.set_weights(self.best_weights)\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        if self.stopped_epoch > 0:\n",
    "            print(\"Epoch %05d: early stopping\" % (self.stopped_epoch + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Create and Compile Model\"\"\"\n",
    "# import architecture of VAE model and its hyperparameters. learning rate choosen from graph above.\n",
    "vae = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      "7/7 [==============================] - 31s 3s/step - loss: 3254.3674 - val_loss: 3159.4465\n",
      "Epoch 2/2000\n",
      "7/7 [==============================] - 1s 137ms/step - loss: 3055.5668 - val_loss: 2560.0764\n",
      "Epoch 3/2000\n",
      "7/7 [==============================] - 1s 133ms/step - loss: 2318.3921 - val_loss: 1870.3024\n",
      "Epoch 4/2000\n",
      "7/7 [==============================] - 1s 131ms/step - loss: 1711.8773 - val_loss: 1400.0658\n",
      "Epoch 5/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 1346.2728 - val_loss: 1196.4106\n",
      "Epoch 6/2000\n",
      "7/7 [==============================] - 1s 132ms/step - loss: 1177.7114 - val_loss: 1078.8882\n",
      "Epoch 7/2000\n",
      "7/7 [==============================] - 1s 129ms/step - loss: 1072.6851 - val_loss: 1005.2291\n",
      "Epoch 8/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 1015.4078 - val_loss: 936.9968\n",
      "Epoch 9/2000\n",
      "7/7 [==============================] - 1s 130ms/step - loss: 916.8646 - val_loss: 885.5687\n",
      "Epoch 10/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 861.6151 - val_loss: 817.9949\n",
      "Epoch 11/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 816.3740 - val_loss: 802.5820\n",
      "Epoch 12/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 798.1075 - val_loss: 790.5083\n",
      "Epoch 13/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 782.2363 - val_loss: 773.8340\n",
      "Epoch 14/2000\n",
      "7/7 [==============================] - 1s 129ms/step - loss: 771.5298 - val_loss: 769.0086\n",
      "Epoch 15/2000\n",
      "7/7 [==============================] - 1s 131ms/step - loss: 757.0176 - val_loss: 755.8652\n",
      "Epoch 16/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 750.0837 - val_loss: 740.5544\n",
      "Epoch 17/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 735.0568 - val_loss: 719.0779\n",
      "Epoch 18/2000\n",
      "7/7 [==============================] - 1s 129ms/step - loss: 710.4434 - val_loss: 703.3613\n",
      "Epoch 19/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 691.9876 - val_loss: 694.3454\n",
      "Epoch 20/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 672.7366 - val_loss: 683.4174\n",
      "Epoch 21/2000\n",
      "7/7 [==============================] - 1s 136ms/step - loss: 667.2961 - val_loss: 670.4085\n",
      "Epoch 22/2000\n",
      "7/7 [==============================] - 1s 129ms/step - loss: 658.9150 - val_loss: 660.8862\n",
      "Epoch 23/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 643.5927 - val_loss: 652.2289\n",
      "Epoch 24/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 637.6357 - val_loss: 648.5593\n",
      "Epoch 25/2000\n",
      "7/7 [==============================] - 1s 131ms/step - loss: 631.0383 - val_loss: 637.9943\n",
      "Epoch 26/2000\n",
      "7/7 [==============================] - 1s 129ms/step - loss: 625.0227 - val_loss: 635.1472\n",
      "Epoch 27/2000\n",
      "7/7 [==============================] - 1s 129ms/step - loss: 615.0790 - val_loss: 622.9553\n",
      "Epoch 28/2000\n",
      "7/7 [==============================] - 1s 130ms/step - loss: 606.9324 - val_loss: 613.4857\n",
      "Epoch 29/2000\n",
      "7/7 [==============================] - 1s 131ms/step - loss: 595.3404 - val_loss: 604.4037\n",
      "Epoch 30/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 590.5251 - val_loss: 596.0927\n",
      "Epoch 31/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 579.1071 - val_loss: 589.5621\n",
      "Epoch 32/2000\n",
      "7/7 [==============================] - 1s 129ms/step - loss: 573.3439 - val_loss: 581.6299\n",
      "Epoch 33/2000\n",
      "7/7 [==============================] - 1s 129ms/step - loss: 567.8176 - val_loss: 576.5435\n",
      "Epoch 34/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 559.8953 - val_loss: 571.9959\n",
      "Epoch 35/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 556.0625 - val_loss: 560.1078\n",
      "Epoch 36/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 546.8972 - val_loss: 554.3514\n",
      "Epoch 37/2000\n",
      "7/7 [==============================] - 1s 130ms/step - loss: 542.1229 - val_loss: 548.7607\n",
      "Epoch 38/2000\n",
      "7/7 [==============================] - 1s 129ms/step - loss: 533.3387 - val_loss: 544.2378\n",
      "Epoch 39/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 527.8999 - val_loss: 536.3986\n",
      "Epoch 40/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 523.8646 - val_loss: 536.1318\n",
      "Epoch 41/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 517.6669 - val_loss: 526.8264\n",
      "Epoch 42/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 509.8016 - val_loss: 523.5748\n",
      "Epoch 43/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 504.1510 - val_loss: 520.7679\n",
      "Epoch 44/2000\n",
      "7/7 [==============================] - 1s 129ms/step - loss: 501.1243 - val_loss: 514.2687\n",
      "Epoch 45/2000\n",
      "7/7 [==============================] - 1s 129ms/step - loss: 497.9643 - val_loss: 507.8609\n",
      "Epoch 46/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 493.0759 - val_loss: 505.5701\n",
      "Epoch 47/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 487.8803 - val_loss: 498.7970\n",
      "Epoch 48/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 483.7020 - val_loss: 495.3360\n",
      "Epoch 49/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 480.8854 - val_loss: 490.2725\n",
      "Epoch 50/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 476.7057 - val_loss: 486.3601\n",
      "Epoch 51/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 466.8251 - val_loss: 480.9157\n",
      "Epoch 52/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 466.1058 - val_loss: 477.2020\n",
      "Epoch 53/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 457.7639 - val_loss: 473.6639\n",
      "Epoch 54/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 454.9183 - val_loss: 470.7976\n",
      "Epoch 55/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 451.7948 - val_loss: 462.7660\n",
      "Epoch 56/2000\n",
      "7/7 [==============================] - 1s 129ms/step - loss: 446.8638 - val_loss: 462.7307\n",
      "Epoch 57/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 446.6857 - val_loss: 457.8532\n",
      "Epoch 58/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 441.0766 - val_loss: 457.4240\n",
      "Epoch 59/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 437.6309 - val_loss: 448.5836\n",
      "Epoch 60/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 430.3262 - val_loss: 444.2075\n",
      "Epoch 61/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 426.0543 - val_loss: 439.3902\n",
      "Epoch 62/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 423.6785 - val_loss: 434.3753\n",
      "Epoch 63/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 416.0365 - val_loss: 421.2369\n",
      "Epoch 64/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 400.5439 - val_loss: 401.4069\n",
      "Epoch 65/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 378.5185 - val_loss: 368.2493\n",
      "Epoch 66/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 346.9485 - val_loss: 333.8293\n",
      "Epoch 67/2000\n",
      "7/7 [==============================] - 1s 129ms/step - loss: 312.6077 - val_loss: 298.8364\n",
      "Epoch 68/2000\n",
      "7/7 [==============================] - 1s 129ms/step - loss: 276.2076 - val_loss: 260.9978\n",
      "Epoch 69/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 243.8639 - val_loss: 228.4868\n",
      "Epoch 70/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 213.5097 - val_loss: 200.5723\n",
      "Epoch 71/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 187.0794 - val_loss: 179.6895\n",
      "Epoch 72/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 168.4665 - val_loss: 161.2065\n",
      "Epoch 73/2000\n",
      "7/7 [==============================] - 1s 130ms/step - loss: 150.7378 - val_loss: 146.4020\n",
      "Epoch 74/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 137.6645 - val_loss: 135.7494\n",
      "Epoch 75/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 126.9285 - val_loss: 129.8069\n",
      "Epoch 76/2000\n",
      "7/7 [==============================] - 1s 130ms/step - loss: 121.1142 - val_loss: 120.6389\n",
      "Epoch 77/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 112.5159 - val_loss: 113.5342\n",
      "Epoch 78/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 107.6954 - val_loss: 107.5810\n",
      "Epoch 79/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 102.0729 - val_loss: 99.7151\n",
      "Epoch 80/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 97.4181 - val_loss: 95.7669\n",
      "Epoch 81/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 93.5016 - val_loss: 96.7888\n",
      "Epoch 82/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 92.9919 - val_loss: 91.7891\n",
      "Epoch 83/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 90.3600 - val_loss: 91.8675\n",
      "Epoch 84/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 89.9537 - val_loss: 89.4651\n",
      "Epoch 85/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 87.8096 - val_loss: 90.9350\n",
      "Epoch 86/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 88.5912 - val_loss: 88.9357\n",
      "Epoch 87/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 87.1275 - val_loss: 87.1557\n",
      "Epoch 88/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 86.0173 - val_loss: 89.5337\n",
      "Epoch 89/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 86.0614 - val_loss: 85.8163\n",
      "Epoch 90/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 85.9377 - val_loss: 86.4386\n",
      "Epoch 91/2000\n",
      "7/7 [==============================] - 1s 130ms/step - loss: 85.4590 - val_loss: 84.8971\n",
      "Epoch 92/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 83.9561 - val_loss: 84.8944\n",
      "Epoch 93/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 82.6730 - val_loss: 83.8730\n",
      "Epoch 94/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 82.6296 - val_loss: 85.2901\n",
      "Epoch 95/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 83.0757 - val_loss: 83.1218\n",
      "Epoch 96/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 82.9738 - val_loss: 82.3610\n",
      "Epoch 97/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 81.9896 - val_loss: 83.4826\n",
      "Epoch 98/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 81.5318 - val_loss: 82.7904\n",
      "Epoch 99/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 80.3054 - val_loss: 81.3222\n",
      "Epoch 100/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 80.1806 - val_loss: 80.1628\n",
      "Epoch 101/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 79.4036 - val_loss: 81.7510\n",
      "Epoch 102/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 81.1326 - val_loss: 81.4164\n",
      "Epoch 103/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 79.2588 - val_loss: 81.0902\n",
      "Epoch 104/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 78.8130 - val_loss: 80.5919\n",
      "Epoch 105/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 78.0567 - val_loss: 81.5251\n",
      "Epoch 106/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 78.4934 - val_loss: 78.4478\n",
      "Epoch 107/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 77.1913 - val_loss: 77.4423\n",
      "Epoch 108/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 77.6486 - val_loss: 82.1936\n",
      "Epoch 109/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 79.7373 - val_loss: 79.1933\n",
      "Epoch 110/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 77.1354 - val_loss: 77.4536\n",
      "Epoch 111/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 76.7992 - val_loss: 82.3156\n",
      "Epoch 112/2000\n",
      "7/7 [==============================] - 1s 129ms/step - loss: 78.0598 - val_loss: 80.7292\n",
      "Epoch 113/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 77.3205 - val_loss: 77.4270\n",
      "Epoch 114/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 76.9428 - val_loss: 77.6102\n",
      "Epoch 115/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 75.4770 - val_loss: 77.3536\n",
      "Epoch 116/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 75.0464 - val_loss: 76.3406\n",
      "Epoch 117/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 73.6715 - val_loss: 75.8275\n",
      "Epoch 118/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 74.2745 - val_loss: 75.3093\n",
      "Epoch 119/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 73.8267 - val_loss: 75.5605\n",
      "Epoch 120/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 73.2333 - val_loss: 76.2156\n",
      "Epoch 121/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 74.1228 - val_loss: 76.7426\n",
      "Epoch 122/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 73.1580 - val_loss: 74.1987\n",
      "Epoch 123/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 72.9378 - val_loss: 75.5330\n",
      "Epoch 124/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 72.6820 - val_loss: 74.5237\n",
      "Epoch 125/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 72.9797 - val_loss: 74.6026\n",
      "Epoch 126/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 71.8948 - val_loss: 73.2816\n",
      "Epoch 127/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 72.3265 - val_loss: 74.7455\n",
      "Epoch 128/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 71.4541 - val_loss: 75.5246\n",
      "Epoch 129/2000\n",
      "7/7 [==============================] - 1s 129ms/step - loss: 71.4821 - val_loss: 74.2661\n",
      "Epoch 130/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 71.4186 - val_loss: 75.9784\n",
      "Epoch 131/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 72.1286 - val_loss: 75.7146\n",
      "Epoch 132/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 72.3021 - val_loss: 72.2745\n",
      "Epoch 133/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 69.9157 - val_loss: 74.1410\n",
      "Epoch 134/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 70.9508 - val_loss: 76.0054\n",
      "Epoch 135/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 72.6530 - val_loss: 74.9180\n",
      "Epoch 136/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 71.2481 - val_loss: 73.3112\n",
      "Epoch 137/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 70.2590 - val_loss: 74.2027\n",
      "Epoch 138/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 70.3605 - val_loss: 72.7984\n",
      "Epoch 139/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 69.9582 - val_loss: 72.8843\n",
      "Epoch 140/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 69.5693 - val_loss: 72.6342\n",
      "Epoch 141/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 69.9826 - val_loss: 73.2841\n",
      "Epoch 142/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 69.7530 - val_loss: 74.8766\n",
      "Epoch 143/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 69.9759 - val_loss: 72.1264\n",
      "Epoch 144/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 68.3777 - val_loss: 72.4902\n",
      "Epoch 145/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 67.5276 - val_loss: 71.3507\n",
      "Epoch 146/2000\n",
      "7/7 [==============================] - 1s 129ms/step - loss: 67.8680 - val_loss: 71.4344\n",
      "Epoch 147/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 68.2038 - val_loss: 69.9798\n",
      "Epoch 148/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 68.2072 - val_loss: 72.2595\n",
      "Epoch 149/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 68.2173 - val_loss: 70.1895\n",
      "Epoch 150/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 68.1080 - val_loss: 70.5796\n",
      "Epoch 151/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 67.4974 - val_loss: 71.4860\n",
      "Epoch 152/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 67.4115 - val_loss: 71.1009\n",
      "Epoch 153/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 67.2043 - val_loss: 70.9512\n",
      "Epoch 154/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 67.9055 - val_loss: 72.2861\n",
      "Epoch 155/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 67.8058 - val_loss: 72.3052\n",
      "Epoch 156/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 67.1302 - val_loss: 72.7584\n",
      "Epoch 157/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 67.2230 - val_loss: 70.7912\n",
      "Epoch 158/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 66.7762 - val_loss: 70.6754\n",
      "Epoch 159/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 67.5129 - val_loss: 72.2810\n",
      "Epoch 160/2000\n",
      "7/7 [==============================] - 1s 130ms/step - loss: 67.3984 - val_loss: 70.0531\n",
      "Epoch 161/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 66.3696 - val_loss: 72.9013\n",
      "Epoch 162/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 67.1582 - val_loss: 70.4183\n",
      "Epoch 163/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 66.3360 - val_loss: 69.7841\n",
      "Epoch 164/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 65.8251 - val_loss: 70.2894\n",
      "Epoch 165/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 66.1435 - val_loss: 68.6729\n",
      "Epoch 166/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 65.2695 - val_loss: 68.8408\n",
      "Epoch 167/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 64.7080 - val_loss: 70.1611\n",
      "Epoch 168/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 65.3552 - val_loss: 68.7540\n",
      "Epoch 169/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 65.8187 - val_loss: 71.4057\n",
      "Epoch 170/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 67.5423 - val_loss: 71.3633\n",
      "Epoch 171/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 66.1478 - val_loss: 69.7748\n",
      "Epoch 172/2000\n",
      "7/7 [==============================] - 1s 129ms/step - loss: 64.6801 - val_loss: 68.2549\n",
      "Epoch 173/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 65.0466 - val_loss: 69.8269\n",
      "Epoch 174/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 64.4939 - val_loss: 70.1441\n",
      "Epoch 175/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 66.1043 - val_loss: 69.3516\n",
      "Epoch 176/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 64.9388 - val_loss: 69.4375\n",
      "Epoch 177/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 64.2964 - val_loss: 70.4351\n",
      "Epoch 178/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 64.0345 - val_loss: 69.1602\n",
      "Epoch 179/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 63.7112 - val_loss: 69.7074\n",
      "Epoch 180/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 63.5895 - val_loss: 67.6296\n",
      "Epoch 181/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 63.1731 - val_loss: 68.1239\n",
      "Epoch 182/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 63.5277 - val_loss: 70.3841\n",
      "Epoch 183/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 64.8955 - val_loss: 69.0369\n",
      "Epoch 184/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 63.7403 - val_loss: 67.9999\n",
      "Epoch 185/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 63.3396 - val_loss: 68.6386\n",
      "Epoch 186/2000\n",
      "7/7 [==============================] - 1s 129ms/step - loss: 63.1387 - val_loss: 68.1676\n",
      "Epoch 187/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 63.3616 - val_loss: 68.5471\n",
      "Epoch 188/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 63.5975 - val_loss: 66.7490\n",
      "Epoch 189/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 62.5971 - val_loss: 67.6891\n",
      "Epoch 190/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 62.7207 - val_loss: 69.0428\n",
      "Epoch 191/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 63.3302 - val_loss: 68.4240\n",
      "Epoch 192/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 63.1608 - val_loss: 68.2948\n",
      "Epoch 193/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 62.7139 - val_loss: 68.1568\n",
      "Epoch 194/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 62.8430 - val_loss: 69.4209\n",
      "Epoch 195/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 63.9444 - val_loss: 68.4087\n",
      "Epoch 196/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 62.6602 - val_loss: 68.3516\n",
      "Epoch 197/2000\n",
      "7/7 [==============================] - 1s 129ms/step - loss: 63.2206 - val_loss: 70.0552\n",
      "Epoch 198/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 63.8980 - val_loss: 67.9337\n",
      "Epoch 199/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 62.9965 - val_loss: 68.5537\n",
      "Epoch 200/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 63.0224 - val_loss: 68.5702\n",
      "\n",
      "Epoch 00200: saving model to saved_models/latent32/cp-0200.h5\n",
      "Epoch 201/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 61.8534 - val_loss: 67.0227\n",
      "Epoch 202/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 62.2410 - val_loss: 67.8317\n",
      "Epoch 203/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 61.2283 - val_loss: 66.5213\n",
      "Epoch 204/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 61.4099 - val_loss: 66.9398\n",
      "Epoch 205/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 61.2027 - val_loss: 66.8643\n",
      "Epoch 206/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 60.9093 - val_loss: 67.1732\n",
      "Epoch 207/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 61.7547 - val_loss: 67.0366\n",
      "Epoch 208/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 60.8467 - val_loss: 66.8206\n",
      "Epoch 209/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 61.4532 - val_loss: 67.7166\n",
      "Epoch 210/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 61.0158 - val_loss: 67.5191\n",
      "Epoch 211/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 61.4561 - val_loss: 66.9343\n",
      "Epoch 212/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 61.0342 - val_loss: 66.6738\n",
      "Epoch 213/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 60.9717 - val_loss: 65.8454\n",
      "Epoch 214/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 60.8243 - val_loss: 67.6133\n",
      "Epoch 215/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 61.2591 - val_loss: 68.0822\n",
      "Epoch 216/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 61.2015 - val_loss: 65.8282\n",
      "Epoch 217/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 61.0128 - val_loss: 65.6367\n",
      "Epoch 218/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 60.6607 - val_loss: 67.4907\n",
      "Epoch 219/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 59.9449 - val_loss: 65.5526\n",
      "Epoch 220/2000\n",
      "7/7 [==============================] - 1s 129ms/step - loss: 60.0404 - val_loss: 66.6390\n",
      "Epoch 221/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 60.7048 - val_loss: 66.6424\n",
      "Epoch 222/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 61.1254 - val_loss: 67.8294\n",
      "Epoch 223/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 60.1238 - val_loss: 66.4543\n",
      "Epoch 224/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 59.4685 - val_loss: 67.1330\n",
      "Epoch 225/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 59.4279 - val_loss: 67.4174\n",
      "Epoch 226/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 59.9852 - val_loss: 67.9443\n",
      "Epoch 227/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 59.7643 - val_loss: 68.0605\n",
      "Epoch 228/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 60.1451 - val_loss: 65.4733\n",
      "Epoch 229/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 59.4265 - val_loss: 67.0546\n",
      "Epoch 230/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 60.0978 - val_loss: 65.7807\n",
      "Epoch 231/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 59.8173 - val_loss: 64.7866\n",
      "Epoch 232/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 59.2389 - val_loss: 65.9426\n",
      "Epoch 233/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 59.6456 - val_loss: 66.5028\n",
      "Epoch 234/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 58.6280 - val_loss: 66.3467\n",
      "Epoch 235/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 59.2461 - val_loss: 66.5216\n",
      "Epoch 236/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 60.1837 - val_loss: 67.8075\n",
      "Epoch 237/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 59.8291 - val_loss: 65.6222\n",
      "Epoch 238/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 59.0408 - val_loss: 66.2776\n",
      "Epoch 239/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 59.5478 - val_loss: 65.1303\n",
      "Epoch 240/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 58.5460 - val_loss: 64.8308\n",
      "Epoch 241/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 58.5020 - val_loss: 66.2205\n",
      "Epoch 242/2000\n",
      "7/7 [==============================] - 1s 129ms/step - loss: 58.3571 - val_loss: 65.7561\n",
      "Epoch 243/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 58.5950 - val_loss: 66.3124\n",
      "Epoch 244/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 58.5787 - val_loss: 65.5226\n",
      "Epoch 245/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 59.0551 - val_loss: 67.6495\n",
      "Epoch 246/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 59.2318 - val_loss: 67.3358\n",
      "Epoch 247/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 59.9874 - val_loss: 68.6178\n",
      "Epoch 248/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 60.3882 - val_loss: 67.8791\n",
      "Epoch 249/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 59.7255 - val_loss: 65.8508\n",
      "Epoch 250/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 58.0952 - val_loss: 64.8113\n",
      "Epoch 251/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 57.5069 - val_loss: 64.8304\n",
      "Epoch 252/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 58.2088 - val_loss: 64.5425\n",
      "Epoch 253/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 58.0581 - val_loss: 65.3237\n",
      "Epoch 254/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 57.3887 - val_loss: 64.4972\n",
      "Epoch 255/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 57.6121 - val_loss: 65.3064\n",
      "Epoch 256/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 57.8344 - val_loss: 66.0149\n",
      "Epoch 257/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 58.1995 - val_loss: 64.4385\n",
      "Epoch 258/2000\n",
      "7/7 [==============================] - 1s 129ms/step - loss: 57.8545 - val_loss: 64.9429\n",
      "Epoch 259/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 57.5602 - val_loss: 66.0477\n",
      "Epoch 260/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 56.9934 - val_loss: 64.4542\n",
      "Epoch 261/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 57.6403 - val_loss: 64.6805\n",
      "Epoch 262/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 57.8072 - val_loss: 63.6822\n",
      "Epoch 263/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 57.3331 - val_loss: 65.1392\n",
      "Epoch 264/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 57.2065 - val_loss: 66.6897\n",
      "Epoch 265/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 57.5946 - val_loss: 65.4519\n",
      "Epoch 266/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 57.2595 - val_loss: 65.3860\n",
      "Epoch 267/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 57.5508 - val_loss: 65.6214\n",
      "Epoch 268/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 57.5756 - val_loss: 67.3097\n",
      "Epoch 269/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 57.7709 - val_loss: 65.2942\n",
      "Epoch 270/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 57.1272 - val_loss: 65.0567\n",
      "Epoch 271/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 56.8966 - val_loss: 64.2948\n",
      "Epoch 272/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 57.2056 - val_loss: 65.5888\n",
      "Epoch 273/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 56.9360 - val_loss: 64.2836\n",
      "Epoch 274/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 56.5340 - val_loss: 64.1757\n",
      "Epoch 275/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 56.3205 - val_loss: 64.1342\n",
      "Epoch 276/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 56.6709 - val_loss: 63.6236\n",
      "Epoch 277/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 56.5095 - val_loss: 65.0191\n",
      "Epoch 278/2000\n",
      "7/7 [==============================] - 1s 129ms/step - loss: 56.1989 - val_loss: 64.4952\n",
      "Epoch 279/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 56.2898 - val_loss: 64.8484\n",
      "Epoch 280/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 57.1751 - val_loss: 65.2309\n",
      "Epoch 281/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 57.1142 - val_loss: 64.8526\n",
      "Epoch 282/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 57.2241 - val_loss: 70.2302\n",
      "Epoch 283/2000\n",
      "7/7 [==============================] - 1s 130ms/step - loss: 58.0910 - val_loss: 63.9419\n",
      "Epoch 284/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 56.0178 - val_loss: 63.7546\n",
      "Epoch 285/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 55.4343 - val_loss: 63.7531\n",
      "Epoch 286/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 55.9453 - val_loss: 66.7210\n",
      "Epoch 287/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 57.2864 - val_loss: 65.2080\n",
      "Epoch 288/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 57.5518 - val_loss: 64.5012\n",
      "Epoch 289/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 56.0984 - val_loss: 63.8214\n",
      "Epoch 290/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 55.5375 - val_loss: 64.4079\n",
      "Epoch 291/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 56.0867 - val_loss: 65.4660\n",
      "Epoch 292/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 55.5397 - val_loss: 63.6370\n",
      "Epoch 293/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 55.5759 - val_loss: 64.9829\n",
      "Epoch 294/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 55.6557 - val_loss: 64.7520\n",
      "Epoch 295/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 55.4771 - val_loss: 64.1150\n",
      "Epoch 296/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 55.0269 - val_loss: 63.5432\n",
      "Epoch 297/2000\n",
      "7/7 [==============================] - 1s 129ms/step - loss: 54.9470 - val_loss: 63.6264\n",
      "Epoch 298/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 54.5157 - val_loss: 64.8930\n",
      "Epoch 299/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 55.3012 - val_loss: 63.8279\n",
      "Epoch 300/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 54.8227 - val_loss: 64.0916\n",
      "Epoch 301/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 54.9884 - val_loss: 63.4463\n",
      "Epoch 302/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 55.5157 - val_loss: 65.4595\n",
      "Epoch 303/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 55.6581 - val_loss: 63.7788\n",
      "Epoch 304/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 54.9179 - val_loss: 63.6234\n",
      "Epoch 305/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 55.8371 - val_loss: 64.4060\n",
      "Epoch 306/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 55.5695 - val_loss: 66.8050\n",
      "Epoch 307/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 56.2102 - val_loss: 64.2811\n",
      "Epoch 308/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 55.0039 - val_loss: 64.4840\n",
      "Epoch 309/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 54.7184 - val_loss: 65.0219\n",
      "Epoch 310/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 55.3352 - val_loss: 65.5596\n",
      "Epoch 311/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 55.1326 - val_loss: 65.1760\n",
      "Epoch 312/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 55.1896 - val_loss: 63.8337\n",
      "Epoch 313/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 54.0217 - val_loss: 63.5658\n",
      "Epoch 314/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 54.1073 - val_loss: 64.2508\n",
      "Epoch 315/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 54.6356 - val_loss: 64.5513\n",
      "Epoch 316/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 54.9626 - val_loss: 63.5024\n",
      "Epoch 317/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 53.9432 - val_loss: 64.3521\n",
      "Epoch 318/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 54.3746 - val_loss: 63.6921\n",
      "Epoch 319/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 55.0296 - val_loss: 65.8496\n",
      "Epoch 320/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 54.9852 - val_loss: 64.9116\n",
      "Epoch 321/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 55.8995 - val_loss: 66.7656\n",
      "Epoch 322/2000\n",
      "7/7 [==============================] - 1s 132ms/step - loss: 55.8399 - val_loss: 65.2286\n",
      "Epoch 323/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 55.2758 - val_loss: 64.1422\n",
      "Epoch 324/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 53.9024 - val_loss: 64.2214\n",
      "Epoch 325/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 53.6643 - val_loss: 63.2253\n",
      "Epoch 326/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 53.6313 - val_loss: 63.4462\n",
      "Epoch 327/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 53.4218 - val_loss: 64.0667\n",
      "Epoch 328/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 54.0520 - val_loss: 63.0649\n",
      "Epoch 329/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 53.3388 - val_loss: 62.7143\n",
      "Epoch 330/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 53.3068 - val_loss: 63.5453\n",
      "Epoch 331/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 53.1987 - val_loss: 63.7405\n",
      "Epoch 332/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 53.5418 - val_loss: 64.1283\n",
      "Epoch 333/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 53.5511 - val_loss: 63.6709\n",
      "Epoch 334/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 52.9289 - val_loss: 62.7195\n",
      "Epoch 335/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 52.9291 - val_loss: 63.0449\n",
      "Epoch 336/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 53.7017 - val_loss: 64.1690\n",
      "Epoch 337/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 54.2001 - val_loss: 63.3026\n",
      "Epoch 338/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 54.0219 - val_loss: 68.0636\n",
      "Epoch 339/2000\n",
      "7/7 [==============================] - 1s 129ms/step - loss: 55.4478 - val_loss: 65.3236\n",
      "Epoch 340/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 54.1428 - val_loss: 64.9340\n",
      "Epoch 341/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 54.4386 - val_loss: 64.1669\n",
      "Epoch 342/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 54.3414 - val_loss: 64.0507\n",
      "Epoch 343/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 53.6290 - val_loss: 63.5889\n",
      "Epoch 344/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 53.1705 - val_loss: 64.3653\n",
      "Epoch 345/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 53.3638 - val_loss: 63.4520\n",
      "Epoch 346/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 53.4924 - val_loss: 65.0338\n",
      "Epoch 347/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 53.4934 - val_loss: 66.0713\n",
      "Epoch 348/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 53.7856 - val_loss: 66.7739\n",
      "Epoch 349/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 54.4171 - val_loss: 63.1104\n",
      "Epoch 350/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 53.0553 - val_loss: 63.6595\n",
      "Epoch 351/2000\n",
      "7/7 [==============================] - 1s 129ms/step - loss: 52.2462 - val_loss: 62.6452\n",
      "Epoch 352/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 51.9761 - val_loss: 63.2644\n",
      "Epoch 353/2000\n",
      "7/7 [==============================] - 1s 129ms/step - loss: 53.2594 - val_loss: 62.4424\n",
      "Epoch 354/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 52.5905 - val_loss: 63.9883\n",
      "Epoch 355/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 51.9794 - val_loss: 62.7240\n",
      "Epoch 356/2000\n",
      "7/7 [==============================] - 1s 138ms/step - loss: 52.2258 - val_loss: 63.3269\n",
      "Epoch 357/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 52.9563 - val_loss: 63.8072\n",
      "Epoch 358/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 52.9268 - val_loss: 63.3173\n",
      "Epoch 359/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 52.6079 - val_loss: 62.4746\n",
      "Epoch 360/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 51.9616 - val_loss: 62.7768\n",
      "Epoch 361/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 51.9147 - val_loss: 63.5973\n",
      "Epoch 362/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 51.9409 - val_loss: 62.8489\n",
      "Epoch 363/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 52.5626 - val_loss: 62.9100\n",
      "Epoch 364/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 52.1171 - val_loss: 62.7234\n",
      "Epoch 365/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 52.7086 - val_loss: 63.3755\n",
      "Epoch 366/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 51.7090 - val_loss: 63.0460\n",
      "Epoch 367/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 51.7222 - val_loss: 63.0212\n",
      "Epoch 368/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 51.4828 - val_loss: 63.1825\n",
      "Epoch 369/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 51.6714 - val_loss: 62.0332\n",
      "Epoch 370/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 51.6197 - val_loss: 62.1948\n",
      "Epoch 371/2000\n",
      "7/7 [==============================] - 1s 129ms/step - loss: 51.3701 - val_loss: 62.8373\n",
      "Epoch 372/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 51.3832 - val_loss: 63.5162\n",
      "Epoch 373/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 51.3620 - val_loss: 65.1514\n",
      "Epoch 374/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 52.6462 - val_loss: 63.2115\n",
      "Epoch 375/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 51.2319 - val_loss: 61.4851\n",
      "Epoch 376/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 51.8650 - val_loss: 62.5351\n",
      "Epoch 377/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 51.2845 - val_loss: 62.6527\n",
      "Epoch 378/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 51.2026 - val_loss: 63.0430\n",
      "Epoch 379/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 51.2157 - val_loss: 62.8168\n",
      "Epoch 380/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 51.5322 - val_loss: 62.9858\n",
      "Epoch 381/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 50.9622 - val_loss: 66.8752\n",
      "Epoch 382/2000\n",
      "7/7 [==============================] - 1s 129ms/step - loss: 52.1678 - val_loss: 63.0953\n",
      "Epoch 383/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 51.7918 - val_loss: 64.0545\n",
      "Epoch 384/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 51.9885 - val_loss: 63.6754\n",
      "Epoch 385/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 52.0980 - val_loss: 64.9085\n",
      "Epoch 386/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 51.9957 - val_loss: 63.2133\n",
      "Epoch 387/2000\n",
      "7/7 [==============================] - 1s 129ms/step - loss: 51.3221 - val_loss: 62.6460\n",
      "Epoch 388/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 51.2148 - val_loss: 63.4180\n",
      "Epoch 389/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 51.1252 - val_loss: 63.7524\n",
      "Epoch 390/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 50.6132 - val_loss: 62.3896\n",
      "Epoch 391/2000\n",
      "7/7 [==============================] - 1s 130ms/step - loss: 50.6756 - val_loss: 61.8470\n",
      "Epoch 392/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 50.3362 - val_loss: 62.2867\n",
      "Epoch 393/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 51.1338 - val_loss: 63.3548\n",
      "Epoch 394/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 50.0731 - val_loss: 62.6502\n",
      "Epoch 395/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 50.7805 - val_loss: 63.2594\n",
      "Epoch 396/2000\n",
      "7/7 [==============================] - 1s 130ms/step - loss: 50.9381 - val_loss: 63.4963\n",
      "Epoch 397/2000\n",
      "7/7 [==============================] - 1s 129ms/step - loss: 51.2581 - val_loss: 62.7050\n",
      "Epoch 398/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 52.0641 - val_loss: 64.2243\n",
      "Epoch 399/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 51.5968 - val_loss: 63.1867\n",
      "Epoch 400/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 50.5019 - val_loss: 63.7642\n",
      "\n",
      "Epoch 00400: saving model to saved_models/latent32/cp-0400.h5\n",
      "Epoch 401/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 51.5074 - val_loss: 64.3264\n",
      "Epoch 402/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 52.1757 - val_loss: 62.6766\n",
      "Epoch 403/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 51.3682 - val_loss: 63.1847\n",
      "Epoch 404/2000\n",
      "7/7 [==============================] - 1s 129ms/step - loss: 51.0554 - val_loss: 62.6611\n",
      "Epoch 405/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 50.4340 - val_loss: 61.8591\n",
      "Epoch 406/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 50.6820 - val_loss: 62.8697\n",
      "Epoch 407/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 50.8558 - val_loss: 64.0773\n",
      "Epoch 408/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 51.6844 - val_loss: 63.5580\n",
      "Epoch 409/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 50.3240 - val_loss: 63.1832\n",
      "Epoch 410/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 50.7590 - val_loss: 62.7700\n",
      "Epoch 411/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 50.2727 - val_loss: 62.3435\n",
      "Epoch 412/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 50.3606 - val_loss: 63.9228\n",
      "Epoch 413/2000\n",
      "7/7 [==============================] - 1s 129ms/step - loss: 50.4936 - val_loss: 63.4835\n",
      "Epoch 414/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 50.2239 - val_loss: 63.1570\n",
      "Epoch 415/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 49.9982 - val_loss: 62.9064\n",
      "Epoch 416/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 50.4257 - val_loss: 64.6382\n",
      "Epoch 417/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 50.7497 - val_loss: 62.2380\n",
      "Epoch 418/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 49.8080 - val_loss: 63.2661\n",
      "Epoch 419/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 50.2124 - val_loss: 62.5690\n",
      "Epoch 420/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 49.8194 - val_loss: 65.2775\n",
      "Epoch 421/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 50.6591 - val_loss: 62.7983\n",
      "Epoch 422/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 50.0061 - val_loss: 63.1643\n",
      "Epoch 423/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 49.7392 - val_loss: 62.4546\n",
      "Epoch 424/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 49.4654 - val_loss: 63.0573\n",
      "Epoch 425/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 49.5424 - val_loss: 62.7012\n",
      "Epoch 426/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 49.2127 - val_loss: 63.2063\n",
      "Epoch 427/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 49.8893 - val_loss: 62.6827\n",
      "Epoch 428/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 50.1186 - val_loss: 62.8665\n",
      "Epoch 429/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 49.8304 - val_loss: 62.9072\n",
      "Epoch 430/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 49.8729 - val_loss: 63.5761\n",
      "Epoch 431/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 50.8211 - val_loss: 63.5860\n",
      "Epoch 432/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 49.6586 - val_loss: 62.2366\n",
      "Epoch 433/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 49.8454 - val_loss: 63.9656\n",
      "Epoch 434/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 49.7041 - val_loss: 61.7207\n",
      "Epoch 435/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 49.4807 - val_loss: 62.8704\n",
      "Epoch 436/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 49.9635 - val_loss: 63.6021\n",
      "Epoch 437/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 49.4517 - val_loss: 62.7841\n",
      "Epoch 438/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 49.7647 - val_loss: 63.7700\n",
      "Epoch 439/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 49.7505 - val_loss: 62.7017\n",
      "Epoch 440/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 49.2496 - val_loss: 64.1007\n",
      "Epoch 441/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 50.9901 - val_loss: 66.1998\n",
      "Epoch 442/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 51.0029 - val_loss: 63.6471\n",
      "Epoch 443/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 49.9800 - val_loss: 63.4758\n",
      "Epoch 444/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 50.1403 - val_loss: 62.0092\n",
      "Epoch 445/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 49.2147 - val_loss: 63.1214\n",
      "Epoch 446/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 49.7940 - val_loss: 62.9575\n",
      "Epoch 447/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 48.9941 - val_loss: 62.9947\n",
      "Epoch 448/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 49.2367 - val_loss: 63.2643\n",
      "Epoch 449/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 49.7019 - val_loss: 65.0069\n",
      "Epoch 450/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 50.1391 - val_loss: 63.3233\n",
      "Epoch 451/2000\n",
      "7/7 [==============================] - 1s 129ms/step - loss: 49.6903 - val_loss: 63.7074\n",
      "Epoch 452/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 49.7582 - val_loss: 62.5404\n",
      "Epoch 453/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 49.1644 - val_loss: 63.3846\n",
      "Epoch 454/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 48.8116 - val_loss: 63.1991\n",
      "Epoch 455/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 49.2916 - val_loss: 63.6979\n",
      "Epoch 456/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 49.0003 - val_loss: 64.0023\n",
      "Epoch 457/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 48.7866 - val_loss: 62.6200\n",
      "Epoch 458/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 49.0897 - val_loss: 62.2858\n",
      "Epoch 459/2000\n",
      "7/7 [==============================] - 1s 126ms/step - loss: 48.5615 - val_loss: 64.8576\n",
      "Epoch 460/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 49.5893 - val_loss: 62.7488\n",
      "Epoch 461/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 48.9580 - val_loss: 62.6623\n",
      "Epoch 462/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 48.8198 - val_loss: 62.2204\n",
      "Epoch 463/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 48.8014 - val_loss: 62.3187\n",
      "Epoch 464/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 48.6104 - val_loss: 62.0471\n",
      "Epoch 465/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 48.3378 - val_loss: 63.1426\n",
      "Epoch 466/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 48.4654 - val_loss: 62.7730\n",
      "Epoch 467/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 49.0828 - val_loss: 61.9098\n",
      "Epoch 468/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 48.4920 - val_loss: 62.7446\n",
      "Epoch 469/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 48.4814 - val_loss: 63.3726\n",
      "Epoch 470/2000\n",
      "7/7 [==============================] - 1s 130ms/step - loss: 48.5404 - val_loss: 62.3893\n",
      "Epoch 471/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 48.3651 - val_loss: 63.8808\n",
      "Epoch 472/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 48.7731 - val_loss: 63.1059\n",
      "Epoch 473/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 48.1906 - val_loss: 63.4250\n",
      "Epoch 474/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 48.4163 - val_loss: 63.6361\n",
      "Epoch 475/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 48.8340 - val_loss: 63.9129\n",
      "Epoch 476/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 48.8735 - val_loss: 62.9333\n",
      "Epoch 477/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 48.5375 - val_loss: 63.6284\n",
      "Epoch 478/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 48.3010 - val_loss: 63.5090\n",
      "Epoch 479/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 48.6666 - val_loss: 62.7860\n",
      "Epoch 480/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 47.9489 - val_loss: 62.8609\n",
      "Epoch 481/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 48.0268 - val_loss: 62.0767\n",
      "Epoch 482/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 48.2879 - val_loss: 62.2352\n",
      "Epoch 483/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 48.2622 - val_loss: 62.4311\n",
      "Epoch 484/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 47.4613 - val_loss: 62.7974\n",
      "Epoch 485/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 48.1649 - val_loss: 62.2356\n",
      "Epoch 486/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 47.8486 - val_loss: 64.2266\n",
      "Epoch 487/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 48.2867 - val_loss: 62.6408\n",
      "Epoch 488/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 47.5217 - val_loss: 61.6758\n",
      "Epoch 489/2000\n",
      "7/7 [==============================] - 1s 130ms/step - loss: 47.6732 - val_loss: 63.7927\n",
      "Epoch 490/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 48.0832 - val_loss: 62.7535\n",
      "Epoch 491/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 47.7974 - val_loss: 64.4739\n",
      "Epoch 492/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 48.0274 - val_loss: 63.5035\n",
      "Epoch 493/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 48.2661 - val_loss: 62.7489\n",
      "Epoch 494/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 48.3041 - val_loss: 62.5153\n",
      "Epoch 495/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 47.5220 - val_loss: 63.4242\n",
      "Epoch 496/2000\n",
      "7/7 [==============================] - 1s 129ms/step - loss: 47.9785 - val_loss: 63.5510\n",
      "Epoch 497/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 48.1995 - val_loss: 62.8942\n",
      "Epoch 498/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 47.9554 - val_loss: 63.5935\n",
      "Epoch 499/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 47.8615 - val_loss: 62.7259\n",
      "Epoch 500/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 47.7679 - val_loss: 62.8084\n",
      "Epoch 501/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 47.2429 - val_loss: 61.6019\n",
      "Epoch 502/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 47.1131 - val_loss: 63.4454\n",
      "Epoch 503/2000\n",
      "7/7 [==============================] - 1s 129ms/step - loss: 47.5366 - val_loss: 64.1912\n",
      "Epoch 504/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 47.9784 - val_loss: 63.1606\n",
      "Epoch 505/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 47.9499 - val_loss: 63.1334\n",
      "Epoch 506/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 47.5877 - val_loss: 64.6166\n",
      "Epoch 507/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 48.3015 - val_loss: 62.9898\n",
      "Epoch 508/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 47.8887 - val_loss: 63.1781\n",
      "Epoch 509/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 47.3371 - val_loss: 62.4457\n",
      "Epoch 510/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 47.4977 - val_loss: 63.8125\n",
      "Epoch 511/2000\n",
      "7/7 [==============================] - 1s 129ms/step - loss: 47.2768 - val_loss: 63.0206\n",
      "Epoch 512/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 47.4815 - val_loss: 64.8334\n",
      "Epoch 513/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 47.7377 - val_loss: 62.7776\n",
      "Epoch 514/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 48.2193 - val_loss: 63.9354\n",
      "Epoch 515/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 47.7574 - val_loss: 62.8984\n",
      "Epoch 516/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 47.5650 - val_loss: 65.1249\n",
      "Epoch 517/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 48.9000 - val_loss: 63.4383\n",
      "Epoch 518/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 48.9539 - val_loss: 65.5760\n",
      "Epoch 519/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 49.1132 - val_loss: 64.3032\n",
      "Epoch 520/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 48.0247 - val_loss: 63.3452\n",
      "Epoch 521/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 47.4950 - val_loss: 64.3732\n",
      "Epoch 522/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 47.1243 - val_loss: 63.2403\n",
      "Epoch 523/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 47.1409 - val_loss: 62.7867\n",
      "Epoch 524/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 46.6392 - val_loss: 62.7887\n",
      "Epoch 525/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 47.2022 - val_loss: 62.1155\n",
      "Epoch 526/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 46.7878 - val_loss: 63.2299\n",
      "Epoch 527/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 47.1067 - val_loss: 63.2465\n",
      "Epoch 528/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 47.6452 - val_loss: 63.6060\n",
      "Epoch 529/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 47.3289 - val_loss: 64.4771\n",
      "Epoch 530/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 47.8493 - val_loss: 62.8320\n",
      "Epoch 531/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 47.1469 - val_loss: 63.2076\n",
      "Epoch 532/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 47.3031 - val_loss: 62.9375\n",
      "Epoch 533/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 46.6763 - val_loss: 62.8016\n",
      "Epoch 534/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 46.5618 - val_loss: 62.8726\n",
      "Epoch 535/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 47.0112 - val_loss: 63.4701\n",
      "Epoch 536/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 47.0939 - val_loss: 63.2774\n",
      "Epoch 537/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 46.7825 - val_loss: 62.3434\n",
      "Epoch 538/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 46.2891 - val_loss: 62.5086\n",
      "Epoch 539/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 46.3126 - val_loss: 62.7255\n",
      "Epoch 540/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 46.3976 - val_loss: 64.9622\n",
      "Epoch 541/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 47.6698 - val_loss: 65.2243\n",
      "Epoch 542/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 47.7375 - val_loss: 63.9854\n",
      "Epoch 543/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 47.3063 - val_loss: 62.3442\n",
      "Epoch 544/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 46.6946 - val_loss: 63.2407\n",
      "Epoch 545/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 47.0763 - val_loss: 62.5871\n",
      "Epoch 546/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 46.7115 - val_loss: 63.8860\n",
      "Epoch 547/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 47.2892 - val_loss: 63.1449\n",
      "Epoch 548/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 46.8078 - val_loss: 63.0615\n",
      "Epoch 549/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 47.0663 - val_loss: 63.2053\n",
      "Epoch 550/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 46.4729 - val_loss: 63.3954\n",
      "Epoch 551/2000\n",
      "7/7 [==============================] - 1s 129ms/step - loss: 46.5003 - val_loss: 62.6048\n",
      "Epoch 552/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 46.3264 - val_loss: 62.7574\n",
      "Epoch 553/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 45.9103 - val_loss: 63.1922\n",
      "Epoch 554/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 46.8062 - val_loss: 64.5080\n",
      "Epoch 555/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 48.2121 - val_loss: 62.7773\n",
      "Epoch 556/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 47.0490 - val_loss: 63.4695\n",
      "Epoch 557/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 46.4640 - val_loss: 62.6306\n",
      "Epoch 558/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 46.1209 - val_loss: 62.9313\n",
      "Epoch 559/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 46.4670 - val_loss: 64.2723\n",
      "Epoch 560/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 46.7071 - val_loss: 62.6441\n",
      "Epoch 561/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 46.7691 - val_loss: 65.1434\n",
      "Epoch 562/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 47.2258 - val_loss: 64.3211\n",
      "Epoch 563/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 47.5318 - val_loss: 63.6674\n",
      "Epoch 564/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 46.3474 - val_loss: 62.6929\n",
      "Epoch 565/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 45.9039 - val_loss: 62.6226\n",
      "Epoch 566/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 46.6989 - val_loss: 63.0255\n",
      "Epoch 567/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 46.3617 - val_loss: 63.6203\n",
      "Epoch 568/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 46.6581 - val_loss: 62.6553\n",
      "Epoch 569/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 46.2978 - val_loss: 62.0984\n",
      "Epoch 570/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 46.0093 - val_loss: 63.4453\n",
      "Epoch 571/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 45.9164 - val_loss: 62.8729\n",
      "Epoch 572/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 45.9370 - val_loss: 63.4213\n",
      "Epoch 573/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 46.4267 - val_loss: 64.9034\n",
      "Epoch 574/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 46.6979 - val_loss: 63.2722\n",
      "Epoch 575/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 45.8506 - val_loss: 63.1450\n",
      "Epoch 576/2000\n",
      "7/7 [==============================] - 1s 136ms/step - loss: 46.0884 - val_loss: 62.4408\n",
      "Epoch 577/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 46.0949 - val_loss: 62.4240\n",
      "Epoch 578/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 46.2273 - val_loss: 62.5480\n",
      "Epoch 579/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 45.8380 - val_loss: 62.9028\n",
      "Epoch 580/2000\n",
      "7/7 [==============================] - 1s 131ms/step - loss: 46.0782 - val_loss: 61.8656\n",
      "Epoch 581/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 46.1186 - val_loss: 62.6512\n",
      "Epoch 582/2000\n",
      "7/7 [==============================] - 1s 133ms/step - loss: 46.3853 - val_loss: 63.0723\n",
      "Epoch 583/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 45.7992 - val_loss: 63.6605\n",
      "Epoch 584/2000\n",
      "7/7 [==============================] - 1s 130ms/step - loss: 46.1830 - val_loss: 62.8588\n",
      "Epoch 585/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 45.9009 - val_loss: 62.8337\n",
      "Epoch 586/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 46.2566 - val_loss: 62.8603\n",
      "Epoch 587/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 45.8001 - val_loss: 63.0279\n",
      "Epoch 588/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 46.0476 - val_loss: 63.5849\n",
      "Epoch 589/2000\n",
      "7/7 [==============================] - 1s 129ms/step - loss: 45.9994 - val_loss: 62.7695\n",
      "Epoch 590/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 45.8102 - val_loss: 62.3220\n",
      "Epoch 591/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 45.6245 - val_loss: 62.4206\n",
      "Epoch 592/2000\n",
      "7/7 [==============================] - 1s 131ms/step - loss: 45.6540 - val_loss: 62.4838\n",
      "Epoch 593/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 45.4897 - val_loss: 62.1744\n",
      "Epoch 594/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 45.8099 - val_loss: 62.5100\n",
      "Epoch 595/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 45.9980 - val_loss: 63.6330\n",
      "Epoch 596/2000\n",
      "7/7 [==============================] - 1s 129ms/step - loss: 46.4160 - val_loss: 63.5100\n",
      "Epoch 597/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 46.0366 - val_loss: 63.4237\n",
      "Epoch 598/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 46.1328 - val_loss: 63.2287\n",
      "Epoch 599/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 45.7043 - val_loss: 62.8416\n",
      "Epoch 600/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 45.9289 - val_loss: 63.1683\n",
      "\n",
      "Epoch 00600: saving model to saved_models/latent32/cp-0600.h5\n",
      "Epoch 601/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 46.5303 - val_loss: 61.8371\n",
      "Epoch 602/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 45.7189 - val_loss: 64.1436\n",
      "Epoch 603/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 45.9311 - val_loss: 62.1072\n",
      "Epoch 604/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 45.4548 - val_loss: 63.3814\n",
      "Epoch 605/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 45.7546 - val_loss: 63.3163\n",
      "Epoch 606/2000\n",
      "7/7 [==============================] - 1s 129ms/step - loss: 45.6670 - val_loss: 63.3472\n",
      "Epoch 607/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 45.5726 - val_loss: 63.9949\n",
      "Epoch 608/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 45.6012 - val_loss: 63.9909\n",
      "Epoch 609/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 45.9607 - val_loss: 62.9107\n",
      "Epoch 610/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 45.7485 - val_loss: 61.8108\n",
      "Epoch 611/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 45.0264 - val_loss: 62.2006\n",
      "Epoch 612/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 45.5191 - val_loss: 62.1847\n",
      "Epoch 613/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 44.5913 - val_loss: 62.3727\n",
      "Epoch 614/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 45.0829 - val_loss: 62.4914\n",
      "Epoch 615/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 45.2519 - val_loss: 64.0964\n",
      "Epoch 616/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 45.1700 - val_loss: 63.6482\n",
      "Epoch 617/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 45.1170 - val_loss: 62.7364\n",
      "Epoch 618/2000\n",
      "7/7 [==============================] - 1s 126ms/step - loss: 44.9491 - val_loss: 63.1486\n",
      "Epoch 619/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 45.1656 - val_loss: 62.2750\n",
      "Epoch 620/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 44.9628 - val_loss: 62.6176\n",
      "Epoch 621/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 44.9794 - val_loss: 64.7601\n",
      "Epoch 622/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 46.2033 - val_loss: 62.5690\n",
      "Epoch 623/2000\n",
      "7/7 [==============================] - 1s 126ms/step - loss: 45.5847 - val_loss: 63.9033\n",
      "Epoch 624/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 45.7118 - val_loss: 61.9079\n",
      "Epoch 625/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 46.2682 - val_loss: 63.8361\n",
      "Epoch 626/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 46.3042 - val_loss: 64.5439\n",
      "Epoch 627/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 46.0986 - val_loss: 63.1740\n",
      "Epoch 628/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 45.6620 - val_loss: 63.2029\n",
      "Epoch 629/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 45.5134 - val_loss: 63.6788\n",
      "Epoch 630/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 45.5251 - val_loss: 63.2488\n",
      "Epoch 631/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 46.0166 - val_loss: 64.6847\n",
      "Epoch 632/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 45.7675 - val_loss: 62.7679\n",
      "Epoch 633/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 45.5050 - val_loss: 62.5596\n",
      "Epoch 634/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 45.0425 - val_loss: 62.6830\n",
      "Epoch 635/2000\n",
      "7/7 [==============================] - 1s 129ms/step - loss: 44.8373 - val_loss: 62.1551\n",
      "Epoch 636/2000\n",
      "7/7 [==============================] - 1s 128ms/step - loss: 44.8299 - val_loss: 62.4421\n",
      "Epoch 637/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 44.5018 - val_loss: 61.7327\n",
      "Epoch 638/2000\n",
      "7/7 [==============================] - 1s 127ms/step - loss: 44.7126 - val_loss: 63.0213\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00638: early stopping\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Train VAE model with callbacks \"\"\"\n",
    "history = vae.fit(trainX, trainX, \n",
    "                  batch_size = batch_size, \n",
    "                  epochs = epochs, \n",
    "                  callbacks=[initial_checkpoint, checkpoint, EarlyStoppingAtMinLoss()],\n",
    "                  #callbacks=[initial_checkpoint, checkpoint],\n",
    "                  shuffle=True,\n",
    "                  validation_data=(valX, valX)\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step size: 7.0\n"
     ]
    }
   ],
   "source": [
    "# In this GPU implementation, it shows the number of batches/iterations for one epoch (and not the training samples), which is:\n",
    "print (\"Step size:\", np.ceil(trainX.shape[0]/batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Save the entire model \"\"\"\n",
    "# Save notes about hyperparameters used:\n",
    "with open(f'{SAVE_FOLDER}/notes.txt', 'w') as f:\n",
    "    f.write(f'Epochs: {epochs} \\n')\n",
    "    f.write(f'Batch size: {batch_size} \\n')\n",
    "    f.write(f'Latent dimension: {latent_dim} \\n')\n",
    "    f.write(f'Filter sizes: {filters} \\n')\n",
    "    f.write(f'img_width: {img_width} \\n')\n",
    "    f.write(f'img_height: {img_height} \\n')\n",
    "    f.write(f'num_channels: {num_channels}')\n",
    "    f.close()\n",
    "\n",
    "# Save weights for encoder model:\n",
    "encoder.save_weights(f'{SAVE_FOLDER}/ENCODER_WEIGHTS.h5')\n",
    "\n",
    "# Save weights for decoder model:\n",
    "decoder.save_weights(f'{SAVE_FOLDER}/DECODER_WEIGHTS.h5')\n",
    "\n",
    "# Save weights for total VAE:\n",
    "vae.save_weights(f'{SAVE_FOLDER}/VAE_WEIGHTS.h5')\n",
    "\n",
    "# Save the history at each epochs (cost of train and validation sets):\n",
    "np.save(f'{SAVE_FOLDER}/HISTORY.npy', history.history)\n",
    "\n",
    "# Save exact training, validation and testing data set (as numpy arrays):\n",
    "np.save(f'{SAVE_FOLDER}/trainX.npy', trainX)\n",
    "np.save(f'{SAVE_FOLDER}/valX.npy', valX)\n",
    "np.save(f'{SAVE_FOLDER}/testAllX.npy', testAllX)\n",
    "np.save(f'{SAVE_FOLDER}/testAllY.npy', testAllY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate training process\n",
    "\n",
    "Now that the training process is complete, lets evaluate the average loss of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 936x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set_style(\"darkgrid\")\n",
    "plt.figure(figsize=(13, 6))\n",
    "\n",
    "# summarize history for loss:\n",
    "plt.plot(history.history['loss'], color='blue', label='train')\n",
    "plt.plot(history.history['val_loss'], color='blue', alpha=0.4, label='validation')\n",
    "plt.xlabel('Epoch', fontsize=16)\n",
    "plt.ylabel('Cost', fontsize=16)\n",
    "plt.tick_params(axis='both', which='major', labelsize=14)\n",
    "plt.title('Average Loss During Training', fontsize=18)\n",
    "#plt.xticks(range(0,epochs))\n",
    "plt.legend(loc='upper right', fontsize=16)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
