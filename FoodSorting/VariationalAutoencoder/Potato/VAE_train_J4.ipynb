{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Autoencoder: Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import axis, colorbar, imshow, show, figure, subplot\n",
    "from matplotlib import cm\n",
    "import matplotlib as mpl\n",
    "mpl.rc('axes', labelsize=18)\n",
    "mpl.rc('xtick', labelsize=16)\n",
    "mpl.rc('ytick', labelsize=16)\n",
    "%matplotlib inline\n",
    "\n",
    "import glob\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "import cv2\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "## ----- GPU ------------------------------------\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'    # use of GPU 0 (ERDA) (use before importing torch or tensorflow/keeas)\n",
    "## ----------------------------------------------\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.layers import Conv2D, Conv2DTranspose, Input, Flatten, Dense, Lambda, Reshape\n",
    "from keras.layers import BatchNormalization, ReLU, LeakyReLU\n",
    "from keras.models import Model\n",
    "from keras.losses import binary_crossentropy, mse\n",
    "from keras.activations import relu\n",
    "from keras.callbacks import Callback, ModelCheckpoint\n",
    "from keras import backend as K                         #contains calls for tensor manipulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n",
      "2.4.3\n"
     ]
    }
   ],
   "source": [
    "print (tf.__version__)\n",
    "print (keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NORMAL_TRAIN_AUG:       /home/jovyan/work/Speciale/FoodSorting/generated_dataset/Potato//DataAugmentation/NormalTrainAugmentations\n",
      "NORMAL_VALIDATION_AUG:  /home/jovyan/work/Speciale/FoodSorting/generated_dataset/Potato//DataAugmentation/NormalValidationAugmentations\n",
      "NORMAL_TEST_AUG:        /home/jovyan/work/Speciale/FoodSorting/generated_dataset/Potato//DataAugmentation/NormalTestAugmentations\n",
      "ANOMALY1_AUG:           /home/jovyan/work/Speciale/FoodSorting/generated_dataset/Potato//DataAugmentation/Anomaly1Augmentations\n",
      "ANOMALY2_AUG:           /home/jovyan/work/Speciale/FoodSorting/generated_dataset/Potato//DataAugmentation/Anomaly2Augmentations\n"
     ]
    }
   ],
   "source": [
    "from utils_vae_potato_paths import *\n",
    "\n",
    "# Get work directions for augmentated data set:\n",
    "print (\"NORMAL_TRAIN_AUG:      \", NORMAL_TRAIN_AUG)\n",
    "print (\"NORMAL_VALIDATION_AUG: \", NORMAL_VAL_AUG)\n",
    "print (\"NORMAL_TEST_AUG:       \", NORMAL_TEST_AUG)\n",
    "print (\"ANOMALY1_AUG:          \", ANOMALY1_AUG)\n",
    "print (\"ANOMALY2_AUG:          \", ANOMALY2_AUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Which Folder The Models Are Saved In: saved_models/latent4\n"
     ]
    }
   ],
   "source": [
    "# What latent dimension and filter size are we using?:\n",
    "latent_dim = 4\n",
    "filters    = 32\n",
    "\n",
    "# Get work direction for saving files:\n",
    "SAVE_FOLDER = f'saved_models/latent{latent_dim}'\n",
    "print (\"Which Folder The Models Are Saved In:\", SAVE_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] deleting existing files in folder...\n",
      "         done in 0.004 minutes\n"
     ]
    }
   ],
   "source": [
    "# Delete all existing files in folder where models are saved:\n",
    "print(\"[INFO] deleting existing files in folder...\")\n",
    "t0 = time()\n",
    "\n",
    "files = glob.glob(f'{SAVE_FOLDER}/*')\n",
    "for f in files:\n",
    "    os.remove(f)\n",
    "\n",
    "print (\"         done in %0.3f minutes\" % ((time() - t0)/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_VAE_AE import get_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Investigation of Ground Truths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of \"normal\" training images is:     1639\n",
      "Number of \"normal\" validation images is:   80\n",
      "Number of \"normal\" test images is:         320\n",
      "Total number of \"anomaly\" images is:       680\n"
     ]
    }
   ],
   "source": [
    "# Glob the directories and get the lists of good and bad images\n",
    "good_train_fns = [f for f in os.listdir(NORMAL_TRAIN_AUG) if not f.startswith('.')]\n",
    "good_val_fns = [f for f in os.listdir(NORMAL_VAL_AUG) if not f.startswith('.')]\n",
    "good_test_fns = [f for f in os.listdir(NORMAL_TEST_AUG) if not f.startswith('.')]\n",
    "bad1_fns = [f for f in os.listdir(ANOMALY1_AUG) if not f.startswith('.')]\n",
    "bad2_fns = [f for f in os.listdir(ANOMALY2_AUG) if not f.startswith('.')]\n",
    "\n",
    "print('Number of \"normal\" training images is:     {}'.format(len(good_train_fns)))\n",
    "print('Number of \"normal\" validation images is:   {}'.format(len(good_val_fns)))\n",
    "print('Number of \"normal\" test images is:         {}'.format(len(good_test_fns)))\n",
    "print('Total number of \"anomaly\" images is:       {}'.format(len(bad1_fns) + len(bad2_fns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Procentage of normal samples (ground truth):   74.99 %\n",
      "> Procentage of anomaly samples (ground truth):  25.01 %\n"
     ]
    }
   ],
   "source": [
    "# compute procentage of bad images vs good images:\n",
    "normal_fns = len(good_train_fns) + len(good_val_fns) + len(good_test_fns)\n",
    "anomaly_fns = len(bad1_fns + bad2_fns)\n",
    "print(f\"> Procentage of normal samples (ground truth):   {(normal_fns / (normal_fns + anomaly_fns)) * 100:.2f} %\")\n",
    "print(f\"> Procentage of anomaly samples (ground truth):  {(anomaly_fns / (normal_fns + anomaly_fns)) * 100:.2f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Normal Data\n",
    "\n",
    "Load normal samples in pre-splitted data sets: train, valdiation and test.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading normal training images...\n",
      "          done\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] loading normal training images...\")\n",
    "trainX = [cv2.imread(file, 0) for file in glob.glob(f\"{NORMAL_TRAIN_AUG}/*.png\")]  # read as grayscale\n",
    "trainX = np.array(trainX)\n",
    "print(\"          done\")\n",
    "\n",
    "# create a corresponding list of labels:\n",
    "trainY = get_labels(trainX, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading normal validation images...\n",
      "          done\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] loading normal validation images...\")\n",
    "x_normal_val = [cv2.imread(file, 0) for file in glob.glob(f\"{NORMAL_VAL_AUG}/*.png\")]  # read as grayscale\n",
    "x_normal_val = np.array(x_normal_val)\n",
    "print(\"          done\")\n",
    "\n",
    "# create a corresponding list of labels:\n",
    "y_normal_val = get_labels(x_normal_val, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading normal testing images...\n",
      "          done\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] loading normal testing images...\")\n",
    "x_normal_test = [cv2.imread(file, 0) for file in glob.glob(f\"{NORMAL_TEST_AUG}/*.png\")]  # read as grayscale\n",
    "x_normal_test = np.array(x_normal_test)\n",
    "print(\"          done\")\n",
    "\n",
    "# create a corresponding list of labels:\n",
    "y_normal_test = get_labels(x_normal_test, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Anomaly Class 1 Data (i.e. 'metal' sampels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading anomaly class 1 ('metal') images...\n",
      "          done\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] loading anomaly class 1 ('metal') images...\")\n",
    "x_metal = [cv2.imread(file, 0) for file in glob.glob(f\"{ANOMALY1_AUG}/*.png\")]  # read as grayscale\n",
    "x_metal = np.array(x_metal)\n",
    "print(\"          done\")\n",
    "\n",
    "# create a corresponding list of labels:\n",
    "y_metal = get_labels(x_metal, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Anomaly Class 2 Data (i.e. 'hollow' sampels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading anomaly class 2 ('hollow') images...\n",
      "          done\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] loading anomaly class 2 ('hollow') images...\")\n",
    "x_hollow = [cv2.imread(file, 0) for file in glob.glob(f\"{ANOMALY2_AUG}/*.png\")]  # read as grayscale\n",
    "x_hollow = np.array(x_hollow)\n",
    "print(\"          done\")\n",
    "\n",
    "# create a corresponding list of labels:\n",
    "y_hollow = get_labels(x_hollow, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine normal samples and anomaly samples and split them into training, validation and test sets\n",
    "\n",
    "\n",
    "`label 0` == Normal Samples (=> Perfect' Samples)\n",
    "\n",
    "`label 1` == Anomaly 1 Samples (=> 'Metal' Samples)\n",
    "\n",
    "`label 2` == Anomaly 2 Samples (=> 'Hollow' Samples)\n",
    "\n",
    "Train and Validation sets only consists of `Normal Data`, aka. `label 0`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine pre-loaded validation and testing set into a new testing set:\n",
    "testvalX = np.vstack((x_normal_val, x_normal_test))\n",
    "testvalY = get_labels(testvalX, 0)\n",
    "\n",
    "# randomly split in order to make new validation set:\n",
    "NoUsageX, valX, NoUsageY, valY = train_test_split(testvalX, testvalY, test_size=0.25, random_state=4345672)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Testing set \"\"\"\n",
    "# combine pre-loaded validation and testing set into a new testing set:\n",
    "testNormalX = np.vstack((x_normal_val, x_normal_test))\n",
    "testNormalY = get_labels(testNormalX, 0)\n",
    "\n",
    "# Anomaly Class 1 (i.e. 'metal' samples):\n",
    "testAnomaly1X, testAnomaly1Y = x_metal, y_metal\n",
    "\n",
    "# Anomaly Class 2 (i.e. 'hollow' samples):\n",
    "testAnomaly2X, testAnomaly2Y = x_hollow, y_hollow\n",
    "\n",
    "# Combine all testing data in one array (the test set is NOT used during any part but inference):\n",
    "testAllX = np.vstack((testNormalX, testAnomaly1X, testAnomaly2X))\n",
    "testAllY = np.hstack((testNormalY, testAnomaly1Y, testAnomaly2Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Data Dimensions and Distributions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect data shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:\n",
      " > images: (1639, 128, 128)\n",
      " > labels: (1639,)\n",
      "\n",
      "Validation set:\n",
      " > images: (100, 128, 128)\n",
      " > labels: (100,)\n",
      "\n",
      "Test set:\n",
      " > images: (1080, 128, 128)\n",
      " > labels: (1080,)\n",
      "\t'Normal' test set:\n",
      "\t  > images: (400, 128, 128)\n",
      "\t  > labels: (400,)\n",
      "\t'Anomaly 1' test set:\n",
      "\t  > images: (392, 128, 128)\n",
      "\t  > labels: (392,)\n",
      "\t'Anomaly 2' test set:\n",
      "\t  > images: (288, 128, 128)\n",
      "\t  > labels: (288,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set:\")\n",
    "print(\" > images:\", trainX.shape)\n",
    "print(\" > labels:\", trainY.shape)\n",
    "print (\"\")\n",
    "\n",
    "print(\"Validation set:\")\n",
    "print(\" > images:\", valX.shape)\n",
    "print(\" > labels:\", valY.shape)\n",
    "print (\"\")\n",
    "\n",
    "print(\"Test set:\")\n",
    "print(\" > images:\", testAllX.shape)\n",
    "print(\" > labels:\", testAllY.shape)\n",
    "print(\"\\t'Normal' test set:\")\n",
    "print(\"\\t  > images:\", testNormalX.shape)\n",
    "print(\"\\t  > labels:\", testNormalY.shape)\n",
    "print(\"\\t'Anomaly 1' test set:\")\n",
    "print(\"\\t  > images:\", testAnomaly1X.shape)\n",
    "print(\"\\t  > labels:\", testAnomaly1Y.shape)\n",
    "print(\"\\t'Anomaly 2' test set:\")\n",
    "print(\"\\t  > images:\", testAnomaly2X.shape)\n",
    "print(\"\\t  > labels:\", testAnomaly2Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procentage of training samples: 76.62 %\n",
      "Procentage of validation samples: 4.68 %\n",
      "Procentage of (only normal) testing samples: 18.70 %\n"
     ]
    }
   ],
   "source": [
    "# compute procentage between training, validation and test data sets (only normal data):\n",
    "print(f\"Procentage of training samples: {(len(trainX) / (len(trainX) + len(valX) + len(testNormalX))) * 100:.2f} %\")\n",
    "print(f\"Procentage of validation samples: {(len(valX) / (len(trainX) + len(valX) + len(testNormalX))) * 100:.2f} %\")\n",
    "print(f\"Procentage of (only normal) testing samples: {(len(testNormalX) / (len(trainX) + len(valX) + len(testNormalX))) * 100:.2f} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procentage of training samples: 58.14 %\n",
      "Procentage of validation samples: 3.55 %\n",
      "Procentage of (total) testing samples: 38.31 %\n"
     ]
    }
   ],
   "source": [
    "# compute procentage between training, validation and test data sets:\n",
    "print(f\"Procentage of training samples: {(len(trainX) / (len(trainX) + len(valX) + len(testAllX))) * 100:.2f} %\")\n",
    "print(f\"Procentage of validation samples: {(len(valX) / (len(trainX) + len(valX) + len(testAllX))) * 100:.2f} %\")\n",
    "print(f\"Procentage of (total) testing samples: {(len(testAllX) / (len(trainX) + len(valX) + len(testAllX))) * 100:.2f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for (un)balanced data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels = ['0: normal', '1: metal', '2: hollow']\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(16,4))\n",
    "ax[0].hist(trainY, align=\"left\"); ax[0].set_title('train', fontsize=18); ax[0].set_xticks([0,1,2]); ax[0].set_xticklabels(labels); ax[0].tick_params(axis='both', which='major', labelsize=16); ax[0].set_xlim(-0.5, 2.5);\n",
    "ax[1].hist(valY, align=\"left\"); ax[1].set_title('validation', fontsize=18); ax[1].set_xticks([0,1,2]); ax[1].set_xticklabels(labels); ax[1].tick_params(axis='both', which='major', labelsize=16); ax[1].set_xlim(-0.5, 2.5);\n",
    "ax[2].hist(testAllY, align=\"left\"); ax[2].set_title('test', fontsize=18); ax[2].set_xticks([0,1,2]); ax[2].set_xticklabels(labels); ax[2].tick_params(axis='both', which='major', labelsize=16); ax[2].set_xlim(-0.5, 2.5);\n",
    "\n",
    "ax[0].set_ylabel(\"Number of images\", fontsize=18)\n",
    "ax[1].set_xlabel(\"Class Labels\", fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procentage of normal test samples: 37.04 %\n",
      "Procentage of total anomaly test samples: 62.96 %\n"
     ]
    }
   ],
   "source": [
    "# compute procentage of normal test images vs total anomaly test images:\n",
    "print(f\"Procentage of normal test samples: {(len(testNormalX) / (len(testNormalX) + (len(testAnomaly1X) + len(testAnomaly2X)))) * 100:.2f} %\")\n",
    "print(f\"Procentage of total anomaly test samples: {((len(testAnomaly1X) + len(testAnomaly2X)) / (len(testNormalX) + (len(testAnomaly1X) + len(testAnomaly2X)))) * 100:.2f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling\n",
    "\n",
    "Only normalization has been used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data configuration\n",
    "img_width, img_height = trainX.shape[1], trainX.shape[2]      # input image dimensions\n",
    "num_channels = 1                                              # gray-channel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization\n",
    "\n",
    "Neural Networks learns faster with normalized data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize to be in  the [0, 1] range:\n",
    "trainX = trainX.astype('float32') / 255.\n",
    "valX = valX.astype('float32') / 255.\n",
    "testAllX = testAllX.astype('float32') / 255.\n",
    "\n",
    "# reshape data to keras inputs --> (n_samples, img_rows, img_cols, n_channels):\n",
    "trainX = trainX.reshape(trainX.shape[0], img_height, img_width, num_channels)\n",
    "valX = valX.reshape(valX.shape[0], img_height, img_width, num_channels)\n",
    "testAllX = testAllX.reshape(testAllX.shape[0], img_height, img_width, num_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture and Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import architecture of VAE model and its hyperparameters:\n",
    "from J4_VAE_model import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Encoder Part*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_input (InputLayer)      [(None, 128, 128, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 64, 64, 32)   320         encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)         (None, 64, 64, 32)   0           conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 32, 32, 32)   9248        leaky_re_lu[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)       (None, 32, 32, 32)   0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 16, 16, 32)   9248        leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)       (None, 16, 16, 32)   0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 8, 8, 32)     9248        leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)       (None, 8, 8, 32)     0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 4, 4, 32)     9248        leaky_re_lu_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)       (None, 4, 4, 32)     0           conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 512)          0           leaky_re_lu_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "latent (Dense)                  (None, 200)          102600      flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)       (None, 200)          0           latent[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "z_mean (Dense)                  (None, 4)            804         leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "z_log_mean (Dense)              (None, 4)            804         leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "z (Lambda)                      (None, 4)            0           z_mean[0][0]                     \n",
      "                                                                 z_log_mean[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 141,520\n",
      "Trainable params: 141,520\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Decoder Part*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "decoder_input (InputLayer)   [(None, 4)]               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               2560      \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 4, 4, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose (Conv2DTran (None, 8, 8, 32)          9248      \n",
      "_________________________________________________________________\n",
      "re_lu (ReLU)                 (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 16, 16, 32)        9248      \n",
      "_________________________________________________________________\n",
      "re_lu_1 (ReLU)               (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTr (None, 32, 32, 32)        9248      \n",
      "_________________________________________________________________\n",
      "re_lu_2 (ReLU)               (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTr (None, 64, 64, 32)        9248      \n",
      "_________________________________________________________________\n",
      "re_lu_3 (ReLU)               (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_4 (Conv2DTr (None, 128, 128, 32)      9248      \n",
      "_________________________________________________________________\n",
      "re_lu_4 (ReLU)               (None, 128, 128, 32)      0         \n",
      "_________________________________________________________________\n",
      "decoder_output (Conv2DTransp (None, 128, 128, 1)       289       \n",
      "=================================================================\n",
      "Total params: 49,089\n",
      "Trainable params: 49,089\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Total VAE Model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vae\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder_input (InputLayer)   [(None, 128, 128, 1)]     0         \n",
      "_________________________________________________________________\n",
      "encoder (Functional)         [(None, 4), (None, 4), (N 141520    \n",
      "_________________________________________________________________\n",
      "decoder (Functional)         (None, 128, 128, 1)       49089     \n",
      "=================================================================\n",
      "Total params: 190,609\n",
      "Trainable params: 190,609\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vae.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile and train model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Hyperparameters \"\"\"\n",
    "batch_size  = 256\n",
    "epochs      = 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Checkpoint callback options: save model at 1'st epoch (inital model)\n",
    "\n",
    "https://stackoverflow.com/questions/54323960/save-keras-model-at-specific-epochs\n",
    "\"\"\"\n",
    "\n",
    "class CustomSaver(keras.callbacks.Callback):\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        if epoch == 1:\n",
    "            self.model.save_weights(f\"{SAVE_FOLDER}/cp-0001.h5\")\n",
    "            \n",
    "# create and use callback:\n",
    "initial_checkpoint = CustomSaver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Checkpoint callback options: save model at every k'te epochs\n",
    "\"\"\"\n",
    "# Include the epoch in the file name (uses `str.format`):\n",
    "checkpoint_path = \"saved_models/latent4/cp-{epoch:04d}.h5\"\n",
    "\n",
    "# Create a callback that saves the model's weights every k'te epochs:\n",
    "checkpoint = ModelCheckpoint(filepath = checkpoint_path,\n",
    "                             monitor='loss',           # name of the metrics to monitor\n",
    "                             verbose=1, \n",
    "                             #save_best_only=False,     # if False, the best model will not be overridden.\n",
    "                             save_weights_only=True,   # if True, only the weights of the models will be saved.\n",
    "                                                        # if False, the whole models will be saved.\n",
    "                             #mode='auto', \n",
    "                             period=200                  # save after every k'te epochs\n",
    "                            )\n",
    "\n",
    "# Save the weights using the `checkpoint_path` format\n",
    "vae.save_weights(checkpoint_path.format(epoch=0))          # save for zero epoch (inital)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Checkpoint callback options: Early stopping\n",
    "https://www.tensorflow.org/guide/keras/custom_callback\n",
    "\"\"\"\n",
    "\n",
    "class EarlyStoppingAtMinLoss(keras.callbacks.Callback):\n",
    "    \"\"\"Stop training when the loss is at its min, i.e. the loss stops decreasing.\n",
    "\n",
    "  Arguments:\n",
    "      patience: Number of epochs to wait after min has been hit. After this\n",
    "      number of no improvement, training stops.\n",
    "  \"\"\"\n",
    "\n",
    "    def __init__(self, patience=100):\n",
    "        super(EarlyStoppingAtMinLoss, self).__init__()\n",
    "        self.patience = patience\n",
    "        # best_weights to store the weights at which the minimum loss occurs.\n",
    "        self.best_weights = None\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        # The number of epoch it has waited when loss is no longer minimum.\n",
    "        self.wait = 0\n",
    "        # The epoch the training stops at.\n",
    "        self.stopped_epoch = 0\n",
    "        # Initialize the best as infinity.\n",
    "        self.best = np.Inf\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        current = logs.get(\"loss\")\n",
    "        if np.less(current, self.best):\n",
    "            self.best = current\n",
    "            self.wait = 0\n",
    "            # Record the best weights if current results is better (less).\n",
    "            self.best_weights = self.model.get_weights()\n",
    "        else:\n",
    "            self.wait += 1\n",
    "            if self.wait >= self.patience:\n",
    "                self.stopped_epoch = epoch\n",
    "                self.model.stop_training = True\n",
    "                print(\"Restoring model weights from the end of the best epoch.\")\n",
    "                self.model.set_weights(self.best_weights)\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        if self.stopped_epoch > 0:\n",
    "            print(\"Epoch %05d: early stopping\" % (self.stopped_epoch + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Create and Compile Model\"\"\"\n",
    "# import architecture of VAE model and its hyperparameters. learning rate choosen from graph above.\n",
    "vae = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      "7/7 [==============================] - 1s 192ms/step - loss: 2325.8599 - val_loss: 2266.9893\n",
      "Epoch 2/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 2131.6638 - val_loss: 1832.3625\n",
      "Epoch 3/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 1583.8553 - val_loss: 1396.6720\n",
      "Epoch 4/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 1251.4753 - val_loss: 1040.3693\n",
      "Epoch 5/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 883.4132 - val_loss: 735.4534\n",
      "Epoch 6/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 636.2025 - val_loss: 525.7544\n",
      "Epoch 7/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 467.1660 - val_loss: 418.6764\n",
      "Epoch 8/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 393.3799 - val_loss: 377.3850\n",
      "Epoch 9/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 361.3528 - val_loss: 353.9035\n",
      "Epoch 10/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 341.0907 - val_loss: 336.4652\n",
      "Epoch 11/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 323.4617 - val_loss: 320.6142\n",
      "Epoch 12/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 309.2043 - val_loss: 314.0224\n",
      "Epoch 13/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 304.3185 - val_loss: 297.1656\n",
      "Epoch 14/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 289.2025 - val_loss: 289.3736\n",
      "Epoch 15/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 283.6016 - val_loss: 289.2160\n",
      "Epoch 16/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 278.6618 - val_loss: 279.8159\n",
      "Epoch 17/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 273.1820 - val_loss: 275.3911\n",
      "Epoch 18/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 268.3324 - val_loss: 270.8080\n",
      "Epoch 19/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 264.4936 - val_loss: 275.3302\n",
      "Epoch 20/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 265.0337 - val_loss: 272.1844\n",
      "Epoch 21/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 258.9075 - val_loss: 264.1532\n",
      "Epoch 22/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 256.1916 - val_loss: 258.4980\n",
      "Epoch 23/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 252.6981 - val_loss: 254.4145\n",
      "Epoch 24/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 249.2254 - val_loss: 254.4133\n",
      "Epoch 25/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 247.9079 - val_loss: 252.8582\n",
      "Epoch 26/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 248.5360 - val_loss: 248.1293\n",
      "Epoch 27/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 244.2156 - val_loss: 244.3039\n",
      "Epoch 28/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 238.4601 - val_loss: 238.9807\n",
      "Epoch 29/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 234.7163 - val_loss: 237.1731\n",
      "Epoch 30/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 230.4250 - val_loss: 230.4895\n",
      "Epoch 31/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 226.2165 - val_loss: 226.9787\n",
      "Epoch 32/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 224.9124 - val_loss: 223.2612\n",
      "Epoch 33/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 221.2448 - val_loss: 222.5431\n",
      "Epoch 34/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 217.7932 - val_loss: 217.9286\n",
      "Epoch 35/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 214.9823 - val_loss: 215.5445\n",
      "Epoch 36/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 212.3364 - val_loss: 213.7613\n",
      "Epoch 37/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 210.2928 - val_loss: 210.7035\n",
      "Epoch 38/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 209.3312 - val_loss: 210.0401\n",
      "Epoch 39/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 207.7837 - val_loss: 207.0648\n",
      "Epoch 40/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 203.8833 - val_loss: 205.8443\n",
      "Epoch 41/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 202.7376 - val_loss: 202.6765\n",
      "Epoch 42/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 200.6502 - val_loss: 201.1584\n",
      "Epoch 43/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 199.1922 - val_loss: 198.8979\n",
      "Epoch 44/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 198.7368 - val_loss: 199.6702\n",
      "Epoch 45/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 196.0970 - val_loss: 195.3048\n",
      "Epoch 46/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 192.7592 - val_loss: 194.2294\n",
      "Epoch 47/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 192.2124 - val_loss: 193.2312\n",
      "Epoch 48/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 190.2063 - val_loss: 191.3332\n",
      "Epoch 49/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 189.5713 - val_loss: 188.8970\n",
      "Epoch 50/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 187.3477 - val_loss: 187.4532\n",
      "Epoch 51/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 185.2769 - val_loss: 186.6541\n",
      "Epoch 52/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 183.9890 - val_loss: 185.1499\n",
      "Epoch 53/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 183.1882 - val_loss: 182.4974\n",
      "Epoch 54/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 181.2381 - val_loss: 182.3108\n",
      "Epoch 55/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 179.6163 - val_loss: 179.4014\n",
      "Epoch 56/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 177.0868 - val_loss: 177.7856\n",
      "Epoch 57/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 176.2246 - val_loss: 175.9238\n",
      "Epoch 58/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 174.2872 - val_loss: 174.4491\n",
      "Epoch 59/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 172.1942 - val_loss: 172.3919\n",
      "Epoch 60/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 170.6102 - val_loss: 170.7058\n",
      "Epoch 61/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 169.9317 - val_loss: 168.8076\n",
      "Epoch 62/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 168.1672 - val_loss: 168.8320\n",
      "Epoch 63/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 166.6721 - val_loss: 166.4460\n",
      "Epoch 64/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 166.0938 - val_loss: 165.7755\n",
      "Epoch 65/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 164.4218 - val_loss: 165.2809\n",
      "Epoch 66/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 162.9089 - val_loss: 162.9619\n",
      "Epoch 67/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 161.2276 - val_loss: 162.0578\n",
      "Epoch 68/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 159.8987 - val_loss: 159.5211\n",
      "Epoch 69/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 158.2110 - val_loss: 158.6940\n",
      "Epoch 70/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 157.3016 - val_loss: 157.2335\n",
      "Epoch 71/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 156.5499 - val_loss: 155.9415\n",
      "Epoch 72/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 154.9236 - val_loss: 154.1364\n",
      "Epoch 73/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 154.1226 - val_loss: 154.6663\n",
      "Epoch 74/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 153.8495 - val_loss: 154.7002\n",
      "Epoch 75/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 151.8227 - val_loss: 152.4076\n",
      "Epoch 76/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 150.5878 - val_loss: 150.3592\n",
      "Epoch 77/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 149.2362 - val_loss: 149.7376\n",
      "Epoch 78/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 147.6925 - val_loss: 148.2307\n",
      "Epoch 79/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 147.1694 - val_loss: 146.8701\n",
      "Epoch 80/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 147.0191 - val_loss: 145.8984\n",
      "Epoch 81/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 144.8512 - val_loss: 144.6369\n",
      "Epoch 82/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 143.8593 - val_loss: 144.2510\n",
      "Epoch 83/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 142.2414 - val_loss: 142.4124\n",
      "Epoch 84/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 141.7854 - val_loss: 143.2350\n",
      "Epoch 85/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 140.6300 - val_loss: 140.8803\n",
      "Epoch 86/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 139.3586 - val_loss: 139.2937\n",
      "Epoch 87/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 138.0733 - val_loss: 137.7203\n",
      "Epoch 88/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 136.8697 - val_loss: 136.7230\n",
      "Epoch 89/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 135.8000 - val_loss: 135.7971\n",
      "Epoch 90/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 134.0336 - val_loss: 134.1936\n",
      "Epoch 91/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 133.8613 - val_loss: 133.4291\n",
      "Epoch 92/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 132.0720 - val_loss: 133.5600\n",
      "Epoch 93/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 131.8585 - val_loss: 132.0348\n",
      "Epoch 94/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 130.6811 - val_loss: 132.6723\n",
      "Epoch 95/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 130.4534 - val_loss: 129.6669\n",
      "Epoch 96/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 129.1463 - val_loss: 128.7476\n",
      "Epoch 97/2000\n",
      "7/7 [==============================] - 1s 107ms/step - loss: 127.6594 - val_loss: 128.3415\n",
      "Epoch 98/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 127.2096 - val_loss: 127.7814\n",
      "Epoch 99/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 125.8875 - val_loss: 125.9607\n",
      "Epoch 100/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 124.7775 - val_loss: 125.6208\n",
      "Epoch 101/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 124.4500 - val_loss: 124.4335\n",
      "Epoch 102/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 123.3437 - val_loss: 124.7773\n",
      "Epoch 103/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 123.0459 - val_loss: 123.0183\n",
      "Epoch 104/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 121.8896 - val_loss: 122.4910\n",
      "Epoch 105/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 120.5093 - val_loss: 120.5947\n",
      "Epoch 106/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 119.7996 - val_loss: 120.5754\n",
      "Epoch 107/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 119.3693 - val_loss: 118.5593\n",
      "Epoch 108/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 117.8473 - val_loss: 118.2308\n",
      "Epoch 109/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 117.4010 - val_loss: 118.0144\n",
      "Epoch 110/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 116.2866 - val_loss: 117.9155\n",
      "Epoch 111/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 115.2902 - val_loss: 116.0353\n",
      "Epoch 112/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 114.2152 - val_loss: 115.1842\n",
      "Epoch 113/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 114.0888 - val_loss: 114.8344\n",
      "Epoch 114/2000\n",
      "7/7 [==============================] - 1s 107ms/step - loss: 113.3567 - val_loss: 113.3926\n",
      "Epoch 115/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 113.1118 - val_loss: 113.2035\n",
      "Epoch 116/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 111.6862 - val_loss: 112.2334\n",
      "Epoch 117/2000\n",
      "7/7 [==============================] - 1s 107ms/step - loss: 110.8008 - val_loss: 111.9368\n",
      "Epoch 118/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 109.9966 - val_loss: 109.9529\n",
      "Epoch 119/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 109.2049 - val_loss: 109.5577\n",
      "Epoch 120/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 108.2736 - val_loss: 108.4958\n",
      "Epoch 121/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 108.3071 - val_loss: 109.4408\n",
      "Epoch 122/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 107.7792 - val_loss: 108.1096\n",
      "Epoch 123/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 106.8396 - val_loss: 107.3185\n",
      "Epoch 124/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 106.2181 - val_loss: 106.1665\n",
      "Epoch 125/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 105.3185 - val_loss: 106.8888\n",
      "Epoch 126/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 105.4238 - val_loss: 105.2283\n",
      "Epoch 127/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 104.1566 - val_loss: 104.2492\n",
      "Epoch 128/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 103.0169 - val_loss: 103.7790\n",
      "Epoch 129/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 102.6670 - val_loss: 103.2325\n",
      "Epoch 130/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 101.7532 - val_loss: 102.1786\n",
      "Epoch 131/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 101.3639 - val_loss: 101.9909\n",
      "Epoch 132/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 100.8509 - val_loss: 101.0486\n",
      "Epoch 133/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 100.4336 - val_loss: 100.6984\n",
      "Epoch 134/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 99.8683 - val_loss: 101.1582\n",
      "Epoch 135/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 99.7659 - val_loss: 101.3294\n",
      "Epoch 136/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 100.1602 - val_loss: 99.6944\n",
      "Epoch 137/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 97.7067 - val_loss: 98.0789\n",
      "Epoch 138/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 97.0075 - val_loss: 97.9417\n",
      "Epoch 139/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 96.6976 - val_loss: 97.4450\n",
      "Epoch 140/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 96.5831 - val_loss: 96.9260\n",
      "Epoch 141/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 95.6290 - val_loss: 96.5134\n",
      "Epoch 142/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 95.0802 - val_loss: 95.1536\n",
      "Epoch 143/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 94.9038 - val_loss: 94.7214\n",
      "Epoch 144/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 93.9351 - val_loss: 94.1950\n",
      "Epoch 145/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 93.2082 - val_loss: 93.4498\n",
      "Epoch 146/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 92.6866 - val_loss: 93.1518\n",
      "Epoch 147/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 91.9391 - val_loss: 92.7902\n",
      "Epoch 148/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 91.3327 - val_loss: 92.0058\n",
      "Epoch 149/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 91.4593 - val_loss: 90.9135\n",
      "Epoch 150/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 90.5171 - val_loss: 90.3832\n",
      "Epoch 151/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 89.7233 - val_loss: 90.9748\n",
      "Epoch 152/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 90.3072 - val_loss: 90.2996\n",
      "Epoch 153/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 89.0703 - val_loss: 90.1436\n",
      "Epoch 154/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 88.7973 - val_loss: 89.6084\n",
      "Epoch 155/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 87.7995 - val_loss: 90.5470\n",
      "Epoch 156/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 87.9493 - val_loss: 88.2673\n",
      "Epoch 157/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 87.4927 - val_loss: 89.2259\n",
      "Epoch 158/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 87.0989 - val_loss: 87.9976\n",
      "Epoch 159/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 87.3607 - val_loss: 87.0064\n",
      "Epoch 160/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 86.9311 - val_loss: 87.1094\n",
      "Epoch 161/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 85.8682 - val_loss: 86.3756\n",
      "Epoch 162/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 84.8938 - val_loss: 87.5381\n",
      "Epoch 163/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 85.9677 - val_loss: 86.1389\n",
      "Epoch 164/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 84.6218 - val_loss: 85.2462\n",
      "Epoch 165/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 83.5922 - val_loss: 84.3902\n",
      "Epoch 166/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 83.4136 - val_loss: 83.6626\n",
      "Epoch 167/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 82.8791 - val_loss: 83.9620\n",
      "Epoch 168/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 82.7280 - val_loss: 82.9049\n",
      "Epoch 169/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 81.8954 - val_loss: 83.3301\n",
      "Epoch 170/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 82.7346 - val_loss: 82.7169\n",
      "Epoch 171/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 81.2699 - val_loss: 82.4632\n",
      "Epoch 172/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 80.7955 - val_loss: 81.9987\n",
      "Epoch 173/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 80.6826 - val_loss: 81.7778\n",
      "Epoch 174/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 81.0857 - val_loss: 80.9648\n",
      "Epoch 175/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 79.7985 - val_loss: 80.0437\n",
      "Epoch 176/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 79.5181 - val_loss: 79.8822\n",
      "Epoch 177/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 78.9951 - val_loss: 79.9102\n",
      "Epoch 178/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 78.4545 - val_loss: 79.9483\n",
      "Epoch 179/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 78.1617 - val_loss: 78.8152\n",
      "Epoch 180/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 77.2589 - val_loss: 79.5569\n",
      "Epoch 181/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 77.9591 - val_loss: 79.0001\n",
      "Epoch 182/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 76.9572 - val_loss: 78.0485\n",
      "Epoch 183/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 76.6962 - val_loss: 77.7477\n",
      "Epoch 184/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 77.1359 - val_loss: 77.8451\n",
      "Epoch 185/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 76.0002 - val_loss: 79.8156\n",
      "Epoch 186/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 76.1876 - val_loss: 76.6966\n",
      "Epoch 187/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 75.4068 - val_loss: 76.3094\n",
      "Epoch 188/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 74.7689 - val_loss: 75.4577\n",
      "Epoch 189/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 74.8240 - val_loss: 75.4965\n",
      "Epoch 190/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 73.9090 - val_loss: 75.0251\n",
      "Epoch 191/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 73.5580 - val_loss: 74.7461\n",
      "Epoch 192/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 73.6104 - val_loss: 75.0956\n",
      "Epoch 193/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 73.2364 - val_loss: 74.4673\n",
      "Epoch 194/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 73.2954 - val_loss: 73.3590\n",
      "Epoch 195/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 72.9096 - val_loss: 74.7336\n",
      "Epoch 196/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 73.3971 - val_loss: 73.8402\n",
      "Epoch 197/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 72.0798 - val_loss: 73.8780\n",
      "Epoch 198/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 72.6642 - val_loss: 72.9524\n",
      "Epoch 199/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 72.3137 - val_loss: 73.1632\n",
      "Epoch 200/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 71.8082\n",
      "Epoch 00200: saving model to saved_models/latent4/cp-0200.h5\n",
      "7/7 [==============================] - 1s 146ms/step - loss: 71.8082 - val_loss: 72.4757\n",
      "Epoch 201/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 70.8005 - val_loss: 72.3891\n",
      "Epoch 202/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 70.9940 - val_loss: 72.3107\n",
      "Epoch 203/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 70.7199 - val_loss: 71.1856\n",
      "Epoch 204/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 70.1234 - val_loss: 71.2401\n",
      "Epoch 205/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 70.1751 - val_loss: 71.9259\n",
      "Epoch 206/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 70.3058 - val_loss: 70.5584\n",
      "Epoch 207/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 69.9780 - val_loss: 70.5039\n",
      "Epoch 208/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 69.1609 - val_loss: 70.7102\n",
      "Epoch 209/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 69.6073 - val_loss: 71.1510\n",
      "Epoch 210/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 69.1691 - val_loss: 70.0937\n",
      "Epoch 211/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 68.4285 - val_loss: 69.2608\n",
      "Epoch 212/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 68.6532 - val_loss: 71.2679\n",
      "Epoch 213/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 68.7009 - val_loss: 68.8758\n",
      "Epoch 214/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 67.8385 - val_loss: 70.5006\n",
      "Epoch 215/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 67.5907 - val_loss: 68.8206\n",
      "Epoch 216/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 67.1022 - val_loss: 68.2514\n",
      "Epoch 217/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 66.9395 - val_loss: 68.0563\n",
      "Epoch 218/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 66.3474 - val_loss: 69.4539\n",
      "Epoch 219/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 67.4979 - val_loss: 69.4443\n",
      "Epoch 220/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 67.6919 - val_loss: 67.8757\n",
      "Epoch 221/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 66.5443 - val_loss: 68.6497\n",
      "Epoch 222/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 66.3984 - val_loss: 68.0785\n",
      "Epoch 223/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 65.5694 - val_loss: 67.0515\n",
      "Epoch 224/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 65.3543 - val_loss: 66.6985\n",
      "Epoch 225/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 64.9402 - val_loss: 67.0173\n",
      "Epoch 226/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 64.8982 - val_loss: 66.2036\n",
      "Epoch 227/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 64.8337 - val_loss: 65.7969\n",
      "Epoch 228/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 64.8213 - val_loss: 66.0517\n",
      "Epoch 229/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 64.3062 - val_loss: 65.4228\n",
      "Epoch 230/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 64.0129 - val_loss: 65.5484\n",
      "Epoch 231/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 63.7736 - val_loss: 65.3701\n",
      "Epoch 232/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 63.3602 - val_loss: 65.1520\n",
      "Epoch 233/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 63.3417 - val_loss: 63.5627\n",
      "Epoch 234/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 62.9582 - val_loss: 64.1839\n",
      "Epoch 235/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 63.4224 - val_loss: 64.7191\n",
      "Epoch 236/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 62.6944 - val_loss: 64.3014\n",
      "Epoch 237/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 62.8224 - val_loss: 63.3344\n",
      "Epoch 238/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 62.6597 - val_loss: 63.6672\n",
      "Epoch 239/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 61.9728 - val_loss: 63.9602\n",
      "Epoch 240/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 61.9661 - val_loss: 63.2727\n",
      "Epoch 241/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 62.1482 - val_loss: 63.1534\n",
      "Epoch 242/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 62.4677 - val_loss: 63.1847\n",
      "Epoch 243/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 61.9395 - val_loss: 63.2140\n",
      "Epoch 244/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 61.2078 - val_loss: 63.6453\n",
      "Epoch 245/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 61.3169 - val_loss: 62.6250\n",
      "Epoch 246/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 61.2267 - val_loss: 62.8103\n",
      "Epoch 247/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 61.3966 - val_loss: 62.4690\n",
      "Epoch 248/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 60.8269 - val_loss: 61.3050\n",
      "Epoch 249/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 60.0748 - val_loss: 62.3660\n",
      "Epoch 250/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 60.1220 - val_loss: 61.6727\n",
      "Epoch 251/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 59.6227 - val_loss: 61.6209\n",
      "Epoch 252/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 59.6502 - val_loss: 61.0164\n",
      "Epoch 253/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 59.3550 - val_loss: 61.0891\n",
      "Epoch 254/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 59.3526 - val_loss: 62.0547\n",
      "Epoch 255/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 59.7347 - val_loss: 60.5776\n",
      "Epoch 256/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 59.2570 - val_loss: 61.0543\n",
      "Epoch 257/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 59.2720 - val_loss: 61.2965\n",
      "Epoch 258/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 59.1059 - val_loss: 61.2468\n",
      "Epoch 259/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 59.3731 - val_loss: 60.9698\n",
      "Epoch 260/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 58.5539 - val_loss: 60.2920\n",
      "Epoch 261/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 58.4949 - val_loss: 60.1322\n",
      "Epoch 262/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 58.1837 - val_loss: 59.6127\n",
      "Epoch 263/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 57.7570 - val_loss: 59.3180\n",
      "Epoch 264/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 57.4814 - val_loss: 59.2607\n",
      "Epoch 265/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 57.4942 - val_loss: 59.2161\n",
      "Epoch 266/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 56.8684 - val_loss: 59.0644\n",
      "Epoch 267/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 57.1599 - val_loss: 58.3572\n",
      "Epoch 268/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 57.1436 - val_loss: 59.2201\n",
      "Epoch 269/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 57.3016 - val_loss: 58.1019\n",
      "Epoch 270/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 57.3411 - val_loss: 59.4153\n",
      "Epoch 271/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 57.4957 - val_loss: 59.9782\n",
      "Epoch 272/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 57.1247 - val_loss: 58.1313\n",
      "Epoch 273/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 56.7931 - val_loss: 58.2139\n",
      "Epoch 274/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 56.3788 - val_loss: 57.7383\n",
      "Epoch 275/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 56.2956 - val_loss: 58.1536\n",
      "Epoch 276/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 56.0959 - val_loss: 57.2571\n",
      "Epoch 277/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 55.7056 - val_loss: 58.3942\n",
      "Epoch 278/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 55.8264 - val_loss: 57.0749\n",
      "Epoch 279/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 55.6711 - val_loss: 56.5037\n",
      "Epoch 280/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 55.0561 - val_loss: 57.2932\n",
      "Epoch 281/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 55.1161 - val_loss: 57.7885\n",
      "Epoch 282/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 55.3544 - val_loss: 57.5684\n",
      "Epoch 283/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 55.2032 - val_loss: 58.1804\n",
      "Epoch 284/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 55.4288 - val_loss: 56.5112\n",
      "Epoch 285/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 54.9214 - val_loss: 56.8200\n",
      "Epoch 286/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 54.8374 - val_loss: 57.1246\n",
      "Epoch 287/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 54.6978 - val_loss: 56.3763\n",
      "Epoch 288/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 54.6285 - val_loss: 55.9195\n",
      "Epoch 289/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 54.2178 - val_loss: 56.7648\n",
      "Epoch 290/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 54.1595 - val_loss: 56.6208\n",
      "Epoch 291/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 54.9694 - val_loss: 55.7973\n",
      "Epoch 292/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 54.0806 - val_loss: 55.1683\n",
      "Epoch 293/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 53.9219 - val_loss: 55.9385\n",
      "Epoch 294/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 53.6152 - val_loss: 55.6197\n",
      "Epoch 295/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 53.3826 - val_loss: 55.3552\n",
      "Epoch 296/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 53.4924 - val_loss: 55.2425\n",
      "Epoch 297/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 53.1978 - val_loss: 55.4093\n",
      "Epoch 298/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 53.1633 - val_loss: 54.9855\n",
      "Epoch 299/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 53.0561 - val_loss: 54.3601\n",
      "Epoch 300/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 52.8341 - val_loss: 54.6326\n",
      "Epoch 301/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 52.9485 - val_loss: 54.8828\n",
      "Epoch 302/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 53.2981 - val_loss: 55.9929\n",
      "Epoch 303/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 52.7726 - val_loss: 54.8475\n",
      "Epoch 304/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 52.7499 - val_loss: 54.6038\n",
      "Epoch 305/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 52.6170 - val_loss: 57.0853\n",
      "Epoch 306/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 53.0701 - val_loss: 54.4441\n",
      "Epoch 307/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 52.1938 - val_loss: 55.5958\n",
      "Epoch 308/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 53.3309 - val_loss: 54.6941\n",
      "Epoch 309/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 52.1950 - val_loss: 53.6837\n",
      "Epoch 310/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 51.8582 - val_loss: 54.3090\n",
      "Epoch 311/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 51.5769 - val_loss: 53.1978\n",
      "Epoch 312/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 51.4442 - val_loss: 54.9345\n",
      "Epoch 313/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 51.3829 - val_loss: 53.3825\n",
      "Epoch 314/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 51.3816 - val_loss: 53.5468\n",
      "Epoch 315/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 50.9777 - val_loss: 53.8528\n",
      "Epoch 316/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 51.3449 - val_loss: 53.2341\n",
      "Epoch 317/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 50.7916 - val_loss: 52.7929\n",
      "Epoch 318/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 50.9116 - val_loss: 52.9194\n",
      "Epoch 319/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 50.8680 - val_loss: 52.4542\n",
      "Epoch 320/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 50.6702 - val_loss: 52.3492\n",
      "Epoch 321/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 50.6297 - val_loss: 52.9224\n",
      "Epoch 322/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 50.1059 - val_loss: 52.5836\n",
      "Epoch 323/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 50.5201 - val_loss: 52.0713\n",
      "Epoch 324/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 50.7144 - val_loss: 52.7558\n",
      "Epoch 325/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 50.2964 - val_loss: 52.3287\n",
      "Epoch 326/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 50.3432 - val_loss: 52.5144\n",
      "Epoch 327/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 50.2359 - val_loss: 51.9101\n",
      "Epoch 328/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 50.0627 - val_loss: 52.2265\n",
      "Epoch 329/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 50.0720 - val_loss: 52.4199\n",
      "Epoch 330/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 50.8393 - val_loss: 52.3770\n",
      "Epoch 331/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 50.3301 - val_loss: 52.9919\n",
      "Epoch 332/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 50.1058 - val_loss: 51.6750\n",
      "Epoch 333/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 49.9054 - val_loss: 52.1847\n",
      "Epoch 334/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 49.6664 - val_loss: 51.4472\n",
      "Epoch 335/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 49.4123 - val_loss: 51.8679\n",
      "Epoch 336/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 49.1747 - val_loss: 52.2715\n",
      "Epoch 337/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 49.4448 - val_loss: 52.5031\n",
      "Epoch 338/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 49.3022 - val_loss: 52.0101\n",
      "Epoch 339/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 49.2038 - val_loss: 51.0598\n",
      "Epoch 340/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 49.2660 - val_loss: 51.2250\n",
      "Epoch 341/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 48.6707 - val_loss: 50.1986\n",
      "Epoch 342/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 48.6601 - val_loss: 50.2834\n",
      "Epoch 343/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 48.7755 - val_loss: 51.9335\n",
      "Epoch 344/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 48.4378 - val_loss: 51.0464\n",
      "Epoch 345/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 48.7983 - val_loss: 50.2001\n",
      "Epoch 346/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 48.2590 - val_loss: 50.9986\n",
      "Epoch 347/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 48.4199 - val_loss: 50.2138\n",
      "Epoch 348/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 48.2946 - val_loss: 51.0962\n",
      "Epoch 349/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 48.0612 - val_loss: 50.6929\n",
      "Epoch 350/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 48.0587 - val_loss: 50.0985\n",
      "Epoch 351/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 47.7514 - val_loss: 50.9925\n",
      "Epoch 352/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 48.0243 - val_loss: 50.7563\n",
      "Epoch 353/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 47.6992 - val_loss: 50.6965\n",
      "Epoch 354/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 47.7859 - val_loss: 50.2221\n",
      "Epoch 355/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 47.5060 - val_loss: 50.0896\n",
      "Epoch 356/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 47.9734 - val_loss: 49.6346\n",
      "Epoch 357/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 47.3106 - val_loss: 51.1048\n",
      "Epoch 358/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 47.7590 - val_loss: 51.1551\n",
      "Epoch 359/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 47.7484 - val_loss: 50.1194\n",
      "Epoch 360/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 47.5896 - val_loss: 49.5614\n",
      "Epoch 361/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 47.1748 - val_loss: 50.2341\n",
      "Epoch 362/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 46.9056 - val_loss: 50.3838\n",
      "Epoch 363/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 47.1290 - val_loss: 49.9088\n",
      "Epoch 364/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 47.2477 - val_loss: 49.8144\n",
      "Epoch 365/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 47.0467 - val_loss: 49.8282\n",
      "Epoch 366/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 47.3105 - val_loss: 49.9715\n",
      "Epoch 367/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 47.0001 - val_loss: 49.0054\n",
      "Epoch 368/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 46.9746 - val_loss: 49.2931\n",
      "Epoch 369/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 46.8500 - val_loss: 49.2666\n",
      "Epoch 370/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 46.8549 - val_loss: 48.5112\n",
      "Epoch 371/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 46.8092 - val_loss: 48.7920\n",
      "Epoch 372/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 46.7095 - val_loss: 48.5026\n",
      "Epoch 373/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 46.4410 - val_loss: 49.0518\n",
      "Epoch 374/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 46.3750 - val_loss: 48.7684\n",
      "Epoch 375/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 46.3048 - val_loss: 49.0291\n",
      "Epoch 376/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 46.5306 - val_loss: 50.2089\n",
      "Epoch 377/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 46.7533 - val_loss: 50.4238\n",
      "Epoch 378/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 46.8572 - val_loss: 49.6910\n",
      "Epoch 379/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 46.1426 - val_loss: 48.9371\n",
      "Epoch 380/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 45.9260 - val_loss: 49.3527\n",
      "Epoch 381/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 45.8712 - val_loss: 48.6315\n",
      "Epoch 382/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 45.7112 - val_loss: 49.4084\n",
      "Epoch 383/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 46.1902 - val_loss: 49.0619\n",
      "Epoch 384/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 45.7024 - val_loss: 47.7704\n",
      "Epoch 385/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 45.6833 - val_loss: 48.2147\n",
      "Epoch 386/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 45.6639 - val_loss: 48.1058\n",
      "Epoch 387/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 45.3245 - val_loss: 47.3681\n",
      "Epoch 388/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.4001 - val_loss: 47.9245\n",
      "Epoch 389/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.5741 - val_loss: 47.6980\n",
      "Epoch 390/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.6880 - val_loss: 50.6345\n",
      "Epoch 391/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.7770 - val_loss: 47.7876\n",
      "Epoch 392/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.6730 - val_loss: 47.3810\n",
      "Epoch 393/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 45.2894 - val_loss: 48.2272\n",
      "Epoch 394/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 45.0388 - val_loss: 49.1454\n",
      "Epoch 395/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 44.9502 - val_loss: 47.4108\n",
      "Epoch 396/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.0224 - val_loss: 48.5621\n",
      "Epoch 397/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.3133 - val_loss: 49.3658\n",
      "Epoch 398/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.5527 - val_loss: 49.4910\n",
      "Epoch 399/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 44.9165 - val_loss: 46.5138\n",
      "Epoch 400/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 44.6064\n",
      "Epoch 00400: saving model to saved_models/latent4/cp-0400.h5\n",
      "7/7 [==============================] - 1s 158ms/step - loss: 44.6064 - val_loss: 47.5717\n",
      "Epoch 401/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 44.3408 - val_loss: 47.4842\n",
      "Epoch 402/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.6230 - val_loss: 48.4556\n",
      "Epoch 403/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 44.5366 - val_loss: 47.3793\n",
      "Epoch 404/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.4366 - val_loss: 47.1338\n",
      "Epoch 405/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.4703 - val_loss: 47.7733\n",
      "Epoch 406/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.7563 - val_loss: 47.8739\n",
      "Epoch 407/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 44.1864 - val_loss: 47.5589\n",
      "Epoch 408/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.2368 - val_loss: 47.1706\n",
      "Epoch 409/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.2931 - val_loss: 47.1460\n",
      "Epoch 410/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.6327 - val_loss: 46.5897\n",
      "Epoch 411/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.3914 - val_loss: 47.9575\n",
      "Epoch 412/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.3988 - val_loss: 47.4621\n",
      "Epoch 413/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 44.1508 - val_loss: 48.6216\n",
      "Epoch 414/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 44.2870 - val_loss: 45.9259\n",
      "Epoch 415/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.5947 - val_loss: 46.9038\n",
      "Epoch 416/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 44.3758 - val_loss: 49.5310\n",
      "Epoch 417/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.9791 - val_loss: 48.5342\n",
      "Epoch 418/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.7815 - val_loss: 48.3569\n",
      "Epoch 419/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.2584 - val_loss: 46.2438\n",
      "Epoch 420/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 43.6478 - val_loss: 47.3390\n",
      "Epoch 421/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 43.3003 - val_loss: 47.1911\n",
      "Epoch 422/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 43.2535 - val_loss: 46.2452\n",
      "Epoch 423/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.7326 - val_loss: 46.2737\n",
      "Epoch 424/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.5932 - val_loss: 47.3608\n",
      "Epoch 425/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 43.0376 - val_loss: 46.3312\n",
      "Epoch 426/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 43.2324 - val_loss: 48.1576\n",
      "Epoch 427/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.0638 - val_loss: 48.6665\n",
      "Epoch 428/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.7257 - val_loss: 46.7793\n",
      "Epoch 429/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.3147 - val_loss: 46.7207\n",
      "Epoch 430/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.2686 - val_loss: 46.5906\n",
      "Epoch 431/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.1648 - val_loss: 46.9599\n",
      "Epoch 432/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 43.3937 - val_loss: 45.5398\n",
      "Epoch 433/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.5734 - val_loss: 46.0626\n",
      "Epoch 434/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 42.9364 - val_loss: 45.5375\n",
      "Epoch 435/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 42.5673 - val_loss: 46.6757\n",
      "Epoch 436/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 42.2587 - val_loss: 45.5251\n",
      "Epoch 437/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 42.7910 - val_loss: 46.7572\n",
      "Epoch 438/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 42.8037 - val_loss: 45.0985\n",
      "Epoch 439/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 42.8629 - val_loss: 46.3182\n",
      "Epoch 440/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 42.3848 - val_loss: 47.2538\n",
      "Epoch 441/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 42.6335 - val_loss: 46.1756\n",
      "Epoch 442/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 42.6987 - val_loss: 45.3047\n",
      "Epoch 443/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 42.4297 - val_loss: 45.5971\n",
      "Epoch 444/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 42.0558 - val_loss: 45.2325\n",
      "Epoch 445/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 42.4362 - val_loss: 46.5893\n",
      "Epoch 446/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 42.3005 - val_loss: 45.8162\n",
      "Epoch 447/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 42.3137 - val_loss: 45.8522\n",
      "Epoch 448/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 41.9460 - val_loss: 45.4343\n",
      "Epoch 449/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 42.0991 - val_loss: 45.7444\n",
      "Epoch 450/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 41.9574 - val_loss: 45.1443\n",
      "Epoch 451/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 41.8015 - val_loss: 46.1802\n",
      "Epoch 452/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 41.6467 - val_loss: 45.5087\n",
      "Epoch 453/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 41.9231 - val_loss: 44.6661\n",
      "Epoch 454/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 41.8992 - val_loss: 44.6816\n",
      "Epoch 455/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 41.8856 - val_loss: 46.1971\n",
      "Epoch 456/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 41.9684 - val_loss: 46.3335\n",
      "Epoch 457/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 41.7205 - val_loss: 45.0048\n",
      "Epoch 458/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 41.9664 - val_loss: 45.6279\n",
      "Epoch 459/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 41.6842 - val_loss: 44.6873\n",
      "Epoch 460/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 41.6629 - val_loss: 45.4060\n",
      "Epoch 461/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 41.5962 - val_loss: 45.3170\n",
      "Epoch 462/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 41.7675 - val_loss: 44.6941\n",
      "Epoch 463/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 41.4191 - val_loss: 45.9405\n",
      "Epoch 464/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 41.2533 - val_loss: 44.4095\n",
      "Epoch 465/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 41.3357 - val_loss: 43.9676\n",
      "Epoch 466/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 41.6918 - val_loss: 45.1635\n",
      "Epoch 467/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 41.1876 - val_loss: 44.8618\n",
      "Epoch 468/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 41.0534 - val_loss: 45.3141\n",
      "Epoch 469/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 41.0844 - val_loss: 45.3565\n",
      "Epoch 470/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 40.9551 - val_loss: 44.7189\n",
      "Epoch 471/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 41.1250 - val_loss: 44.0775\n",
      "Epoch 472/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 41.1589 - val_loss: 46.0028\n",
      "Epoch 473/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 41.0356 - val_loss: 44.2726\n",
      "Epoch 474/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 40.9581 - val_loss: 44.6432\n",
      "Epoch 475/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 41.0191 - val_loss: 45.0411\n",
      "Epoch 476/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 41.1050 - val_loss: 45.0494\n",
      "Epoch 477/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 41.3807 - val_loss: 44.6932\n",
      "Epoch 478/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 41.0854 - val_loss: 44.6712\n",
      "Epoch 479/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 41.3169 - val_loss: 44.8563\n",
      "Epoch 480/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 41.0623 - val_loss: 44.9866\n",
      "Epoch 481/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 41.3724 - val_loss: 45.3901\n",
      "Epoch 482/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 41.8488 - val_loss: 48.1169\n",
      "Epoch 483/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 42.3979 - val_loss: 44.6651\n",
      "Epoch 484/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 41.7056 - val_loss: 45.7310\n",
      "Epoch 485/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 41.5056 - val_loss: 45.2537\n",
      "Epoch 486/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 41.0129 - val_loss: 45.3340\n",
      "Epoch 487/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 40.7787 - val_loss: 45.1515\n",
      "Epoch 488/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 40.3017 - val_loss: 44.3586\n",
      "Epoch 489/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 40.3965 - val_loss: 45.0429\n",
      "Epoch 490/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 40.1128 - val_loss: 44.6124\n",
      "Epoch 491/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 40.4139 - val_loss: 44.7459\n",
      "Epoch 492/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 40.2505 - val_loss: 44.7846\n",
      "Epoch 493/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 40.0080 - val_loss: 44.9605\n",
      "Epoch 494/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 40.0423 - val_loss: 44.2357\n",
      "Epoch 495/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 40.2603 - val_loss: 44.0993\n",
      "Epoch 496/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 40.4702 - val_loss: 44.5600\n",
      "Epoch 497/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 40.3675 - val_loss: 44.7488\n",
      "Epoch 498/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 40.3306 - val_loss: 44.6307\n",
      "Epoch 499/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 40.8255 - val_loss: 45.6673\n",
      "Epoch 500/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 40.5107 - val_loss: 44.9187\n",
      "Epoch 501/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 40.7852 - val_loss: 45.8054\n",
      "Epoch 502/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 40.6610 - val_loss: 45.4424\n",
      "Epoch 503/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 40.5280 - val_loss: 44.9361\n",
      "Epoch 504/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 40.3159 - val_loss: 44.7647\n",
      "Epoch 505/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 40.2153 - val_loss: 43.6050\n",
      "Epoch 506/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 40.0656 - val_loss: 43.6875\n",
      "Epoch 507/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 39.9571 - val_loss: 45.4739\n",
      "Epoch 508/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 40.0397 - val_loss: 44.2589\n",
      "Epoch 509/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 39.8105 - val_loss: 44.3932\n",
      "Epoch 510/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 39.8436 - val_loss: 44.7297\n",
      "Epoch 511/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 39.8573 - val_loss: 43.4298\n",
      "Epoch 512/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 39.6996 - val_loss: 44.0733\n",
      "Epoch 513/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 39.7264 - val_loss: 45.6039\n",
      "Epoch 514/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 40.2723 - val_loss: 44.9454\n",
      "Epoch 515/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 40.3377 - val_loss: 44.8415\n",
      "Epoch 516/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 40.2555 - val_loss: 44.2301\n",
      "Epoch 517/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 40.2106 - val_loss: 43.9622\n",
      "Epoch 518/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 39.5104 - val_loss: 43.8032\n",
      "Epoch 519/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 39.6774 - val_loss: 43.8041\n",
      "Epoch 520/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 39.2240 - val_loss: 44.6734\n",
      "Epoch 521/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 39.5853 - val_loss: 43.5038\n",
      "Epoch 522/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 39.4078 - val_loss: 43.6429\n",
      "Epoch 523/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 39.3762 - val_loss: 44.8307\n",
      "Epoch 524/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 40.1880 - val_loss: 43.7324\n",
      "Epoch 525/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 40.3965 - val_loss: 44.6121\n",
      "Epoch 526/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 39.9786 - val_loss: 43.4656\n",
      "Epoch 527/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 39.6078 - val_loss: 44.9293\n",
      "Epoch 528/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 39.8671 - val_loss: 43.1882\n",
      "Epoch 529/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 39.4405 - val_loss: 44.1897\n",
      "Epoch 530/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 39.7607 - val_loss: 45.0317\n",
      "Epoch 531/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 39.1385 - val_loss: 44.3335\n",
      "Epoch 532/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 38.9313 - val_loss: 43.4172\n",
      "Epoch 533/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 38.8459 - val_loss: 44.0658\n",
      "Epoch 534/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 38.5702 - val_loss: 43.9542\n",
      "Epoch 535/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 38.6034 - val_loss: 43.4933\n",
      "Epoch 536/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 38.7995 - val_loss: 43.8558\n",
      "Epoch 537/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 38.8022 - val_loss: 43.9824\n",
      "Epoch 538/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 38.7259 - val_loss: 43.6184\n",
      "Epoch 539/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 38.5075 - val_loss: 43.8073\n",
      "Epoch 540/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 39.1917 - val_loss: 45.2249\n",
      "Epoch 541/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 39.1039 - val_loss: 43.9676\n",
      "Epoch 542/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 39.0494 - val_loss: 43.1571\n",
      "Epoch 543/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 38.9431 - val_loss: 44.1411\n",
      "Epoch 544/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 38.8715 - val_loss: 43.9878\n",
      "Epoch 545/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 38.6243 - val_loss: 43.5564\n",
      "Epoch 546/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 38.3447 - val_loss: 43.6072\n",
      "Epoch 547/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 38.2549 - val_loss: 43.2127\n",
      "Epoch 548/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 38.6700 - val_loss: 43.3004\n",
      "Epoch 549/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 38.9085 - val_loss: 43.7202\n",
      "Epoch 550/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 38.5976 - val_loss: 43.8211\n",
      "Epoch 551/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 38.8003 - val_loss: 43.2502\n",
      "Epoch 552/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 39.0607 - val_loss: 43.6607\n",
      "Epoch 553/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 38.3932 - val_loss: 44.5619\n",
      "Epoch 554/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 38.4072 - val_loss: 43.8195\n",
      "Epoch 555/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 38.4193 - val_loss: 43.2058\n",
      "Epoch 556/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 38.6441 - val_loss: 44.0208\n",
      "Epoch 557/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 38.6970 - val_loss: 45.1324\n",
      "Epoch 558/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 38.8851 - val_loss: 43.8948\n",
      "Epoch 559/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 38.4856 - val_loss: 43.4060\n",
      "Epoch 560/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 38.1129 - val_loss: 43.2198\n",
      "Epoch 561/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 38.1490 - val_loss: 43.5748\n",
      "Epoch 562/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 37.9428 - val_loss: 42.4470\n",
      "Epoch 563/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 38.1239 - val_loss: 43.9755\n",
      "Epoch 564/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 38.2359 - val_loss: 44.1740\n",
      "Epoch 565/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 38.2876 - val_loss: 42.7081\n",
      "Epoch 566/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 38.1637 - val_loss: 44.4091\n",
      "Epoch 567/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 38.0902 - val_loss: 42.9709\n",
      "Epoch 568/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 38.4599 - val_loss: 42.8460\n",
      "Epoch 569/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 37.9179 - val_loss: 43.2921\n",
      "Epoch 570/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 38.2265 - val_loss: 42.8393\n",
      "Epoch 571/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 37.9169 - val_loss: 43.7595\n",
      "Epoch 572/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 38.4317 - val_loss: 43.3969\n",
      "Epoch 573/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 38.1312 - val_loss: 42.5517\n",
      "Epoch 574/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 37.6336 - val_loss: 41.7838\n",
      "Epoch 575/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 37.8076 - val_loss: 43.6853\n",
      "Epoch 576/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 38.1225 - val_loss: 43.2351\n",
      "Epoch 577/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 37.7583 - val_loss: 42.9848\n",
      "Epoch 578/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 38.0126 - val_loss: 42.3947\n",
      "Epoch 579/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 37.6563 - val_loss: 43.4236\n",
      "Epoch 580/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 37.7397 - val_loss: 43.4972\n",
      "Epoch 581/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 37.7719 - val_loss: 43.1618\n",
      "Epoch 582/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 37.9677 - val_loss: 42.6159\n",
      "Epoch 583/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 37.9068 - val_loss: 43.3855\n",
      "Epoch 584/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 37.8640 - val_loss: 42.3892\n",
      "Epoch 585/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 37.5013 - val_loss: 42.8872\n",
      "Epoch 586/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 37.4162 - val_loss: 43.0653\n",
      "Epoch 587/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 37.5713 - val_loss: 43.3728\n",
      "Epoch 588/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 37.3275 - val_loss: 42.2524\n",
      "Epoch 589/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 37.2748 - val_loss: 43.3532\n",
      "Epoch 590/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 37.9733 - val_loss: 43.5684\n",
      "Epoch 591/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 38.0100 - val_loss: 42.3932\n",
      "Epoch 592/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 37.7637 - val_loss: 42.9594\n",
      "Epoch 593/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 37.6206 - val_loss: 44.0490\n",
      "Epoch 594/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 38.0219 - val_loss: 43.5694\n",
      "Epoch 595/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 37.7057 - val_loss: 44.0844\n",
      "Epoch 596/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 37.7349 - val_loss: 42.2148\n",
      "Epoch 597/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 37.6125 - val_loss: 43.4096\n",
      "Epoch 598/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 37.3988 - val_loss: 43.0948\n",
      "Epoch 599/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 37.4790 - val_loss: 43.1823\n",
      "Epoch 600/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 37.5850\n",
      "Epoch 00600: saving model to saved_models/latent4/cp-0600.h5\n",
      "7/7 [==============================] - 1s 141ms/step - loss: 37.5850 - val_loss: 43.7060\n",
      "Epoch 601/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 37.5333 - val_loss: 43.0702\n",
      "Epoch 602/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 37.7141 - val_loss: 42.4624\n",
      "Epoch 603/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 37.9322 - val_loss: 44.3340\n",
      "Epoch 604/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 37.4436 - val_loss: 43.2094\n",
      "Epoch 605/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 37.4302 - val_loss: 43.1246\n",
      "Epoch 606/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 37.3277 - val_loss: 43.4515\n",
      "Epoch 607/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 37.2787 - val_loss: 43.2962\n",
      "Epoch 608/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 37.1976 - val_loss: 42.6087\n",
      "Epoch 609/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 37.1474 - val_loss: 42.9082\n",
      "Epoch 610/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 37.4184 - val_loss: 42.5270\n",
      "Epoch 611/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 37.1865 - val_loss: 44.0277\n",
      "Epoch 612/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 38.0989 - val_loss: 42.6988\n",
      "Epoch 613/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 37.2889 - val_loss: 44.9365\n",
      "Epoch 614/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 37.4908 - val_loss: 42.4275\n",
      "Epoch 615/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 37.1105 - val_loss: 42.6158\n",
      "Epoch 616/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 37.0870 - val_loss: 42.3910\n",
      "Epoch 617/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 36.8090 - val_loss: 42.4320\n",
      "Epoch 618/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 36.7802 - val_loss: 42.4312\n",
      "Epoch 619/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 36.8991 - val_loss: 42.6610\n",
      "Epoch 620/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 36.9037 - val_loss: 42.2744\n",
      "Epoch 621/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 37.1040 - val_loss: 43.7768\n",
      "Epoch 622/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 36.9552 - val_loss: 42.7333\n",
      "Epoch 623/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 36.7749 - val_loss: 42.2281\n",
      "Epoch 624/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 37.0046 - val_loss: 43.5019\n",
      "Epoch 625/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 37.1307 - val_loss: 42.5770\n",
      "Epoch 626/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 36.7527 - val_loss: 43.3321\n",
      "Epoch 627/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 36.5015 - val_loss: 42.6093\n",
      "Epoch 628/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 36.9525 - val_loss: 44.8373\n",
      "Epoch 629/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 37.8842 - val_loss: 42.6503\n",
      "Epoch 630/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 37.3289 - val_loss: 42.6898\n",
      "Epoch 631/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 36.8336 - val_loss: 42.9439\n",
      "Epoch 632/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 37.0626 - val_loss: 42.2117\n",
      "Epoch 633/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 37.1189 - val_loss: 44.5334\n",
      "Epoch 634/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 37.0261 - val_loss: 43.7295\n",
      "Epoch 635/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 37.1845 - val_loss: 42.7475\n",
      "Epoch 636/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 36.6011 - val_loss: 42.0179\n",
      "Epoch 637/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 37.2425 - val_loss: 43.0366\n",
      "Epoch 638/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 36.7219 - val_loss: 43.5459\n",
      "Epoch 639/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 37.1241 - val_loss: 43.0363\n",
      "Epoch 640/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 36.6222 - val_loss: 42.4032\n",
      "Epoch 641/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 36.6663 - val_loss: 43.9832\n",
      "Epoch 642/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 36.6028 - val_loss: 42.3230\n",
      "Epoch 643/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 36.2637 - val_loss: 42.6362\n",
      "Epoch 644/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 36.1304 - val_loss: 43.1866\n",
      "Epoch 645/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 36.6551 - val_loss: 43.5271\n",
      "Epoch 646/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 36.5427 - val_loss: 42.0808\n",
      "Epoch 647/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 36.5954 - val_loss: 42.8753\n",
      "Epoch 648/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 36.7210 - val_loss: 43.6660\n",
      "Epoch 649/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 36.4638 - val_loss: 43.3317\n",
      "Epoch 650/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 36.5040 - val_loss: 42.6256\n",
      "Epoch 651/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 36.7173 - val_loss: 42.7164\n",
      "Epoch 652/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 36.3622 - val_loss: 43.3882\n",
      "Epoch 653/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 36.0922 - val_loss: 42.1230\n",
      "Epoch 654/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 36.2457 - val_loss: 42.3761\n",
      "Epoch 655/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 36.3692 - val_loss: 43.3247\n",
      "Epoch 656/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 36.8186 - val_loss: 42.6025\n",
      "Epoch 657/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 36.9749 - val_loss: 41.9527\n",
      "Epoch 658/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 36.8353 - val_loss: 42.7386\n",
      "Epoch 659/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 36.7868 - val_loss: 41.9325\n",
      "Epoch 660/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 36.1492 - val_loss: 44.1398\n",
      "Epoch 661/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 36.4049 - val_loss: 43.2510\n",
      "Epoch 662/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 36.4365 - val_loss: 42.6532\n",
      "Epoch 663/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 36.5947 - val_loss: 43.0048\n",
      "Epoch 664/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 36.3996 - val_loss: 41.7731\n",
      "Epoch 665/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 35.8113 - val_loss: 43.7770\n",
      "Epoch 666/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 36.4418 - val_loss: 42.0242\n",
      "Epoch 667/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 36.4594 - val_loss: 43.4536\n",
      "Epoch 668/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 36.2820 - val_loss: 42.4711\n",
      "Epoch 669/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 35.8255 - val_loss: 42.7736\n",
      "Epoch 670/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 35.9633 - val_loss: 45.1361\n",
      "Epoch 671/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 36.5214 - val_loss: 42.3683\n",
      "Epoch 672/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 36.3733 - val_loss: 44.1811\n",
      "Epoch 673/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 36.3568 - val_loss: 43.6479\n",
      "Epoch 674/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 36.4547 - val_loss: 44.0263\n",
      "Epoch 675/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 35.8754 - val_loss: 42.6693\n",
      "Epoch 676/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 36.3868 - val_loss: 43.4369\n",
      "Epoch 677/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 36.4352 - val_loss: 43.4743\n",
      "Epoch 678/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 36.4339 - val_loss: 42.2689\n",
      "Epoch 679/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 36.3256 - val_loss: 44.2863\n",
      "Epoch 680/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 36.6515 - val_loss: 42.0754\n",
      "Epoch 681/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 35.7551 - val_loss: 41.1704\n",
      "Epoch 682/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 35.8291 - val_loss: 43.6050\n",
      "Epoch 683/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 36.2051 - val_loss: 43.0326\n",
      "Epoch 684/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 36.6107 - val_loss: 44.6712\n",
      "Epoch 685/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 36.4537 - val_loss: 42.5125\n",
      "Epoch 686/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 36.5840 - val_loss: 44.9733\n",
      "Epoch 687/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 36.5869 - val_loss: 41.9938\n",
      "Epoch 688/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 35.7868 - val_loss: 43.1775\n",
      "Epoch 689/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 35.8005 - val_loss: 42.1191\n",
      "Epoch 690/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 35.7158 - val_loss: 42.3637\n",
      "Epoch 691/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 36.0725 - val_loss: 42.3324\n",
      "Epoch 692/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 36.1405 - val_loss: 43.4696\n",
      "Epoch 693/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 36.6616 - val_loss: 43.0066\n",
      "Epoch 694/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 36.9326 - val_loss: 42.7019\n",
      "Epoch 695/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 36.1272 - val_loss: 42.7945\n",
      "Epoch 696/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 36.1519 - val_loss: 42.5964\n",
      "Epoch 697/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 35.5474 - val_loss: 41.6550\n",
      "Epoch 698/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 35.6649 - val_loss: 42.8658\n",
      "Epoch 699/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 35.3923 - val_loss: 41.3822\n",
      "Epoch 700/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 35.6712 - val_loss: 42.5224\n",
      "Epoch 701/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 35.7925 - val_loss: 42.3799\n",
      "Epoch 702/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 35.7711 - val_loss: 42.0450\n",
      "Epoch 703/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 35.9801 - val_loss: 44.4179\n",
      "Epoch 704/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 35.9739 - val_loss: 41.8336\n",
      "Epoch 705/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 35.8696 - val_loss: 43.8885\n",
      "Epoch 706/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 35.7386 - val_loss: 42.4440\n",
      "Epoch 707/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 35.3032 - val_loss: 42.6011\n",
      "Epoch 708/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 35.4090 - val_loss: 42.2466\n",
      "Epoch 709/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 35.5968 - val_loss: 43.3526\n",
      "Epoch 710/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 35.4630 - val_loss: 41.1586\n",
      "Epoch 711/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 35.4333 - val_loss: 42.6885\n",
      "Epoch 712/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 35.5531 - val_loss: 43.9498\n",
      "Epoch 713/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 35.1308 - val_loss: 41.5865\n",
      "Epoch 714/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 35.3999 - val_loss: 42.0092\n",
      "Epoch 715/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 35.4010 - val_loss: 42.5485\n",
      "Epoch 716/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 35.7544 - val_loss: 41.7746\n",
      "Epoch 717/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 35.6306 - val_loss: 43.2215\n",
      "Epoch 718/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 35.5546 - val_loss: 42.1085\n",
      "Epoch 719/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 35.5123 - val_loss: 42.0777\n",
      "Epoch 720/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 35.7419 - val_loss: 43.2630\n",
      "Epoch 721/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 35.5938 - val_loss: 41.5446\n",
      "Epoch 722/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 35.4091 - val_loss: 42.1848\n",
      "Epoch 723/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 35.0220 - val_loss: 42.6001\n",
      "Epoch 724/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 35.1382 - val_loss: 41.5587\n",
      "Epoch 725/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 35.0660 - val_loss: 42.5864\n",
      "Epoch 726/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 35.1674 - val_loss: 45.3462\n",
      "Epoch 727/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 36.1806 - val_loss: 43.4438\n",
      "Epoch 728/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 36.9899 - val_loss: 45.2597\n",
      "Epoch 729/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 36.5983 - val_loss: 42.3516\n",
      "Epoch 730/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 35.8000 - val_loss: 43.3194\n",
      "Epoch 731/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 35.4493 - val_loss: 43.1121\n",
      "Epoch 732/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 35.6064 - val_loss: 42.1368\n",
      "Epoch 733/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 35.4679 - val_loss: 44.7225\n",
      "Epoch 734/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 35.3570 - val_loss: 42.7180\n",
      "Epoch 735/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 35.4013 - val_loss: 41.4067\n",
      "Epoch 736/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 35.8763 - val_loss: 43.5739\n",
      "Epoch 737/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 35.1631 - val_loss: 42.4372\n",
      "Epoch 738/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 35.0488 - val_loss: 43.9041\n",
      "Epoch 739/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 35.0830 - val_loss: 42.3642\n",
      "Epoch 740/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 35.0007 - val_loss: 41.4872\n",
      "Epoch 741/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 35.3340 - val_loss: 43.7474\n",
      "Epoch 742/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 35.3311 - val_loss: 41.8561\n",
      "Epoch 743/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 35.1372 - val_loss: 42.3783\n",
      "Epoch 744/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 34.9516 - val_loss: 41.5549\n",
      "Epoch 745/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 35.2085 - val_loss: 42.0684\n",
      "Epoch 746/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 35.1019 - val_loss: 44.1565\n",
      "Epoch 747/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 35.0027 - val_loss: 41.9076\n",
      "Epoch 748/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 34.7891 - val_loss: 42.6380\n",
      "Epoch 749/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 35.1597 - val_loss: 42.7088\n",
      "Epoch 750/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 35.0172 - val_loss: 42.4556\n",
      "Epoch 751/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 35.1636 - val_loss: 43.1048\n",
      "Epoch 752/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 34.7685 - val_loss: 41.9982\n",
      "Epoch 753/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 35.0098 - val_loss: 42.6426\n",
      "Epoch 754/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 35.0198 - val_loss: 42.5904\n",
      "Epoch 755/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 35.1369 - val_loss: 43.0213\n",
      "Epoch 756/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 35.3240 - val_loss: 42.8822\n",
      "Epoch 757/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 35.2630 - val_loss: 43.5889\n",
      "Epoch 758/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 34.8427 - val_loss: 41.4868\n",
      "Epoch 759/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 34.7538 - val_loss: 43.0363\n",
      "Epoch 760/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 35.5110 - val_loss: 42.4779\n",
      "Epoch 761/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 35.2996 - val_loss: 43.2046\n",
      "Epoch 762/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 35.3734 - val_loss: 42.3551\n",
      "Epoch 763/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 35.3928 - val_loss: 42.0775\n",
      "Epoch 764/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 35.0006 - val_loss: 42.6091\n",
      "Epoch 765/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 35.4653 - val_loss: 43.3547\n",
      "Epoch 766/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 35.2743 - val_loss: 41.8989\n",
      "Epoch 767/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 34.8753 - val_loss: 42.3786\n",
      "Epoch 768/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 35.0047 - val_loss: 40.9502\n",
      "Epoch 769/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 34.9808 - val_loss: 43.2630\n",
      "Epoch 770/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 34.8723 - val_loss: 43.1662\n",
      "Epoch 771/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 34.9662 - val_loss: 42.2714\n",
      "Epoch 772/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 35.1793 - val_loss: 41.9752\n",
      "Epoch 773/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 34.8935 - val_loss: 42.8459\n",
      "Epoch 774/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 35.4115 - val_loss: 41.1866\n",
      "Epoch 775/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 34.8307 - val_loss: 42.6168\n",
      "Epoch 776/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 35.0160 - val_loss: 44.2701\n",
      "Epoch 777/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 35.0365 - val_loss: 42.8692\n",
      "Epoch 778/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 34.8913 - val_loss: 42.5279\n",
      "Epoch 779/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 34.6832 - val_loss: 43.2512\n",
      "Epoch 780/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 34.5482 - val_loss: 41.3082\n",
      "Epoch 781/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 34.8102 - val_loss: 42.9699\n",
      "Epoch 782/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 35.1125 - val_loss: 42.7157\n",
      "Epoch 783/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 34.9872 - val_loss: 42.1936\n",
      "Epoch 784/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 34.7192 - val_loss: 43.0669\n",
      "Epoch 785/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 35.1895 - val_loss: 43.1835\n",
      "Epoch 786/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 34.5275 - val_loss: 41.9429\n",
      "Epoch 787/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 34.4774 - val_loss: 42.4696\n",
      "Epoch 788/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 34.4584 - val_loss: 42.2509\n",
      "Epoch 789/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 35.0902 - val_loss: 42.4741\n",
      "Epoch 790/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 34.8525 - val_loss: 43.0158\n",
      "Epoch 791/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 35.3596 - val_loss: 41.6075\n",
      "Epoch 792/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 35.1557 - val_loss: 41.8773\n",
      "Epoch 793/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 34.7857 - val_loss: 43.5149\n",
      "Epoch 794/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 34.7375 - val_loss: 42.2252\n",
      "Epoch 795/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 34.4854 - val_loss: 42.5685\n",
      "Epoch 796/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 34.0828 - val_loss: 42.9383\n",
      "Epoch 797/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 34.9211 - val_loss: 42.6929\n",
      "Epoch 798/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 34.2622 - val_loss: 42.5818\n",
      "Epoch 799/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 34.7942 - val_loss: 43.3760\n",
      "Epoch 800/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 34.8274\n",
      "Epoch 00800: saving model to saved_models/latent4/cp-0800.h5\n",
      "7/7 [==============================] - 1s 141ms/step - loss: 34.8274 - val_loss: 42.2354\n",
      "Epoch 801/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 35.3524 - val_loss: 45.4381\n",
      "Epoch 802/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 35.3903 - val_loss: 41.3674\n",
      "Epoch 803/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 35.0314 - val_loss: 42.7636\n",
      "Epoch 804/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 34.7172 - val_loss: 42.1679\n",
      "Epoch 805/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 34.7156 - val_loss: 42.6531\n",
      "Epoch 806/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 34.6556 - val_loss: 41.9122\n",
      "Epoch 807/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 34.5367 - val_loss: 43.3992\n",
      "Epoch 808/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 34.6081 - val_loss: 41.4217\n",
      "Epoch 809/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 34.1469 - val_loss: 42.8162\n",
      "Epoch 810/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 34.1540 - val_loss: 42.8174\n",
      "Epoch 811/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 34.3130 - val_loss: 42.8415\n",
      "Epoch 812/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 34.4504 - val_loss: 41.9537\n",
      "Epoch 813/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 34.0951 - val_loss: 41.6637\n",
      "Epoch 814/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 34.3970 - val_loss: 43.2044\n",
      "Epoch 815/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 34.1253 - val_loss: 41.5646\n",
      "Epoch 816/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 34.1961 - val_loss: 42.3666\n",
      "Epoch 817/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 34.5686 - val_loss: 44.0021\n",
      "Epoch 818/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 34.2150 - val_loss: 42.5161\n",
      "Epoch 819/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 34.2093 - val_loss: 42.1675\n",
      "Epoch 820/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 34.1809 - val_loss: 42.2562\n",
      "Epoch 821/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 34.0020 - val_loss: 41.9893\n",
      "Epoch 822/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 34.1121 - val_loss: 42.4426\n",
      "Epoch 823/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 34.3920 - val_loss: 42.1685\n",
      "Epoch 824/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 34.4887 - val_loss: 41.0681\n",
      "Epoch 825/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 34.2438 - val_loss: 42.3131\n",
      "Epoch 826/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 34.3520 - val_loss: 42.9682\n",
      "Epoch 827/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 34.1921 - val_loss: 42.2504\n",
      "Epoch 828/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 34.1428 - val_loss: 43.2653\n",
      "Epoch 829/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 34.5702 - val_loss: 42.7138\n",
      "Epoch 830/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 34.5725 - val_loss: 42.0464\n",
      "Epoch 831/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 34.2134 - val_loss: 41.8844\n",
      "Epoch 832/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 34.5417 - val_loss: 43.0995\n",
      "Epoch 833/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 34.2997 - val_loss: 40.9078\n",
      "Epoch 834/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 34.1586 - val_loss: 41.4915\n",
      "Epoch 835/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 34.2901 - val_loss: 44.3756\n",
      "Epoch 836/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 34.4180 - val_loss: 42.0275\n",
      "Epoch 837/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 34.3358 - val_loss: 42.3428\n",
      "Epoch 838/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 34.1530 - val_loss: 42.4418\n",
      "Epoch 839/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 34.4223 - val_loss: 42.2748\n",
      "Epoch 840/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 34.2966 - val_loss: 41.3945\n",
      "Epoch 841/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 33.9024 - val_loss: 42.2990\n",
      "Epoch 842/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 34.0978 - val_loss: 43.1501\n",
      "Epoch 843/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 33.9197 - val_loss: 42.1507\n",
      "Epoch 844/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 34.4662 - val_loss: 42.3216\n",
      "Epoch 845/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 34.0624 - val_loss: 42.9915\n",
      "Epoch 846/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 33.9398 - val_loss: 41.5882\n",
      "Epoch 847/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 33.6317 - val_loss: 42.4837\n",
      "Epoch 848/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 33.9461 - val_loss: 43.2015\n",
      "Epoch 849/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 34.1531 - val_loss: 41.6797\n",
      "Epoch 850/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 34.6498 - val_loss: 43.2442\n",
      "Epoch 851/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 34.7190 - val_loss: 42.2426\n",
      "Epoch 852/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 34.4425 - val_loss: 43.4349\n",
      "Epoch 853/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 34.4958 - val_loss: 42.3158\n",
      "Epoch 854/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 34.0222 - val_loss: 41.4041\n",
      "Epoch 855/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 33.8835 - val_loss: 42.3641\n",
      "Epoch 856/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 34.0501 - val_loss: 42.7879\n",
      "Epoch 857/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 33.9175 - val_loss: 42.1678\n",
      "Epoch 858/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 33.6656 - val_loss: 42.1175\n",
      "Epoch 859/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 33.9905 - val_loss: 42.6914\n",
      "Epoch 860/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 33.9212 - val_loss: 41.8470\n",
      "Epoch 861/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 34.0333 - val_loss: 42.5187\n",
      "Epoch 862/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 33.9445 - val_loss: 43.2079\n",
      "Epoch 863/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 33.8223 - val_loss: 41.9987\n",
      "Epoch 864/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 33.8890 - val_loss: 41.9931\n",
      "Epoch 865/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 34.0331 - val_loss: 43.1057\n",
      "Epoch 866/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 34.2438 - val_loss: 42.2020\n",
      "Epoch 867/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 34.0642 - val_loss: 42.1994\n",
      "Epoch 868/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 34.1647 - val_loss: 42.0915\n",
      "Epoch 869/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 33.9457 - val_loss: 41.5645\n",
      "Epoch 870/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 34.2471 - val_loss: 42.4630\n",
      "Epoch 871/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 33.7593 - val_loss: 41.8297\n",
      "Epoch 872/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 33.9950 - val_loss: 43.9132\n",
      "Epoch 873/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 34.4889 - val_loss: 42.3057\n",
      "Epoch 874/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 34.1366 - val_loss: 42.5842\n",
      "Epoch 875/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 33.9874 - val_loss: 42.5126\n",
      "Epoch 876/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 33.6714 - val_loss: 40.7016\n",
      "Epoch 877/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 33.5909 - val_loss: 42.6345\n",
      "Epoch 878/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 33.6288 - val_loss: 41.3905\n",
      "Epoch 879/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 33.5143 - val_loss: 42.3224\n",
      "Epoch 880/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 33.7338 - val_loss: 42.0716\n",
      "Epoch 881/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 33.4995 - val_loss: 42.9012\n",
      "Epoch 882/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 33.7988 - val_loss: 41.6414\n",
      "Epoch 883/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 33.6027 - val_loss: 42.7119\n",
      "Epoch 884/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 33.9177 - val_loss: 42.4962\n",
      "Epoch 885/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 33.9422 - val_loss: 42.4045\n",
      "Epoch 886/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 34.2802 - val_loss: 42.7134\n",
      "Epoch 887/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 34.2896 - val_loss: 42.4912\n",
      "Epoch 888/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 34.8184 - val_loss: 41.6717\n",
      "Epoch 889/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 34.4639 - val_loss: 45.6874\n",
      "Epoch 890/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 34.7880 - val_loss: 42.6437\n",
      "Epoch 891/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 34.4531 - val_loss: 41.3260\n",
      "Epoch 892/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 33.6616 - val_loss: 43.2912\n",
      "Epoch 893/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 33.7335 - val_loss: 41.8879\n",
      "Epoch 894/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 33.8098 - val_loss: 41.8970\n",
      "Epoch 895/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 33.7684 - val_loss: 41.8475\n",
      "Epoch 896/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 33.9200 - val_loss: 43.4535\n",
      "Epoch 897/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 33.5814 - val_loss: 42.0201\n",
      "Epoch 898/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 33.8920 - val_loss: 45.4638\n",
      "Epoch 899/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 34.1632 - val_loss: 42.2532\n",
      "Epoch 900/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 33.1794 - val_loss: 42.3873\n",
      "Epoch 901/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 33.5992 - val_loss: 43.0018\n",
      "Epoch 902/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 33.4480 - val_loss: 41.7213\n",
      "Epoch 903/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 33.4908 - val_loss: 42.1807\n",
      "Epoch 904/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 33.6419 - val_loss: 42.8265\n",
      "Epoch 905/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 33.5271 - val_loss: 43.4662\n",
      "Epoch 906/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 34.0313 - val_loss: 41.2431\n",
      "Epoch 907/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 34.1736 - val_loss: 42.5024\n",
      "Epoch 908/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 33.9769 - val_loss: 44.2273\n",
      "Epoch 909/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 34.6246 - val_loss: 42.3312\n",
      "Epoch 910/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 34.1830 - val_loss: 42.0658\n",
      "Epoch 911/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 34.2056 - val_loss: 42.1800\n",
      "Epoch 912/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 34.4956 - val_loss: 44.7086\n",
      "Epoch 913/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 35.1524 - val_loss: 42.2663\n",
      "Epoch 914/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 34.1010 - val_loss: 41.7825\n",
      "Epoch 915/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 34.1393 - val_loss: 42.3531\n",
      "Epoch 916/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 34.0447 - val_loss: 44.0029\n",
      "Epoch 917/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 34.0643 - val_loss: 41.9892\n",
      "Epoch 918/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 34.4484 - val_loss: 43.1353\n",
      "Epoch 919/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 34.5190 - val_loss: 43.7417\n",
      "Epoch 920/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 34.3649 - val_loss: 42.7669\n",
      "Epoch 921/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 33.4753 - val_loss: 42.8743\n",
      "Epoch 922/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 33.4513 - val_loss: 41.4507\n",
      "Epoch 923/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 33.2116 - val_loss: 42.4972\n",
      "Epoch 924/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 33.1369 - val_loss: 43.5319\n",
      "Epoch 925/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 33.5601 - val_loss: 43.4738\n",
      "Epoch 926/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 33.6140 - val_loss: 44.5764\n",
      "Epoch 927/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 33.4628 - val_loss: 41.2776\n",
      "Epoch 928/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 33.6330 - val_loss: 42.4633\n",
      "Epoch 929/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 33.5822 - val_loss: 41.6440\n",
      "Epoch 930/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 33.9231 - val_loss: 41.4190\n",
      "Epoch 931/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 33.8557 - val_loss: 44.1745\n",
      "Epoch 932/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 33.7188 - val_loss: 43.3639\n",
      "Epoch 933/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 33.6399 - val_loss: 41.6811\n",
      "Epoch 934/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 33.6398 - val_loss: 42.8932\n",
      "Epoch 935/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 33.6128 - val_loss: 41.7227\n",
      "Epoch 936/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 33.1555 - val_loss: 41.3741\n",
      "Epoch 937/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 33.2055 - val_loss: 41.1104\n",
      "Epoch 938/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 33.2995 - val_loss: 42.5951\n",
      "Epoch 939/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 32.7816 - val_loss: 44.1184\n",
      "Epoch 940/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 33.5648 - val_loss: 43.0130\n",
      "Epoch 941/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 34.3058 - val_loss: 42.5737\n",
      "Epoch 942/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 33.6124 - val_loss: 41.8960\n",
      "Epoch 943/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 33.3573 - val_loss: 41.2713\n",
      "Epoch 944/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 33.1934 - val_loss: 41.5613\n",
      "Epoch 945/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 33.1780 - val_loss: 43.0942\n",
      "Epoch 946/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 33.2221 - val_loss: 42.7189\n",
      "Epoch 947/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 33.3130 - val_loss: 41.8203\n",
      "Epoch 948/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 33.3520 - val_loss: 41.9835\n",
      "Epoch 949/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 33.2988 - val_loss: 43.5574\n",
      "Epoch 950/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 33.3357 - val_loss: 42.4476\n",
      "Epoch 951/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 33.1118 - val_loss: 41.4653\n",
      "Epoch 952/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 33.6452 - val_loss: 43.3754\n",
      "Epoch 953/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 33.2385 - val_loss: 43.8893\n",
      "Epoch 954/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 33.7756 - val_loss: 43.3819\n",
      "Epoch 955/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 33.7277 - val_loss: 43.7580\n",
      "Epoch 956/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 33.4616 - val_loss: 42.3612\n",
      "Epoch 957/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 33.0627 - val_loss: 41.8298\n",
      "Epoch 958/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 33.0386 - val_loss: 42.9050\n",
      "Epoch 959/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.9999 - val_loss: 43.8824\n",
      "Epoch 960/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 33.0925 - val_loss: 42.4765\n",
      "Epoch 961/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 33.1002 - val_loss: 43.0204\n",
      "Epoch 962/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 33.0688 - val_loss: 43.1250\n",
      "Epoch 963/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.9593 - val_loss: 42.8884\n",
      "Epoch 964/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 32.9592 - val_loss: 43.5251\n",
      "Epoch 965/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 33.3228 - val_loss: 42.5668\n",
      "Epoch 966/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 33.1129 - val_loss: 41.4560\n",
      "Epoch 967/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 33.3094 - val_loss: 43.1113\n",
      "Epoch 968/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.8225 - val_loss: 42.2425\n",
      "Epoch 969/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 32.9448 - val_loss: 42.5493\n",
      "Epoch 970/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 33.1391 - val_loss: 43.5896\n",
      "Epoch 971/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 32.9590 - val_loss: 42.3388\n",
      "Epoch 972/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 33.1981 - val_loss: 42.0605\n",
      "Epoch 973/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 33.3918 - val_loss: 43.0730\n",
      "Epoch 974/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 33.2392 - val_loss: 41.5782\n",
      "Epoch 975/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 32.9932 - val_loss: 41.6164\n",
      "Epoch 976/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.9094 - val_loss: 42.1570\n",
      "Epoch 977/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 32.9390 - val_loss: 42.6055\n",
      "Epoch 978/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 33.2293 - val_loss: 44.8760\n",
      "Epoch 979/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 33.2846 - val_loss: 43.6086\n",
      "Epoch 980/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 33.3766 - val_loss: 42.9068\n",
      "Epoch 981/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 33.0587 - val_loss: 43.1416\n",
      "Epoch 982/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 33.2894 - val_loss: 42.3822\n",
      "Epoch 983/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 33.5174 - val_loss: 42.4150\n",
      "Epoch 984/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.8148 - val_loss: 43.1197\n",
      "Epoch 985/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.9972 - val_loss: 43.3803\n",
      "Epoch 986/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 33.3430 - val_loss: 42.6468\n",
      "Epoch 987/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 33.0588 - val_loss: 43.4656\n",
      "Epoch 988/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 33.0856 - val_loss: 41.8984\n",
      "Epoch 989/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 32.9733 - val_loss: 41.6380\n",
      "Epoch 990/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 33.1373 - val_loss: 42.3643\n",
      "Epoch 991/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 33.0504 - val_loss: 43.3139\n",
      "Epoch 992/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 32.8513 - val_loss: 41.9993\n",
      "Epoch 993/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 32.9310 - val_loss: 42.6294\n",
      "Epoch 994/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.9662 - val_loss: 42.5075\n",
      "Epoch 995/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.9548 - val_loss: 42.9565\n",
      "Epoch 996/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 32.6072 - val_loss: 43.8347\n",
      "Epoch 997/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 32.8498 - val_loss: 42.9217\n",
      "Epoch 998/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 33.0924 - val_loss: 41.5614\n",
      "Epoch 999/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 32.8505 - val_loss: 42.1978\n",
      "Epoch 1000/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 32.8345\n",
      "Epoch 01000: saving model to saved_models/latent4/cp-1000.h5\n",
      "7/7 [==============================] - 1s 139ms/step - loss: 32.8345 - val_loss: 43.1880\n",
      "Epoch 1001/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 32.6700 - val_loss: 41.2628\n",
      "Epoch 1002/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 33.2227 - val_loss: 43.4699\n",
      "Epoch 1003/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 33.3245 - val_loss: 42.3106\n",
      "Epoch 1004/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 33.1654 - val_loss: 42.6118\n",
      "Epoch 1005/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.9882 - val_loss: 42.7615\n",
      "Epoch 1006/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 32.7583 - val_loss: 42.2601\n",
      "Epoch 1007/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 33.2438 - val_loss: 41.5586\n",
      "Epoch 1008/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 33.0769 - val_loss: 43.4175\n",
      "Epoch 1009/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 33.4587 - val_loss: 42.5308\n",
      "Epoch 1010/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 32.8943 - val_loss: 42.1440\n",
      "Epoch 1011/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 32.9473 - val_loss: 42.6400\n",
      "Epoch 1012/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.8844 - val_loss: 42.2981\n",
      "Epoch 1013/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 32.6475 - val_loss: 42.2416\n",
      "Epoch 1014/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.7423 - val_loss: 42.1245\n",
      "Epoch 1015/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 33.3594 - val_loss: 43.7311\n",
      "Epoch 1016/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 33.3318 - val_loss: 42.9632\n",
      "Epoch 1017/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.8484 - val_loss: 44.0373\n",
      "Epoch 1018/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 33.5119 - val_loss: 41.4865\n",
      "Epoch 1019/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 33.1077 - val_loss: 44.7208\n",
      "Epoch 1020/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 33.6295 - val_loss: 42.9063\n",
      "Epoch 1021/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 34.1018 - val_loss: 42.9197\n",
      "Epoch 1022/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 33.4623 - val_loss: 43.8068\n",
      "Epoch 1023/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 33.0833 - val_loss: 42.4309\n",
      "Epoch 1024/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 32.9035 - val_loss: 42.7083\n",
      "Epoch 1025/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 33.0495 - val_loss: 43.3515\n",
      "Epoch 1026/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 33.0686 - val_loss: 43.0107\n",
      "Epoch 1027/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.8821 - val_loss: 43.2711\n",
      "Epoch 1028/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.7769 - val_loss: 42.1113\n",
      "Epoch 1029/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 33.0554 - val_loss: 42.9724\n",
      "Epoch 1030/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.7047 - val_loss: 43.2198\n",
      "Epoch 1031/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 32.5387 - val_loss: 42.1997\n",
      "Epoch 1032/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 32.4206 - val_loss: 42.5387\n",
      "Epoch 1033/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 32.4694 - val_loss: 43.3334\n",
      "Epoch 1034/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.5044 - val_loss: 42.1513\n",
      "Epoch 1035/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.7199 - val_loss: 43.2406\n",
      "Epoch 1036/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.7729 - val_loss: 42.6662\n",
      "Epoch 1037/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 33.3154 - val_loss: 43.0452\n",
      "Epoch 1038/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 33.1790 - val_loss: 42.5991\n",
      "Epoch 1039/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 33.5700 - val_loss: 43.6294\n",
      "Epoch 1040/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 33.1805 - val_loss: 42.9550\n",
      "Epoch 1041/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 33.1698 - val_loss: 43.8612\n",
      "Epoch 1042/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 33.1146 - val_loss: 41.6115\n",
      "Epoch 1043/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.9779 - val_loss: 43.1557\n",
      "Epoch 1044/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.7281 - val_loss: 42.2158\n",
      "Epoch 1045/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 33.2325 - val_loss: 42.9508\n",
      "Epoch 1046/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.7342 - val_loss: 42.6022\n",
      "Epoch 1047/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.6964 - val_loss: 41.9245\n",
      "Epoch 1048/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.9457 - val_loss: 43.8464\n",
      "Epoch 1049/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 33.0456 - val_loss: 41.5882\n",
      "Epoch 1050/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 33.0757 - val_loss: 41.6816\n",
      "Epoch 1051/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.8129 - val_loss: 42.6286\n",
      "Epoch 1052/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.7661 - val_loss: 43.0928\n",
      "Epoch 1053/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.8596 - val_loss: 42.2320\n",
      "Epoch 1054/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.5941 - val_loss: 43.3045\n",
      "Epoch 1055/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 32.6194 - val_loss: 41.2316\n",
      "Epoch 1056/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 33.0731 - val_loss: 43.0033\n",
      "Epoch 1057/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.8808 - val_loss: 43.3731\n",
      "Epoch 1058/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 32.4046 - val_loss: 41.9359\n",
      "Epoch 1059/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 32.2403 - val_loss: 41.5609\n",
      "Epoch 1060/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.5041 - val_loss: 42.6855\n",
      "Epoch 1061/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.7999 - val_loss: 43.8635\n",
      "Epoch 1062/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 32.7085 - val_loss: 42.8772\n",
      "Epoch 1063/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.4913 - val_loss: 42.3415\n",
      "Epoch 1064/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.5010 - val_loss: 42.8666\n",
      "Epoch 1065/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.5458 - val_loss: 42.8213\n",
      "Epoch 1066/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.5301 - val_loss: 43.2583\n",
      "Epoch 1067/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.5492 - val_loss: 41.7924\n",
      "Epoch 1068/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.7717 - val_loss: 43.5793\n",
      "Epoch 1069/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 32.5782 - val_loss: 43.4288\n",
      "Epoch 1070/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 33.0787 - val_loss: 41.9705\n",
      "Epoch 1071/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.5059 - val_loss: 43.9590\n",
      "Epoch 1072/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.7244 - val_loss: 43.7772\n",
      "Epoch 1073/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.5012 - val_loss: 43.2621\n",
      "Epoch 1074/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.7002 - val_loss: 43.1748\n",
      "Epoch 1075/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 32.7404 - val_loss: 44.1482\n",
      "Epoch 1076/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 33.0253 - val_loss: 41.5942\n",
      "Epoch 1077/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.5984 - val_loss: 42.9416\n",
      "Epoch 1078/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.4856 - val_loss: 42.7955\n",
      "Epoch 1079/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.3519 - val_loss: 42.5150\n",
      "Epoch 1080/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.5911 - val_loss: 42.8654\n",
      "Epoch 1081/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 32.1872 - val_loss: 43.7043\n",
      "Epoch 1082/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 33.1454 - val_loss: 44.5806\n",
      "Epoch 1083/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 32.8736 - val_loss: 42.5667\n",
      "Epoch 1084/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 32.8538 - val_loss: 43.2535\n",
      "Epoch 1085/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.6019 - val_loss: 42.9029\n",
      "Epoch 1086/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.7516 - val_loss: 42.5123\n",
      "Epoch 1087/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.6986 - val_loss: 43.4677\n",
      "Epoch 1088/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 32.8076 - val_loss: 42.7655\n",
      "Epoch 1089/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.4176 - val_loss: 42.7428\n",
      "Epoch 1090/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.2354 - val_loss: 42.5250\n",
      "Epoch 1091/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.4294 - val_loss: 43.0074\n",
      "Epoch 1092/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 32.8424 - val_loss: 43.4016\n",
      "Epoch 1093/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 32.6326 - val_loss: 40.9914\n",
      "Epoch 1094/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.3461 - val_loss: 42.5214\n",
      "Epoch 1095/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 32.1919 - val_loss: 43.0622\n",
      "Epoch 1096/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.2338 - val_loss: 41.9869\n",
      "Epoch 1097/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 32.4915 - val_loss: 43.2660\n",
      "Epoch 1098/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.8509 - val_loss: 43.1835\n",
      "Epoch 1099/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.5048 - val_loss: 42.7239\n",
      "Epoch 1100/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.3551 - val_loss: 42.3192\n",
      "Epoch 1101/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.5115 - val_loss: 43.0615\n",
      "Epoch 1102/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 32.4368 - val_loss: 42.7289\n",
      "Epoch 1103/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 32.5646 - val_loss: 43.6512\n",
      "Epoch 1104/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.4108 - val_loss: 42.9900\n",
      "Epoch 1105/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 32.1705 - val_loss: 42.6700\n",
      "Epoch 1106/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.2301 - val_loss: 43.3551\n",
      "Epoch 1107/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.3274 - val_loss: 42.6241\n",
      "Epoch 1108/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.9562 - val_loss: 42.7637\n",
      "Epoch 1109/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 31.8877 - val_loss: 42.4891\n",
      "Epoch 1110/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.4846 - val_loss: 44.3012\n",
      "Epoch 1111/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.5298 - val_loss: 42.4789\n",
      "Epoch 1112/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 32.2812 - val_loss: 42.4197\n",
      "Epoch 1113/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 32.6130 - val_loss: 42.9898\n",
      "Epoch 1114/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.9231 - val_loss: 43.3246\n",
      "Epoch 1115/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.9516 - val_loss: 43.0625\n",
      "Epoch 1116/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 33.2390 - val_loss: 41.5380\n",
      "Epoch 1117/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.3869 - val_loss: 42.3630\n",
      "Epoch 1118/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 32.5964 - val_loss: 42.1917\n",
      "Epoch 1119/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.7418 - val_loss: 41.8952\n",
      "Epoch 1120/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.1236 - val_loss: 42.6648\n",
      "Epoch 1121/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.2595 - val_loss: 42.6681\n",
      "Epoch 1122/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 32.5049 - val_loss: 42.9510\n",
      "Epoch 1123/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 32.1327 - val_loss: 43.4058\n",
      "Epoch 1124/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 32.0018 - val_loss: 43.8685\n",
      "Epoch 1125/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.1663 - val_loss: 43.1671\n",
      "Epoch 1126/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 32.5205 - val_loss: 43.0849\n",
      "Epoch 1127/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.3700 - val_loss: 42.7897\n",
      "Epoch 1128/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.6506 - val_loss: 43.2624\n",
      "Epoch 1129/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 32.5237 - val_loss: 43.3183\n",
      "Epoch 1130/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 32.3082 - val_loss: 42.3845\n",
      "Epoch 1131/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 32.4125 - val_loss: 42.8924\n",
      "Epoch 1132/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.1488 - val_loss: 43.3703\n",
      "Epoch 1133/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 32.3796 - val_loss: 42.2161\n",
      "Epoch 1134/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 32.4977 - val_loss: 42.6602\n",
      "Epoch 1135/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 32.5235 - val_loss: 44.7640\n",
      "Epoch 1136/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 32.5221 - val_loss: 42.4902\n",
      "Epoch 1137/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.4380 - val_loss: 43.5838\n",
      "Epoch 1138/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 32.3574 - val_loss: 42.7813\n",
      "Epoch 1139/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 32.4324 - val_loss: 42.6805\n",
      "Epoch 1140/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.9922 - val_loss: 43.1909\n",
      "Epoch 1141/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 32.2229 - val_loss: 42.2696\n",
      "Epoch 1142/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 32.5289 - val_loss: 43.8791\n",
      "Epoch 1143/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 32.5201 - val_loss: 42.9067\n",
      "Epoch 1144/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 32.6746 - val_loss: 43.6238\n",
      "Epoch 1145/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 32.2214 - val_loss: 42.5911\n",
      "Epoch 1146/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 32.2488 - val_loss: 42.7641\n",
      "Epoch 1147/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.3079 - val_loss: 42.0200\n",
      "Epoch 1148/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.4412 - val_loss: 43.3263\n",
      "Epoch 1149/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.4339 - val_loss: 43.7770\n",
      "Epoch 1150/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.6578 - val_loss: 42.1251\n",
      "Epoch 1151/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 32.3580 - val_loss: 42.6018\n",
      "Epoch 1152/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 32.4095 - val_loss: 42.8914\n",
      "Epoch 1153/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.8501 - val_loss: 44.6600\n",
      "Epoch 1154/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 33.2028 - val_loss: 43.2874\n",
      "Epoch 1155/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.6550 - val_loss: 42.0372\n",
      "Epoch 1156/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.1709 - val_loss: 43.0512\n",
      "Epoch 1157/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 32.0072 - val_loss: 43.2868\n",
      "Epoch 1158/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 32.0076 - val_loss: 41.4977\n",
      "Epoch 1159/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.1790 - val_loss: 42.0274\n",
      "Epoch 1160/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 32.4972 - val_loss: 43.4612\n",
      "Epoch 1161/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.3156 - val_loss: 42.8114\n",
      "Epoch 1162/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 32.0261 - val_loss: 42.3121\n",
      "Epoch 1163/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.4989 - val_loss: 42.8807\n",
      "Epoch 1164/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.0424 - val_loss: 43.0082\n",
      "Epoch 1165/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.1100 - val_loss: 41.4905\n",
      "Epoch 1166/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.3130 - val_loss: 43.3050\n",
      "Epoch 1167/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.9463 - val_loss: 42.5245\n",
      "Epoch 1168/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 32.1015 - val_loss: 42.6404\n",
      "Epoch 1169/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 31.7936 - val_loss: 41.9807\n",
      "Epoch 1170/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.9446 - val_loss: 43.2205\n",
      "Epoch 1171/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.5990 - val_loss: 42.2292\n",
      "Epoch 1172/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.5135 - val_loss: 44.2178\n",
      "Epoch 1173/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.4052 - val_loss: 42.6986\n",
      "Epoch 1174/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.6313 - val_loss: 42.6975\n",
      "Epoch 1175/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.2754 - val_loss: 41.7773\n",
      "Epoch 1176/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.8427 - val_loss: 42.4203\n",
      "Epoch 1177/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.6706 - val_loss: 42.8083\n",
      "Epoch 1178/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.7008 - val_loss: 42.7948\n",
      "Epoch 1179/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.7353 - val_loss: 43.1291\n",
      "Epoch 1180/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.8471 - val_loss: 42.1319\n",
      "Epoch 1181/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.3140 - val_loss: 43.6273\n",
      "Epoch 1182/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.0206 - val_loss: 42.9455\n",
      "Epoch 1183/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.7057 - val_loss: 42.6545\n",
      "Epoch 1184/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.9232 - val_loss: 43.9516\n",
      "Epoch 1185/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 32.2674 - val_loss: 42.6100\n",
      "Epoch 1186/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 32.2994 - val_loss: 42.0153\n",
      "Epoch 1187/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 33.0770 - val_loss: 43.8955\n",
      "Epoch 1188/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.2251 - val_loss: 42.6688\n",
      "Epoch 1189/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.6026 - val_loss: 42.6047\n",
      "Epoch 1190/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.5969 - val_loss: 44.6832\n",
      "Epoch 1191/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.7357 - val_loss: 43.9845\n",
      "Epoch 1192/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.9033 - val_loss: 42.6370\n",
      "Epoch 1193/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.3406 - val_loss: 43.2474\n",
      "Epoch 1194/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 32.3074 - val_loss: 42.9656\n",
      "Epoch 1195/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.7876 - val_loss: 43.4041\n",
      "Epoch 1196/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.9208 - val_loss: 43.1147\n",
      "Epoch 1197/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 32.2162 - val_loss: 44.2180\n",
      "Epoch 1198/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.8817 - val_loss: 42.7467\n",
      "Epoch 1199/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 32.1364 - val_loss: 43.5622\n",
      "Epoch 1200/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 32.0045\n",
      "Epoch 01200: saving model to saved_models/latent4/cp-1200.h5\n",
      "7/7 [==============================] - 1s 155ms/step - loss: 32.0045 - val_loss: 42.5077\n",
      "Epoch 1201/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 32.1127 - val_loss: 42.9043\n",
      "Epoch 1202/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.0032 - val_loss: 43.9745\n",
      "Epoch 1203/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.5489 - val_loss: 42.5781\n",
      "Epoch 1204/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 32.6758 - val_loss: 46.6803\n",
      "Epoch 1205/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 33.1581 - val_loss: 43.7218\n",
      "Epoch 1206/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.4831 - val_loss: 42.2385\n",
      "Epoch 1207/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.9886 - val_loss: 42.7585\n",
      "Epoch 1208/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.0274 - val_loss: 43.1079\n",
      "Epoch 1209/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.7346 - val_loss: 43.0251\n",
      "Epoch 1210/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.5988 - val_loss: 42.9504\n",
      "Epoch 1211/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 31.5889 - val_loss: 42.6981\n",
      "Epoch 1212/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 31.5642 - val_loss: 43.1522\n",
      "Epoch 1213/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.8639 - val_loss: 44.2047\n",
      "Epoch 1214/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.8061 - val_loss: 44.0984\n",
      "Epoch 1215/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.3846 - val_loss: 44.1302\n",
      "Epoch 1216/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 32.0721 - val_loss: 42.9848\n",
      "Epoch 1217/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 32.0078 - val_loss: 43.0633\n",
      "Epoch 1218/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 32.2638 - val_loss: 43.9995\n",
      "Epoch 1219/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.0726 - val_loss: 43.0293\n",
      "Epoch 1220/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.7669 - val_loss: 42.7051\n",
      "Epoch 1221/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 32.1980 - val_loss: 42.7863\n",
      "Epoch 1222/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.8553 - val_loss: 43.1838\n",
      "Epoch 1223/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.6877 - val_loss: 43.2143\n",
      "Epoch 1224/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.7800 - val_loss: 42.8201\n",
      "Epoch 1225/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.4881 - val_loss: 42.7788\n",
      "Epoch 1226/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.0711 - val_loss: 43.5186\n",
      "Epoch 1227/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.7748 - val_loss: 43.5809\n",
      "Epoch 1228/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.6023 - val_loss: 43.2901\n",
      "Epoch 1229/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.9815 - val_loss: 43.8517\n",
      "Epoch 1230/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.1577 - val_loss: 46.9484\n",
      "Epoch 1231/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.9587 - val_loss: 42.8317\n",
      "Epoch 1232/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 32.6095 - val_loss: 42.2372\n",
      "Epoch 1233/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.2167 - val_loss: 43.3282\n",
      "Epoch 1234/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.9672 - val_loss: 43.7355\n",
      "Epoch 1235/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.5567 - val_loss: 43.1122\n",
      "Epoch 1236/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 31.4653 - val_loss: 42.9247\n",
      "Epoch 1237/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.5961 - val_loss: 44.0866\n",
      "Epoch 1238/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.7801 - val_loss: 43.0815\n",
      "Epoch 1239/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.6502 - val_loss: 43.6023\n",
      "Epoch 1240/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 32.1146 - val_loss: 44.1502\n",
      "Epoch 1241/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 32.3241 - val_loss: 43.9293\n",
      "Epoch 1242/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.3484 - val_loss: 43.2449\n",
      "Epoch 1243/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 32.3351 - val_loss: 43.2827\n",
      "Epoch 1244/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.8406 - val_loss: 45.4008\n",
      "Epoch 1245/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.2174 - val_loss: 44.2159\n",
      "Epoch 1246/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.1511 - val_loss: 43.8202\n",
      "Epoch 1247/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 32.0688 - val_loss: 42.5837\n",
      "Epoch 1248/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.0911 - val_loss: 42.6700\n",
      "Epoch 1249/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.8172 - val_loss: 43.5740\n",
      "Epoch 1250/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.8953 - val_loss: 43.0866\n",
      "Epoch 1251/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.7104 - val_loss: 44.2030\n",
      "Epoch 1252/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.7335 - val_loss: 42.4163\n",
      "Epoch 1253/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.7859 - val_loss: 42.2079\n",
      "Epoch 1254/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.4870 - val_loss: 42.6391\n",
      "Epoch 1255/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.1513 - val_loss: 42.2550\n",
      "Epoch 1256/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.6514 - val_loss: 41.5336\n",
      "Epoch 1257/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 31.3660 - val_loss: 42.3651\n",
      "Epoch 1258/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.6419 - val_loss: 42.4466\n",
      "Epoch 1259/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.4982 - val_loss: 42.8000\n",
      "Epoch 1260/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.4977 - val_loss: 43.8185\n",
      "Epoch 1261/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.6396 - val_loss: 42.5186\n",
      "Epoch 1262/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.7490 - val_loss: 42.8285\n",
      "Epoch 1263/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.6942 - val_loss: 44.6323\n",
      "Epoch 1264/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.8359 - val_loss: 43.1607\n",
      "Epoch 1265/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 31.2746 - val_loss: 42.7019\n",
      "Epoch 1266/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.6785 - val_loss: 44.1831\n",
      "Epoch 1267/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 32.1317 - val_loss: 42.9677\n",
      "Epoch 1268/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.8091 - val_loss: 43.3632\n",
      "Epoch 1269/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 32.0062 - val_loss: 44.6526\n",
      "Epoch 1270/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.1494 - val_loss: 42.6735\n",
      "Epoch 1271/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.6883 - val_loss: 43.0796\n",
      "Epoch 1272/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.4718 - val_loss: 42.4146\n",
      "Epoch 1273/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.3658 - val_loss: 45.0962\n",
      "Epoch 1274/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.7986 - val_loss: 43.4819\n",
      "Epoch 1275/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.7549 - val_loss: 42.6638\n",
      "Epoch 1276/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.3907 - val_loss: 43.0688\n",
      "Epoch 1277/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.4201 - val_loss: 42.5445\n",
      "Epoch 1278/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.6719 - val_loss: 43.4829\n",
      "Epoch 1279/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.7001 - val_loss: 43.8162\n",
      "Epoch 1280/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.6050 - val_loss: 44.3970\n",
      "Epoch 1281/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.5957 - val_loss: 42.8020\n",
      "Epoch 1282/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.3857 - val_loss: 43.2683\n",
      "Epoch 1283/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.6413 - val_loss: 43.5134\n",
      "Epoch 1284/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.4844 - val_loss: 43.9058\n",
      "Epoch 1285/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.6350 - val_loss: 42.8846\n",
      "Epoch 1286/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.8983 - val_loss: 43.0820\n",
      "Epoch 1287/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.6558 - val_loss: 43.4392\n",
      "Epoch 1288/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.3870 - val_loss: 44.7306\n",
      "Epoch 1289/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.5999 - val_loss: 43.0485\n",
      "Epoch 1290/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.7877 - val_loss: 43.5770\n",
      "Epoch 1291/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.7326 - val_loss: 42.4671\n",
      "Epoch 1292/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.5442 - val_loss: 43.5306\n",
      "Epoch 1293/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.5918 - val_loss: 43.8513\n",
      "Epoch 1294/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.2787 - val_loss: 43.4195\n",
      "Epoch 1295/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.5483 - val_loss: 43.7623\n",
      "Epoch 1296/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.8011 - val_loss: 43.0184\n",
      "Epoch 1297/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.5314 - val_loss: 42.9481\n",
      "Epoch 1298/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.4892 - val_loss: 43.0185\n",
      "Epoch 1299/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.3896 - val_loss: 43.3684\n",
      "Epoch 1300/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.3832 - val_loss: 42.5010\n",
      "Epoch 1301/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.3184 - val_loss: 43.4252\n",
      "Epoch 1302/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.3683 - val_loss: 43.0724\n",
      "Epoch 1303/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.2746 - val_loss: 42.6027\n",
      "Epoch 1304/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.1285 - val_loss: 44.4115\n",
      "Epoch 1305/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.6257 - val_loss: 42.2369\n",
      "Epoch 1306/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.5995 - val_loss: 44.3800\n",
      "Epoch 1307/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.4887 - val_loss: 43.8266\n",
      "Epoch 1308/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.8547 - val_loss: 43.8018\n",
      "Epoch 1309/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.4546 - val_loss: 43.0627\n",
      "Epoch 1310/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.8026 - val_loss: 43.0634\n",
      "Epoch 1311/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.6823 - val_loss: 44.3351\n",
      "Epoch 1312/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.3685 - val_loss: 42.9957\n",
      "Epoch 1313/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.5315 - val_loss: 45.1539\n",
      "Epoch 1314/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.7242 - val_loss: 44.1023\n",
      "Epoch 1315/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.6890 - val_loss: 43.4280\n",
      "Epoch 1316/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.7603 - val_loss: 43.1650\n",
      "Epoch 1317/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.8841 - val_loss: 43.0356\n",
      "Epoch 1318/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.6731 - val_loss: 43.0811\n",
      "Epoch 1319/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.4581 - val_loss: 42.4304\n",
      "Epoch 1320/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.1875 - val_loss: 43.1364\n",
      "Epoch 1321/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.0988 - val_loss: 43.3313\n",
      "Epoch 1322/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.3132 - val_loss: 42.2860\n",
      "Epoch 1323/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 31.3705 - val_loss: 44.1064\n",
      "Epoch 1324/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.2257 - val_loss: 42.5570\n",
      "Epoch 1325/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.9874 - val_loss: 43.5829\n",
      "Epoch 1326/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.9710 - val_loss: 43.3158\n",
      "Epoch 1327/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 32.2325 - val_loss: 44.9971\n",
      "Epoch 1328/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.7754 - val_loss: 42.5654\n",
      "Epoch 1329/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.9016 - val_loss: 43.6336\n",
      "Epoch 1330/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.4716 - val_loss: 43.8727\n",
      "Epoch 1331/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.7679 - val_loss: 43.4778\n",
      "Epoch 1332/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.5450 - val_loss: 43.9359\n",
      "Epoch 1333/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.5759 - val_loss: 42.3819\n",
      "Epoch 1334/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.5415 - val_loss: 42.5780\n",
      "Epoch 1335/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.8135 - val_loss: 43.8300\n",
      "Epoch 1336/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.6953 - val_loss: 44.8965\n",
      "Epoch 1337/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.5347 - val_loss: 42.9203\n",
      "Epoch 1338/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.5081 - val_loss: 41.9550\n",
      "Epoch 1339/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.3156 - val_loss: 43.5547\n",
      "Epoch 1340/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.2533 - val_loss: 43.2743\n",
      "Epoch 1341/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.1999 - val_loss: 43.2822\n",
      "Epoch 1342/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.4505 - val_loss: 43.1303\n",
      "Epoch 1343/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.3128 - val_loss: 43.2480\n",
      "Epoch 1344/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.3172 - val_loss: 41.7800\n",
      "Epoch 1345/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.0450 - val_loss: 43.5406\n",
      "Epoch 1346/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.3349 - val_loss: 43.3568\n",
      "Epoch 1347/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.3918 - val_loss: 43.6717\n",
      "Epoch 1348/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.3089 - val_loss: 42.3766\n",
      "Epoch 1349/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.1898 - val_loss: 42.9512\n",
      "Epoch 1350/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.2432 - val_loss: 42.8499\n",
      "Epoch 1351/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.2712 - val_loss: 43.6991\n",
      "Epoch 1352/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.5586 - val_loss: 42.7475\n",
      "Epoch 1353/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.5223 - val_loss: 44.9217\n",
      "Epoch 1354/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.2680 - val_loss: 42.8082\n",
      "Epoch 1355/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.1063 - val_loss: 43.4924\n",
      "Epoch 1356/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.3604 - val_loss: 45.2714\n",
      "Epoch 1357/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.6488 - val_loss: 43.0150\n",
      "Epoch 1358/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.4044 - val_loss: 42.8540\n",
      "Epoch 1359/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.1106 - val_loss: 43.6551\n",
      "Epoch 1360/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.6668 - val_loss: 44.2238\n",
      "Epoch 1361/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.2327 - val_loss: 44.2483\n",
      "Epoch 1362/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.4452 - val_loss: 41.7319\n",
      "Epoch 1363/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.4733 - val_loss: 42.8844\n",
      "Epoch 1364/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.3128 - val_loss: 42.6365\n",
      "Epoch 1365/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.0397 - val_loss: 42.9438\n",
      "Epoch 1366/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.0495 - val_loss: 44.0435\n",
      "Epoch 1367/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.2428 - val_loss: 43.8006\n",
      "Epoch 1368/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.1838 - val_loss: 41.6730\n",
      "Epoch 1369/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.3953 - val_loss: 42.9577\n",
      "Epoch 1370/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.7947 - val_loss: 43.0752\n",
      "Epoch 1371/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.5121 - val_loss: 43.3912\n",
      "Epoch 1372/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.2859 - val_loss: 43.0632\n",
      "Epoch 1373/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 31.0290 - val_loss: 43.0447\n",
      "Epoch 1374/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.1298 - val_loss: 43.0587\n",
      "Epoch 1375/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.2540 - val_loss: 43.8441\n",
      "Epoch 1376/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.0802 - val_loss: 43.1214\n",
      "Epoch 1377/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.1160 - val_loss: 42.9815\n",
      "Epoch 1378/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 30.8157 - val_loss: 43.4375\n",
      "Epoch 1379/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.4363 - val_loss: 42.9345\n",
      "Epoch 1380/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.2563 - val_loss: 44.5260\n",
      "Epoch 1381/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.9269 - val_loss: 43.1839\n",
      "Epoch 1382/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.9694 - val_loss: 43.6216\n",
      "Epoch 1383/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.3078 - val_loss: 44.3461\n",
      "Epoch 1384/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.0828 - val_loss: 43.6546\n",
      "Epoch 1385/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 31.4971 - val_loss: 42.4511\n",
      "Epoch 1386/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.5180 - val_loss: 43.8740\n",
      "Epoch 1387/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.3642 - val_loss: 43.5192\n",
      "Epoch 1388/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.5103 - val_loss: 43.4526\n",
      "Epoch 1389/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.4824 - val_loss: 42.8463\n",
      "Epoch 1390/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.3780 - val_loss: 44.0252\n",
      "Epoch 1391/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.4465 - val_loss: 44.7031\n",
      "Epoch 1392/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.2130 - val_loss: 43.3642\n",
      "Epoch 1393/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.0953 - val_loss: 43.3211\n",
      "Epoch 1394/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.2359 - val_loss: 43.3260\n",
      "Epoch 1395/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.4103 - val_loss: 44.7269\n",
      "Epoch 1396/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.1934 - val_loss: 43.6613\n",
      "Epoch 1397/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.3050 - val_loss: 41.9770\n",
      "Epoch 1398/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.1635 - val_loss: 43.4473\n",
      "Epoch 1399/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.0024 - val_loss: 42.9014\n",
      "Epoch 1400/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 31.0653\n",
      "Epoch 01400: saving model to saved_models/latent4/cp-1400.h5\n",
      "7/7 [==============================] - 1s 139ms/step - loss: 31.0653 - val_loss: 43.3371\n",
      "Epoch 1401/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.9358 - val_loss: 42.6889\n",
      "Epoch 1402/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.4350 - val_loss: 43.9595\n",
      "Epoch 1403/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.8803 - val_loss: 43.5970\n",
      "Epoch 1404/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.1437 - val_loss: 43.4647\n",
      "Epoch 1405/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.0139 - val_loss: 43.6816\n",
      "Epoch 1406/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.1828 - val_loss: 45.6377\n",
      "Epoch 1407/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.2468 - val_loss: 42.9178\n",
      "Epoch 1408/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.2113 - val_loss: 44.1424\n",
      "Epoch 1409/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.1752 - val_loss: 42.9225\n",
      "Epoch 1410/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 31.0505 - val_loss: 43.5968\n",
      "Epoch 1411/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.8426 - val_loss: 44.6470\n",
      "Epoch 1412/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.7600 - val_loss: 43.7598\n",
      "Epoch 1413/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.7117 - val_loss: 45.3363\n",
      "Epoch 1414/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.6836 - val_loss: 43.9994\n",
      "Epoch 1415/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.9368 - val_loss: 45.5263\n",
      "Epoch 1416/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.9733 - val_loss: 42.6908\n",
      "Epoch 1417/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.2186 - val_loss: 43.2901\n",
      "Epoch 1418/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.5196 - val_loss: 44.3343\n",
      "Epoch 1419/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 30.8999 - val_loss: 43.1150\n",
      "Epoch 1420/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.2109 - val_loss: 42.7623\n",
      "Epoch 1421/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.1271 - val_loss: 43.1562\n",
      "Epoch 1422/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.3955 - val_loss: 43.8326\n",
      "Epoch 1423/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.9348 - val_loss: 42.8530\n",
      "Epoch 1424/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.9248 - val_loss: 43.5349\n",
      "Epoch 1425/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.7928 - val_loss: 44.2732\n",
      "Epoch 1426/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.0786 - val_loss: 43.3642\n",
      "Epoch 1427/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.0845 - val_loss: 43.2573\n",
      "Epoch 1428/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.8993 - val_loss: 43.6691\n",
      "Epoch 1429/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.9254 - val_loss: 43.4112\n",
      "Epoch 1430/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.8449 - val_loss: 43.5352\n",
      "Epoch 1431/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.9496 - val_loss: 43.4295\n",
      "Epoch 1432/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.0863 - val_loss: 43.9960\n",
      "Epoch 1433/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.3017 - val_loss: 44.5257\n",
      "Epoch 1434/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.1837 - val_loss: 44.0149\n",
      "Epoch 1435/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.7053 - val_loss: 43.8040\n",
      "Epoch 1436/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.0503 - val_loss: 43.1891\n",
      "Epoch 1437/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.8686 - val_loss: 44.4206\n",
      "Epoch 1438/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.8372 - val_loss: 43.6837\n",
      "Epoch 1439/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.0718 - val_loss: 43.8453\n",
      "Epoch 1440/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.8933 - val_loss: 44.0611\n",
      "Epoch 1441/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.9922 - val_loss: 44.0763\n",
      "Epoch 1442/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.8591 - val_loss: 43.7637\n",
      "Epoch 1443/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.7343 - val_loss: 43.5014\n",
      "Epoch 1444/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.0227 - val_loss: 43.2700\n",
      "Epoch 1445/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.1047 - val_loss: 44.3720\n",
      "Epoch 1446/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.8105 - val_loss: 44.6921\n",
      "Epoch 1447/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.0363 - val_loss: 42.7073\n",
      "Epoch 1448/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.1210 - val_loss: 42.7151\n",
      "Epoch 1449/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.9991 - val_loss: 44.1691\n",
      "Epoch 1450/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.9379 - val_loss: 44.2736\n",
      "Epoch 1451/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.9898 - val_loss: 43.5392\n",
      "Epoch 1452/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.0493 - val_loss: 43.1942\n",
      "Epoch 1453/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.9504 - val_loss: 43.3293\n",
      "Epoch 1454/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.1255 - val_loss: 44.5823\n",
      "Epoch 1455/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.3571 - val_loss: 44.3662\n",
      "Epoch 1456/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.5490 - val_loss: 43.1366\n",
      "Epoch 1457/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.1406 - val_loss: 42.8354\n",
      "Epoch 1458/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.1305 - val_loss: 43.3835\n",
      "Epoch 1459/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.7536 - val_loss: 43.6958\n",
      "Epoch 1460/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.8574 - val_loss: 43.4134\n",
      "Epoch 1461/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.2552 - val_loss: 44.7877\n",
      "Epoch 1462/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.4054 - val_loss: 43.2472\n",
      "Epoch 1463/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.5325 - val_loss: 44.3024\n",
      "Epoch 1464/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.3634 - val_loss: 43.5994\n",
      "Epoch 1465/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.4831 - val_loss: 44.1253\n",
      "Epoch 1466/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.2030 - val_loss: 45.7303\n",
      "Epoch 1467/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.5506 - val_loss: 43.1803\n",
      "Epoch 1468/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.2991 - val_loss: 44.0655\n",
      "Epoch 1469/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.7737 - val_loss: 44.0126\n",
      "Epoch 1470/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.1435 - val_loss: 44.2019\n",
      "Epoch 1471/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.2510 - val_loss: 43.9536\n",
      "Epoch 1472/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.7518 - val_loss: 44.4730\n",
      "Epoch 1473/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.2042 - val_loss: 44.5502\n",
      "Epoch 1474/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.4188 - val_loss: 44.7597\n",
      "Epoch 1475/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.1957 - val_loss: 44.4736\n",
      "Epoch 1476/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.0765 - val_loss: 42.5829\n",
      "Epoch 1477/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.1328 - val_loss: 43.7989\n",
      "Epoch 1478/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.7889 - val_loss: 44.8250\n",
      "Epoch 1479/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.8775 - val_loss: 41.8807\n",
      "Epoch 1480/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.8411 - val_loss: 45.2945\n",
      "Epoch 1481/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.5741 - val_loss: 44.8170\n",
      "Epoch 1482/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.8901 - val_loss: 45.4607\n",
      "Epoch 1483/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.5257 - val_loss: 44.5233\n",
      "Epoch 1484/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.4207 - val_loss: 43.0649\n",
      "Epoch 1485/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.1931 - val_loss: 45.3614\n",
      "Epoch 1486/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.8388 - val_loss: 43.9439\n",
      "Epoch 1487/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.7647 - val_loss: 45.0838\n",
      "Epoch 1488/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.7081 - val_loss: 43.7278\n",
      "Epoch 1489/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.7488 - val_loss: 44.0166\n",
      "Epoch 1490/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.7118 - val_loss: 43.1681\n",
      "Epoch 1491/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.7612 - val_loss: 43.0454\n",
      "Epoch 1492/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 30.5988 - val_loss: 44.2281\n",
      "Epoch 1493/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.9121 - val_loss: 43.2214\n",
      "Epoch 1494/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.6269 - val_loss: 43.7299\n",
      "Epoch 1495/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.8094 - val_loss: 43.5173\n",
      "Epoch 1496/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.1847 - val_loss: 43.7341\n",
      "Epoch 1497/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.8831 - val_loss: 42.9819\n",
      "Epoch 1498/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.6230 - val_loss: 42.5030\n",
      "Epoch 1499/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.7682 - val_loss: 43.5166\n",
      "Epoch 1500/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.6782 - val_loss: 44.5397\n",
      "Epoch 1501/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.0017 - val_loss: 44.5372\n",
      "Epoch 1502/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.8554 - val_loss: 44.2778\n",
      "Epoch 1503/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.7944 - val_loss: 44.7961\n",
      "Epoch 1504/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.7125 - val_loss: 44.0148\n",
      "Epoch 1505/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.7951 - val_loss: 44.0761\n",
      "Epoch 1506/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 30.4890 - val_loss: 43.0688\n",
      "Epoch 1507/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.0486 - val_loss: 45.3065\n",
      "Epoch 1508/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.8604 - val_loss: 43.9163\n",
      "Epoch 1509/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.8581 - val_loss: 44.1143\n",
      "Epoch 1510/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.7900 - val_loss: 44.0072\n",
      "Epoch 1511/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.4992 - val_loss: 43.6439\n",
      "Epoch 1512/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.8107 - val_loss: 43.9170\n",
      "Epoch 1513/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.1171 - val_loss: 43.8928\n",
      "Epoch 1514/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.6991 - val_loss: 43.2075\n",
      "Epoch 1515/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.6562 - val_loss: 43.9140\n",
      "Epoch 1516/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.7062 - val_loss: 45.3730\n",
      "Epoch 1517/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.2070 - val_loss: 44.5084\n",
      "Epoch 1518/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.8566 - val_loss: 43.9735\n",
      "Epoch 1519/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.4027 - val_loss: 43.9688\n",
      "Epoch 1520/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.7844 - val_loss: 44.4211\n",
      "Epoch 1521/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.0476 - val_loss: 43.6094\n",
      "Epoch 1522/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.9124 - val_loss: 43.2809\n",
      "Epoch 1523/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.8425 - val_loss: 44.2851\n",
      "Epoch 1524/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.8230 - val_loss: 45.1004\n",
      "Epoch 1525/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.8135 - val_loss: 43.8165\n",
      "Epoch 1526/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.1012 - val_loss: 42.8163\n",
      "Epoch 1527/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.2358 - val_loss: 44.6564\n",
      "Epoch 1528/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.2281 - val_loss: 44.6216\n",
      "Epoch 1529/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.9965 - val_loss: 44.9140\n",
      "Epoch 1530/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.0772 - val_loss: 44.6197\n",
      "Epoch 1531/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.1179 - val_loss: 42.5504\n",
      "Epoch 1532/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.9880 - val_loss: 43.5660\n",
      "Epoch 1533/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.6678 - val_loss: 42.9895\n",
      "Epoch 1534/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.0369 - val_loss: 44.9964\n",
      "Epoch 1535/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.4614 - val_loss: 45.0933\n",
      "Epoch 1536/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.1123 - val_loss: 44.3998\n",
      "Epoch 1537/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.6119 - val_loss: 43.2542\n",
      "Epoch 1538/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.9142 - val_loss: 45.4683\n",
      "Epoch 1539/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.0993 - val_loss: 44.1042\n",
      "Epoch 1540/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.8243 - val_loss: 44.1013\n",
      "Epoch 1541/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.6270 - val_loss: 44.6233\n",
      "Epoch 1542/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.7912 - val_loss: 44.3971\n",
      "Epoch 1543/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.6973 - val_loss: 44.2060\n",
      "Epoch 1544/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.6719 - val_loss: 44.3773\n",
      "Epoch 1545/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.8258 - val_loss: 43.1664\n",
      "Epoch 1546/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.6372 - val_loss: 44.8431\n",
      "Epoch 1547/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.8435 - val_loss: 44.3704\n",
      "Epoch 1548/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.7866 - val_loss: 43.4352\n",
      "Epoch 1549/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.0401 - val_loss: 43.0326\n",
      "Epoch 1550/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.8229 - val_loss: 43.9978\n",
      "Epoch 1551/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.5148 - val_loss: 43.6673\n",
      "Epoch 1552/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.6633 - val_loss: 45.3579\n",
      "Epoch 1553/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.6395 - val_loss: 42.8002\n",
      "Epoch 1554/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.9279 - val_loss: 43.1643\n",
      "Epoch 1555/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.0533 - val_loss: 46.3803\n",
      "Epoch 1556/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.4750 - val_loss: 44.2944\n",
      "Epoch 1557/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.9548 - val_loss: 43.7485\n",
      "Epoch 1558/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.5563 - val_loss: 46.4871\n",
      "Epoch 1559/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 31.1326 - val_loss: 43.3490\n",
      "Epoch 1560/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.5418 - val_loss: 44.6717\n",
      "Epoch 1561/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 30.4866 - val_loss: 43.7056\n",
      "Epoch 1562/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.5701 - val_loss: 43.3835\n",
      "Epoch 1563/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.5458 - val_loss: 43.2430\n",
      "Epoch 1564/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.7677 - val_loss: 44.7870\n",
      "Epoch 1565/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.6899 - val_loss: 44.0541\n",
      "Epoch 1566/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.5628 - val_loss: 44.3613\n",
      "Epoch 1567/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.5525 - val_loss: 44.0663\n",
      "Epoch 1568/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 30.3728 - val_loss: 44.1019\n",
      "Epoch 1569/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.6167 - val_loss: 45.1397\n",
      "Epoch 1570/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.5615 - val_loss: 43.1077\n",
      "Epoch 1571/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.5555 - val_loss: 43.7587\n",
      "Epoch 1572/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.7733 - val_loss: 44.1455\n",
      "Epoch 1573/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.7390 - val_loss: 44.2161\n",
      "Epoch 1574/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.5034 - val_loss: 44.2256\n",
      "Epoch 1575/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.4084 - val_loss: 43.0174\n",
      "Epoch 1576/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.5625 - val_loss: 43.4182\n",
      "Epoch 1577/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.3938 - val_loss: 43.3281\n",
      "Epoch 1578/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.6468 - val_loss: 42.7491\n",
      "Epoch 1579/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.3965 - val_loss: 44.7444\n",
      "Epoch 1580/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.5208 - val_loss: 43.9997\n",
      "Epoch 1581/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.8146 - val_loss: 44.1032\n",
      "Epoch 1582/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.7673 - val_loss: 43.9243\n",
      "Epoch 1583/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.7205 - val_loss: 44.6599\n",
      "Epoch 1584/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.7430 - val_loss: 43.6638\n",
      "Epoch 1585/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.0333 - val_loss: 46.0068\n",
      "Epoch 1586/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.1803 - val_loss: 43.8165\n",
      "Epoch 1587/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.7103 - val_loss: 44.6059\n",
      "Epoch 1588/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.6595 - val_loss: 44.5134\n",
      "Epoch 1589/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.4316 - val_loss: 43.7754\n",
      "Epoch 1590/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.5407 - val_loss: 43.2151\n",
      "Epoch 1591/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.2588 - val_loss: 43.3322\n",
      "Epoch 1592/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 29.9998 - val_loss: 45.1036\n",
      "Epoch 1593/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.1843 - val_loss: 44.3005\n",
      "Epoch 1594/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.5004 - val_loss: 43.4472\n",
      "Epoch 1595/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.6798 - val_loss: 44.2715\n",
      "Epoch 1596/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.7236 - val_loss: 45.5971\n",
      "Epoch 1597/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.9701 - val_loss: 43.6183\n",
      "Epoch 1598/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.5822 - val_loss: 44.4512\n",
      "Epoch 1599/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.5586 - val_loss: 43.2176\n",
      "Epoch 1600/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 30.5175\n",
      "Epoch 01600: saving model to saved_models/latent4/cp-1600.h5\n",
      "7/7 [==============================] - 1s 145ms/step - loss: 30.5175 - val_loss: 44.3723\n",
      "Epoch 1601/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.3148 - val_loss: 43.6028\n",
      "Epoch 1602/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.4136 - val_loss: 44.7632\n",
      "Epoch 1603/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.1198 - val_loss: 42.9300\n",
      "Epoch 1604/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.6002 - val_loss: 44.1329\n",
      "Epoch 1605/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.5438 - val_loss: 43.3421\n",
      "Epoch 1606/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.4659 - val_loss: 44.4256\n",
      "Epoch 1607/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.6925 - val_loss: 43.7774\n",
      "Epoch 1608/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.5450 - val_loss: 43.9279\n",
      "Epoch 1609/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.6810 - val_loss: 45.2470\n",
      "Epoch 1610/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.4579 - val_loss: 44.4971\n",
      "Epoch 1611/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.5744 - val_loss: 43.9537\n",
      "Epoch 1612/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.4160 - val_loss: 44.1447\n",
      "Epoch 1613/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.5796 - val_loss: 45.7388\n",
      "Epoch 1614/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.6652 - val_loss: 43.8643\n",
      "Epoch 1615/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.6823 - val_loss: 44.3746\n",
      "Epoch 1616/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.4774 - val_loss: 44.7388\n",
      "Epoch 1617/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.3837 - val_loss: 44.1708\n",
      "Epoch 1618/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.2661 - val_loss: 43.8083\n",
      "Epoch 1619/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.9856 - val_loss: 44.9192\n",
      "Epoch 1620/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.8298 - val_loss: 43.9151\n",
      "Epoch 1621/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.1791 - val_loss: 46.4227\n",
      "Epoch 1622/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.6054 - val_loss: 45.1773\n",
      "Epoch 1623/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.3412 - val_loss: 44.3775\n",
      "Epoch 1624/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.4701 - val_loss: 43.5247\n",
      "Epoch 1625/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.9698 - val_loss: 45.2244\n",
      "Epoch 1626/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.2959 - val_loss: 44.1747\n",
      "Epoch 1627/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.0307 - val_loss: 43.4133\n",
      "Epoch 1628/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.9055 - val_loss: 44.0254\n",
      "Epoch 1629/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.9544 - val_loss: 45.5567\n",
      "Epoch 1630/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.4144 - val_loss: 44.5423\n",
      "Epoch 1631/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.3808 - val_loss: 43.9896\n",
      "Epoch 1632/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.5713 - val_loss: 43.8079\n",
      "Epoch 1633/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.3449 - val_loss: 44.2419\n",
      "Epoch 1634/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.4080 - val_loss: 44.5245\n",
      "Epoch 1635/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.4045 - val_loss: 43.0895\n",
      "Epoch 1636/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.7236 - val_loss: 44.2193\n",
      "Epoch 1637/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.4287 - val_loss: 43.3960\n",
      "Epoch 1638/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.4719 - val_loss: 45.0015\n",
      "Epoch 1639/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.4227 - val_loss: 43.8151\n",
      "Epoch 1640/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.2961 - val_loss: 43.8135\n",
      "Epoch 1641/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.4460 - val_loss: 45.1715\n",
      "Epoch 1642/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.3718 - val_loss: 44.0089\n",
      "Epoch 1643/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.3283 - val_loss: 44.1737\n",
      "Epoch 1644/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.1469 - val_loss: 44.4320\n",
      "Epoch 1645/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.1808 - val_loss: 44.8558\n",
      "Epoch 1646/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.2316 - val_loss: 44.9036\n",
      "Epoch 1647/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.5418 - val_loss: 43.6781\n",
      "Epoch 1648/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.4523 - val_loss: 44.6911\n",
      "Epoch 1649/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.2688 - val_loss: 44.0280\n",
      "Epoch 1650/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.3378 - val_loss: 45.6057\n",
      "Epoch 1651/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.6676 - val_loss: 43.1446\n",
      "Epoch 1652/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.2398 - val_loss: 43.4487\n",
      "Epoch 1653/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.5527 - val_loss: 43.7770\n",
      "Epoch 1654/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.8780 - val_loss: 44.1437\n",
      "Epoch 1655/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.8149 - val_loss: 45.4282\n",
      "Epoch 1656/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.4570 - val_loss: 44.7350\n",
      "Epoch 1657/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.5681 - val_loss: 45.2730\n",
      "Epoch 1658/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.5954 - val_loss: 43.4181\n",
      "Epoch 1659/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.2108 - val_loss: 43.1404\n",
      "Epoch 1660/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.4636 - val_loss: 44.7785\n",
      "Epoch 1661/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.8651 - val_loss: 44.7055\n",
      "Epoch 1662/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.4226 - val_loss: 46.1093\n",
      "Epoch 1663/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.3976 - val_loss: 44.1877\n",
      "Epoch 1664/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.3349 - val_loss: 44.3505\n",
      "Epoch 1665/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.3350 - val_loss: 44.0194\n",
      "Epoch 1666/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.4496 - val_loss: 45.4184\n",
      "Epoch 1667/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.4244 - val_loss: 44.2744\n",
      "Epoch 1668/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.2936 - val_loss: 43.4062\n",
      "Epoch 1669/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.3657 - val_loss: 45.0147\n",
      "Epoch 1670/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.4716 - val_loss: 44.8618\n",
      "Epoch 1671/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.4486 - val_loss: 43.6366\n",
      "Epoch 1672/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.4877 - val_loss: 44.7290\n",
      "Epoch 1673/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.5313 - val_loss: 44.3634\n",
      "Epoch 1674/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.2875 - val_loss: 45.9841\n",
      "Epoch 1675/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.6973 - val_loss: 44.6401\n",
      "Epoch 1676/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.0938 - val_loss: 43.6079\n",
      "Epoch 1677/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.0764 - val_loss: 44.6828\n",
      "Epoch 1678/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.0281 - val_loss: 45.0187\n",
      "Epoch 1679/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.3718 - val_loss: 46.3841\n",
      "Epoch 1680/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.4259 - val_loss: 43.7924\n",
      "Epoch 1681/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.4394 - val_loss: 44.1438\n",
      "Epoch 1682/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.6081 - val_loss: 44.3895\n",
      "Epoch 1683/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.6811 - val_loss: 44.4788\n",
      "Epoch 1684/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.3739 - val_loss: 44.2020\n",
      "Epoch 1685/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.8859 - val_loss: 46.1270\n",
      "Epoch 1686/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.8459 - val_loss: 45.5189\n",
      "Epoch 1687/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.8975 - val_loss: 45.2911\n",
      "Epoch 1688/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.6637 - val_loss: 44.0699\n",
      "Epoch 1689/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.2188 - val_loss: 44.2454\n",
      "Epoch 1690/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.5248 - val_loss: 44.1094\n",
      "Epoch 1691/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.5457 - val_loss: 45.7215\n",
      "Epoch 1692/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 30.3680Restoring model weights from the end of the best epoch.\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.3680 - val_loss: 45.4839\n",
      "Epoch 01692: early stopping\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Train VAE model with callbacks \"\"\"\n",
    "history = vae.fit(trainX, trainX, \n",
    "                  batch_size = batch_size, \n",
    "                  epochs = epochs, \n",
    "                  callbacks=[initial_checkpoint, checkpoint, EarlyStoppingAtMinLoss()],\n",
    "                  #callbacks=[initial_checkpoint, checkpoint],\n",
    "                  shuffle=True,\n",
    "                  validation_data=(valX, valX)\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step size: 7.0\n"
     ]
    }
   ],
   "source": [
    "# In this GPU implementation, it shows the number of batches/iterations for one epoch (and not the training samples), which is:\n",
    "print (\"Step size:\", np.ceil(trainX.shape[0]/batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Save the entire model \"\"\"\n",
    "# Save notes about hyperparameters used:\n",
    "epochs = len(history.history['loss'])\n",
    "with open(f'{SAVE_FOLDER}/notes.txt', 'w') as f:\n",
    "    f.write(f'Epochs: {epochs} \\n')\n",
    "    f.write(f'Batch size: {batch_size} \\n')\n",
    "    f.write(f'Latent dimension: {latent_dim} \\n')\n",
    "    f.write(f'Filter sizes: {filters} \\n')\n",
    "    f.write(f'img_width: {img_width} \\n')\n",
    "    f.write(f'img_height: {img_height} \\n')\n",
    "    f.write(f'num_channels: {num_channels}')\n",
    "    f.close()\n",
    "\n",
    "# Save weights for encoder model:\n",
    "encoder.save_weights(f'{SAVE_FOLDER}/ENCODER_WEIGHTS.h5')\n",
    "\n",
    "# Save weights for decoder model:\n",
    "decoder.save_weights(f'{SAVE_FOLDER}/DECODER_WEIGHTS.h5')\n",
    "\n",
    "# Save weights for total VAE:\n",
    "vae.save_weights(f'{SAVE_FOLDER}/VAE_WEIGHTS.h5')\n",
    "\n",
    "# Save the history at each epochs (cost of train and validation sets):\n",
    "np.save(f'{SAVE_FOLDER}/HISTORY.npy', history.history)\n",
    "\n",
    "# Save exact training, validation and testing data set (as numpy arrays):\n",
    "np.save(f'{SAVE_FOLDER}/trainX.npy', trainX)\n",
    "np.save(f'{SAVE_FOLDER}/valX.npy', valX)\n",
    "np.save(f'{SAVE_FOLDER}/testAllX.npy', testAllX)\n",
    "np.save(f'{SAVE_FOLDER}/testAllY.npy', testAllY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate training process\n",
    "\n",
    "Now that the training process is complete, lets evaluate the average loss of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 936x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set_style(\"darkgrid\")\n",
    "plt.figure(figsize=(13, 6))\n",
    "\n",
    "# summarize history for loss:\n",
    "plt.plot(history.history['loss'], color='blue', label='train')\n",
    "plt.plot(history.history['val_loss'], color='blue', alpha=0.4, label='validation')\n",
    "plt.xlabel('Epoch', fontsize=16)\n",
    "plt.ylabel('Cost', fontsize=16)\n",
    "plt.tick_params(axis='both', which='major', labelsize=14)\n",
    "plt.title('Average Loss During Training', fontsize=18)\n",
    "#plt.xticks(range(0,epochs))\n",
    "plt.legend(loc='upper right', fontsize=16)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
