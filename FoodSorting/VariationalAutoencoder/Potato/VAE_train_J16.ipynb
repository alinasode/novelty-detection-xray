{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Autoencoder: Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import axis, colorbar, imshow, show, figure, subplot\n",
    "from matplotlib import cm\n",
    "import matplotlib as mpl\n",
    "mpl.rc('axes', labelsize=18)\n",
    "mpl.rc('xtick', labelsize=16)\n",
    "mpl.rc('ytick', labelsize=16)\n",
    "%matplotlib inline\n",
    "\n",
    "import glob\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "import cv2\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "## ----- GPU ------------------------------------\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'    # use of GPU 0 (ERDA) (use before importing torch or tensorflow/keeas)\n",
    "## ----------------------------------------------\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.layers import Conv2D, Conv2DTranspose, Input, Flatten, Dense, Lambda, Reshape\n",
    "from keras.layers import BatchNormalization, ReLU, LeakyReLU\n",
    "from keras.models import Model\n",
    "from keras.losses import binary_crossentropy, mse\n",
    "from keras.activations import relu\n",
    "from keras.callbacks import Callback, ModelCheckpoint\n",
    "from keras import backend as K                         #contains calls for tensor manipulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n",
      "2.4.3\n"
     ]
    }
   ],
   "source": [
    "print (tf.__version__)\n",
    "print (keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NORMAL_TRAIN_AUG:       /home/jovyan/work/Speciale/FoodSorting/generated_dataset/Potato//DataAugmentation/NormalTrainAugmentations\n",
      "NORMAL_VALIDATION_AUG:  /home/jovyan/work/Speciale/FoodSorting/generated_dataset/Potato//DataAugmentation/NormalValidationAugmentations\n",
      "NORMAL_TEST_AUG:        /home/jovyan/work/Speciale/FoodSorting/generated_dataset/Potato//DataAugmentation/NormalTestAugmentations\n",
      "ANOMALY1_AUG:           /home/jovyan/work/Speciale/FoodSorting/generated_dataset/Potato//DataAugmentation/Anomaly1Augmentations\n",
      "ANOMALY2_AUG:           /home/jovyan/work/Speciale/FoodSorting/generated_dataset/Potato//DataAugmentation/Anomaly2Augmentations\n"
     ]
    }
   ],
   "source": [
    "from utils_vae_potato_paths import *\n",
    "\n",
    "# Get work directions for augmentated data set:\n",
    "print (\"NORMAL_TRAIN_AUG:      \", NORMAL_TRAIN_AUG)\n",
    "print (\"NORMAL_VALIDATION_AUG: \", NORMAL_VAL_AUG)\n",
    "print (\"NORMAL_TEST_AUG:       \", NORMAL_TEST_AUG)\n",
    "print (\"ANOMALY1_AUG:          \", ANOMALY1_AUG)\n",
    "print (\"ANOMALY2_AUG:          \", ANOMALY2_AUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Which Folder The Models Are Saved In: saved_models/latent16\n"
     ]
    }
   ],
   "source": [
    "# What latent dimension and filter size are we using?:\n",
    "latent_dim = 16\n",
    "filters    = 32\n",
    "\n",
    "# Get work direction for saving files:\n",
    "SAVE_FOLDER = f'saved_models/latent{latent_dim}'\n",
    "print (\"Which Folder The Models Are Saved In:\", SAVE_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] deleting existing files in folder...\n",
      "         done in 0.003 minutes\n"
     ]
    }
   ],
   "source": [
    "# Delete all existing files in folder where models are saved:\n",
    "print(\"[INFO] deleting existing files in folder...\")\n",
    "t0 = time()\n",
    "\n",
    "files = glob.glob(f'{SAVE_FOLDER}/*')\n",
    "for f in files:\n",
    "    os.remove(f)\n",
    "\n",
    "print (\"         done in %0.3f minutes\" % ((time() - t0)/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_VAE_AE import get_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Investigation of Ground Truths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of \"normal\" training images is:     1639\n",
      "Number of \"normal\" validation images is:   80\n",
      "Number of \"normal\" test images is:         320\n",
      "Total number of \"anomaly\" images is:       680\n"
     ]
    }
   ],
   "source": [
    "# Glob the directories and get the lists of good and bad images\n",
    "good_train_fns = [f for f in os.listdir(NORMAL_TRAIN_AUG) if not f.startswith('.')]\n",
    "good_val_fns = [f for f in os.listdir(NORMAL_VAL_AUG) if not f.startswith('.')]\n",
    "good_test_fns = [f for f in os.listdir(NORMAL_TEST_AUG) if not f.startswith('.')]\n",
    "bad1_fns = [f for f in os.listdir(ANOMALY1_AUG) if not f.startswith('.')]\n",
    "bad2_fns = [f for f in os.listdir(ANOMALY2_AUG) if not f.startswith('.')]\n",
    "\n",
    "print('Number of \"normal\" training images is:     {}'.format(len(good_train_fns)))\n",
    "print('Number of \"normal\" validation images is:   {}'.format(len(good_val_fns)))\n",
    "print('Number of \"normal\" test images is:         {}'.format(len(good_test_fns)))\n",
    "print('Total number of \"anomaly\" images is:       {}'.format(len(bad1_fns) + len(bad2_fns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Procentage of normal samples (ground truth):   74.99 %\n",
      "> Procentage of anomaly samples (ground truth):  25.01 %\n"
     ]
    }
   ],
   "source": [
    "# compute procentage of bad images vs good images:\n",
    "normal_fns = len(good_train_fns) + len(good_val_fns) + len(good_test_fns)\n",
    "anomaly_fns = len(bad1_fns + bad2_fns)\n",
    "print(f\"> Procentage of normal samples (ground truth):   {(normal_fns / (normal_fns + anomaly_fns)) * 100:.2f} %\")\n",
    "print(f\"> Procentage of anomaly samples (ground truth):  {(anomaly_fns / (normal_fns + anomaly_fns)) * 100:.2f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Normal Data\n",
    "\n",
    "Load normal samples in pre-splitted data sets: train, valdiation and test.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading normal training images...\n",
      "          done\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] loading normal training images...\")\n",
    "trainX = [cv2.imread(file, 0) for file in glob.glob(f\"{NORMAL_TRAIN_AUG}/*.png\")]  # read as grayscale\n",
    "trainX = np.array(trainX)\n",
    "print(\"          done\")\n",
    "\n",
    "# create a corresponding list of labels:\n",
    "trainY = get_labels(trainX, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading normal validation images...\n",
      "          done\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] loading normal validation images...\")\n",
    "x_normal_val = [cv2.imread(file, 0) for file in glob.glob(f\"{NORMAL_VAL_AUG}/*.png\")]  # read as grayscale\n",
    "x_normal_val = np.array(x_normal_val)\n",
    "print(\"          done\")\n",
    "\n",
    "# create a corresponding list of labels:\n",
    "y_normal_val = get_labels(x_normal_val, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading normal testing images...\n",
      "          done\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] loading normal testing images...\")\n",
    "x_normal_test = [cv2.imread(file, 0) for file in glob.glob(f\"{NORMAL_TEST_AUG}/*.png\")]  # read as grayscale\n",
    "x_normal_test = np.array(x_normal_test)\n",
    "print(\"          done\")\n",
    "\n",
    "# create a corresponding list of labels:\n",
    "y_normal_test = get_labels(x_normal_test, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Anomaly Class 1 Data (i.e. 'metal' sampels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading anomaly class 1 ('metal') images...\n",
      "          done\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] loading anomaly class 1 ('metal') images...\")\n",
    "x_metal = [cv2.imread(file, 0) for file in glob.glob(f\"{ANOMALY1_AUG}/*.png\")]  # read as grayscale\n",
    "x_metal = np.array(x_metal)\n",
    "print(\"          done\")\n",
    "\n",
    "# create a corresponding list of labels:\n",
    "y_metal = get_labels(x_metal, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Anomaly Class 2 Data (i.e. 'hollow' sampels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading anomaly class 2 ('hollow') images...\n",
      "          done\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] loading anomaly class 2 ('hollow') images...\")\n",
    "x_hollow = [cv2.imread(file, 0) for file in glob.glob(f\"{ANOMALY2_AUG}/*.png\")]  # read as grayscale\n",
    "x_hollow = np.array(x_hollow)\n",
    "print(\"          done\")\n",
    "\n",
    "# create a corresponding list of labels:\n",
    "y_hollow = get_labels(x_hollow, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine normal samples and anomaly samples and split them into training, validation and test sets\n",
    "\n",
    "\n",
    "`label 0` == Normal Samples (=> Perfect' Samples)\n",
    "\n",
    "`label 1` == Anomaly 1 Samples (=> 'Metal' Samples)\n",
    "\n",
    "`label 2` == Anomaly 2 Samples (=> 'Hollow' Samples)\n",
    "\n",
    "Train and Validation sets only consists of `Normal Data`, aka. `label 0`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine pre-loaded validation and testing set into a new testing set:\n",
    "testvalX = np.vstack((x_normal_val, x_normal_test))\n",
    "testvalY = get_labels(testvalX, 0)\n",
    "\n",
    "# randomly split in order to make new validation set:\n",
    "NoUsageX, valX, NoUsageY, valY = train_test_split(testvalX, testvalY, test_size=0.25, random_state=4345672)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Testing set \"\"\"\n",
    "# combine pre-loaded validation and testing set into a new testing set:\n",
    "testNormalX = np.vstack((x_normal_val, x_normal_test))\n",
    "testNormalY = get_labels(testNormalX, 0)\n",
    "\n",
    "# Anomaly Class 1 (i.e. 'metal' samples):\n",
    "testAnomaly1X, testAnomaly1Y = x_metal, y_metal\n",
    "\n",
    "# Anomaly Class 2 (i.e. 'hollow' samples):\n",
    "testAnomaly2X, testAnomaly2Y = x_hollow, y_hollow\n",
    "\n",
    "# Combine all testing data in one array (the test set is NOT used during any part but inference):\n",
    "testAllX = np.vstack((testNormalX, testAnomaly1X, testAnomaly2X))\n",
    "testAllY = np.hstack((testNormalY, testAnomaly1Y, testAnomaly2Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Data Dimensions and Distributions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect data shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:\n",
      " > images: (1639, 128, 128)\n",
      " > labels: (1639,)\n",
      "\n",
      "Validation set:\n",
      " > images: (100, 128, 128)\n",
      " > labels: (100,)\n",
      "\n",
      "Test set:\n",
      " > images: (1080, 128, 128)\n",
      " > labels: (1080,)\n",
      "\t'Normal' test set:\n",
      "\t  > images: (400, 128, 128)\n",
      "\t  > labels: (400,)\n",
      "\t'Anomaly 1' test set:\n",
      "\t  > images: (392, 128, 128)\n",
      "\t  > labels: (392,)\n",
      "\t'Anomaly 2' test set:\n",
      "\t  > images: (288, 128, 128)\n",
      "\t  > labels: (288,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set:\")\n",
    "print(\" > images:\", trainX.shape)\n",
    "print(\" > labels:\", trainY.shape)\n",
    "print (\"\")\n",
    "\n",
    "print(\"Validation set:\")\n",
    "print(\" > images:\", valX.shape)\n",
    "print(\" > labels:\", valY.shape)\n",
    "print (\"\")\n",
    "\n",
    "print(\"Test set:\")\n",
    "print(\" > images:\", testAllX.shape)\n",
    "print(\" > labels:\", testAllY.shape)\n",
    "print(\"\\t'Normal' test set:\")\n",
    "print(\"\\t  > images:\", testNormalX.shape)\n",
    "print(\"\\t  > labels:\", testNormalY.shape)\n",
    "print(\"\\t'Anomaly 1' test set:\")\n",
    "print(\"\\t  > images:\", testAnomaly1X.shape)\n",
    "print(\"\\t  > labels:\", testAnomaly1Y.shape)\n",
    "print(\"\\t'Anomaly 2' test set:\")\n",
    "print(\"\\t  > images:\", testAnomaly2X.shape)\n",
    "print(\"\\t  > labels:\", testAnomaly2Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procentage of training samples: 76.62 %\n",
      "Procentage of validation samples: 4.68 %\n",
      "Procentage of (only normal) testing samples: 18.70 %\n"
     ]
    }
   ],
   "source": [
    "# compute procentage between training, validation and test data sets (only normal data):\n",
    "print(f\"Procentage of training samples: {(len(trainX) / (len(trainX) + len(valX) + len(testNormalX))) * 100:.2f} %\")\n",
    "print(f\"Procentage of validation samples: {(len(valX) / (len(trainX) + len(valX) + len(testNormalX))) * 100:.2f} %\")\n",
    "print(f\"Procentage of (only normal) testing samples: {(len(testNormalX) / (len(trainX) + len(valX) + len(testNormalX))) * 100:.2f} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procentage of training samples: 58.14 %\n",
      "Procentage of validation samples: 3.55 %\n",
      "Procentage of (total) testing samples: 38.31 %\n"
     ]
    }
   ],
   "source": [
    "# compute procentage between training, validation and test data sets:\n",
    "print(f\"Procentage of training samples: {(len(trainX) / (len(trainX) + len(valX) + len(testAllX))) * 100:.2f} %\")\n",
    "print(f\"Procentage of validation samples: {(len(valX) / (len(trainX) + len(valX) + len(testAllX))) * 100:.2f} %\")\n",
    "print(f\"Procentage of (total) testing samples: {(len(testAllX) / (len(trainX) + len(valX) + len(testAllX))) * 100:.2f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for (un)balanced data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels = ['0: normal', '1: metal', '2: hollow']\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(16,4))\n",
    "ax[0].hist(trainY, align=\"left\"); ax[0].set_title('train', fontsize=18); ax[0].set_xticks([0,1,2]); ax[0].set_xticklabels(labels); ax[0].tick_params(axis='both', which='major', labelsize=16); ax[0].set_xlim(-0.5, 2.5);\n",
    "ax[1].hist(valY, align=\"left\"); ax[1].set_title('validation', fontsize=18); ax[1].set_xticks([0,1,2]); ax[1].set_xticklabels(labels); ax[1].tick_params(axis='both', which='major', labelsize=16); ax[1].set_xlim(-0.5, 2.5);\n",
    "ax[2].hist(testAllY, align=\"left\"); ax[2].set_title('test', fontsize=18); ax[2].set_xticks([0,1,2]); ax[2].set_xticklabels(labels); ax[2].tick_params(axis='both', which='major', labelsize=16); ax[2].set_xlim(-0.5, 2.5);\n",
    "\n",
    "ax[0].set_ylabel(\"Number of images\", fontsize=18)\n",
    "ax[1].set_xlabel(\"Class Labels\", fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procentage of normal test samples: 37.04 %\n",
      "Procentage of total anomaly test samples: 62.96 %\n"
     ]
    }
   ],
   "source": [
    "# compute procentage of normal test images vs total anomaly test images:\n",
    "print(f\"Procentage of normal test samples: {(len(testNormalX) / (len(testNormalX) + (len(testAnomaly1X) + len(testAnomaly2X)))) * 100:.2f} %\")\n",
    "print(f\"Procentage of total anomaly test samples: {((len(testAnomaly1X) + len(testAnomaly2X)) / (len(testNormalX) + (len(testAnomaly1X) + len(testAnomaly2X)))) * 100:.2f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling\n",
    "\n",
    "Only normalization has been used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data configuration\n",
    "img_width, img_height = trainX.shape[1], trainX.shape[2]      # input image dimensions\n",
    "num_channels = 1                                              # gray-channel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization\n",
    "\n",
    "Neural Networks learns faster with normalized data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize to be in  the [0, 1] range:\n",
    "trainX = trainX.astype('float32') / 255.\n",
    "valX = valX.astype('float32') / 255.\n",
    "testAllX = testAllX.astype('float32') / 255.\n",
    "\n",
    "# reshape data to keras inputs --> (n_samples, img_rows, img_cols, n_channels):\n",
    "trainX = trainX.reshape(trainX.shape[0], img_height, img_width, num_channels)\n",
    "valX = valX.reshape(valX.shape[0], img_height, img_width, num_channels)\n",
    "testAllX = testAllX.reshape(testAllX.shape[0], img_height, img_width, num_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture and Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import architecture of VAE model and its hyperparameters:\n",
    "from J16_VAE_model import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Encoder Part*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_input (InputLayer)      [(None, 128, 128, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 64, 64, 32)   320         encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)         (None, 64, 64, 32)   0           conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 32, 32, 32)   9248        leaky_re_lu[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)       (None, 32, 32, 32)   0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 16, 16, 32)   9248        leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)       (None, 16, 16, 32)   0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 8, 8, 32)     9248        leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)       (None, 8, 8, 32)     0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 4, 4, 32)     9248        leaky_re_lu_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)       (None, 4, 4, 32)     0           conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 512)          0           leaky_re_lu_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "latent (Dense)                  (None, 200)          102600      flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)       (None, 200)          0           latent[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "z_mean (Dense)                  (None, 16)           3216        leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "z_log_mean (Dense)              (None, 16)           3216        leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "z (Lambda)                      (None, 16)           0           z_mean[0][0]                     \n",
      "                                                                 z_log_mean[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 146,344\n",
      "Trainable params: 146,344\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Decoder Part*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "decoder_input (InputLayer)   [(None, 16)]              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               8704      \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 4, 4, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose (Conv2DTran (None, 8, 8, 32)          9248      \n",
      "_________________________________________________________________\n",
      "re_lu (ReLU)                 (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 16, 16, 32)        9248      \n",
      "_________________________________________________________________\n",
      "re_lu_1 (ReLU)               (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTr (None, 32, 32, 32)        9248      \n",
      "_________________________________________________________________\n",
      "re_lu_2 (ReLU)               (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTr (None, 64, 64, 32)        9248      \n",
      "_________________________________________________________________\n",
      "re_lu_3 (ReLU)               (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_4 (Conv2DTr (None, 128, 128, 32)      9248      \n",
      "_________________________________________________________________\n",
      "re_lu_4 (ReLU)               (None, 128, 128, 32)      0         \n",
      "_________________________________________________________________\n",
      "decoder_output (Conv2DTransp (None, 128, 128, 1)       289       \n",
      "=================================================================\n",
      "Total params: 55,233\n",
      "Trainable params: 55,233\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Total VAE Model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vae\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder_input (InputLayer)   [(None, 128, 128, 1)]     0         \n",
      "_________________________________________________________________\n",
      "encoder (Functional)         [(None, 16), (None, 16),  146344    \n",
      "_________________________________________________________________\n",
      "decoder (Functional)         (None, 128, 128, 1)       55233     \n",
      "=================================================================\n",
      "Total params: 201,577\n",
      "Trainable params: 201,577\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vae.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile and train model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Hyperparameters \"\"\"\n",
    "batch_size  = 256\n",
    "epochs      = 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Checkpoint callback options: save model at 1'st epoch (inital model)\n",
    "\n",
    "https://stackoverflow.com/questions/54323960/save-keras-model-at-specific-epochs\n",
    "\"\"\"\n",
    "\n",
    "class CustomSaver(keras.callbacks.Callback):\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        if epoch == 1:\n",
    "            self.model.save_weights(f\"{SAVE_FOLDER}/cp-0001.h5\")\n",
    "            \n",
    "# create and use callback:\n",
    "initial_checkpoint = CustomSaver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Checkpoint callback options: save model at every k'te epochs\n",
    "\"\"\"\n",
    "# Include the epoch in the file name (uses `str.format`):\n",
    "checkpoint_path = \"saved_models/latent16/cp-{epoch:04d}.h5\"\n",
    "\n",
    "# Create a callback that saves the model's weights every k'te epochs:\n",
    "checkpoint = ModelCheckpoint(filepath = checkpoint_path,\n",
    "                             monitor='loss',           # name of the metrics to monitor\n",
    "                             verbose=1, \n",
    "                             #save_best_only=False,     # if False, the best model will not be overridden.\n",
    "                             save_weights_only=True,   # if True, only the weights of the models will be saved.\n",
    "                                                        # if False, the whole models will be saved.\n",
    "                             #mode='auto', \n",
    "                             period=200                  # save after every k'te epochs\n",
    "                            )\n",
    "\n",
    "# Save the weights using the `checkpoint_path` format\n",
    "vae.save_weights(checkpoint_path.format(epoch=0))          # save for zero epoch (inital)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Checkpoint callback options: Early stopping\n",
    "https://www.tensorflow.org/guide/keras/custom_callback\n",
    "\"\"\"\n",
    "\n",
    "class EarlyStoppingAtMinLoss(keras.callbacks.Callback):\n",
    "    \"\"\"Stop training when the loss is at its min, i.e. the loss stops decreasing.\n",
    "\n",
    "  Arguments:\n",
    "      patience: Number of epochs to wait after min has been hit. After this\n",
    "      number of no improvement, training stops.\n",
    "  \"\"\"\n",
    "\n",
    "    def __init__(self, patience=100):\n",
    "        super(EarlyStoppingAtMinLoss, self).__init__()\n",
    "        self.patience = patience\n",
    "        # best_weights to store the weights at which the minimum loss occurs.\n",
    "        self.best_weights = None\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        # The number of epoch it has waited when loss is no longer minimum.\n",
    "        self.wait = 0\n",
    "        # The epoch the training stops at.\n",
    "        self.stopped_epoch = 0\n",
    "        # Initialize the best as infinity.\n",
    "        self.best = np.Inf\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        current = logs.get(\"loss\")\n",
    "        if np.less(current, self.best):\n",
    "            self.best = current\n",
    "            self.wait = 0\n",
    "            # Record the best weights if current results is better (less).\n",
    "            self.best_weights = self.model.get_weights()\n",
    "        else:\n",
    "            self.wait += 1\n",
    "            if self.wait >= self.patience:\n",
    "                self.stopped_epoch = epoch\n",
    "                self.model.stop_training = True\n",
    "                print(\"Restoring model weights from the end of the best epoch.\")\n",
    "                self.model.set_weights(self.best_weights)\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        if self.stopped_epoch > 0:\n",
    "            print(\"Epoch %05d: early stopping\" % (self.stopped_epoch + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Create and Compile Model\"\"\"\n",
    "# import architecture of VAE model and its hyperparameters. learning rate choosen from graph above.\n",
    "vae = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      "7/7 [==============================] - 1s 195ms/step - loss: 2323.1711 - val_loss: 2256.1541\n",
      "Epoch 2/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 2093.8936 - val_loss: 1765.4158\n",
      "Epoch 3/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 1459.6526 - val_loss: 1227.4159\n",
      "Epoch 4/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 1036.5129 - val_loss: 836.0411\n",
      "Epoch 5/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 730.8901 - val_loss: 616.3928\n",
      "Epoch 6/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 535.5724 - val_loss: 465.8549\n",
      "Epoch 7/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 428.4212 - val_loss: 394.7655\n",
      "Epoch 8/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 377.9353 - val_loss: 365.8254\n",
      "Epoch 9/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 351.9335 - val_loss: 344.1267\n",
      "Epoch 10/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 336.9075 - val_loss: 351.7427\n",
      "Epoch 11/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 338.0624 - val_loss: 334.0963\n",
      "Epoch 12/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 323.1194 - val_loss: 319.3846\n",
      "Epoch 13/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 313.6789 - val_loss: 313.8218\n",
      "Epoch 14/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 306.5963 - val_loss: 305.1993\n",
      "Epoch 15/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 299.1638 - val_loss: 301.6671\n",
      "Epoch 16/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 291.6984 - val_loss: 291.6159\n",
      "Epoch 17/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 286.8269 - val_loss: 284.5482\n",
      "Epoch 18/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 280.7986 - val_loss: 283.8065\n",
      "Epoch 19/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 276.9712 - val_loss: 280.9530\n",
      "Epoch 20/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 273.5855 - val_loss: 277.2978\n",
      "Epoch 21/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 270.6211 - val_loss: 280.1126\n",
      "Epoch 22/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 266.0597 - val_loss: 266.7531\n",
      "Epoch 23/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 261.5457 - val_loss: 264.8272\n",
      "Epoch 24/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 258.4839 - val_loss: 260.1655\n",
      "Epoch 25/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 256.0113 - val_loss: 256.4629\n",
      "Epoch 26/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 251.4933 - val_loss: 251.3168\n",
      "Epoch 27/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 246.6530 - val_loss: 246.7908\n",
      "Epoch 28/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 242.3979 - val_loss: 241.2277\n",
      "Epoch 29/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 237.1514 - val_loss: 236.3073\n",
      "Epoch 30/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 231.5350 - val_loss: 236.9180\n",
      "Epoch 31/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 229.2881 - val_loss: 229.0825\n",
      "Epoch 32/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 227.7900 - val_loss: 225.1339\n",
      "Epoch 33/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 222.1197 - val_loss: 223.3839\n",
      "Epoch 34/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 219.2868 - val_loss: 220.4303\n",
      "Epoch 35/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 216.0176 - val_loss: 217.6710\n",
      "Epoch 36/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 214.4233 - val_loss: 215.6168\n",
      "Epoch 37/2000\n",
      "7/7 [==============================] - 1s 107ms/step - loss: 211.9237 - val_loss: 213.0325\n",
      "Epoch 38/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 209.6390 - val_loss: 211.0952\n",
      "Epoch 39/2000\n",
      "7/7 [==============================] - 1s 108ms/step - loss: 208.2723 - val_loss: 209.8821\n",
      "Epoch 40/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 204.1829 - val_loss: 204.9410\n",
      "Epoch 41/2000\n",
      "7/7 [==============================] - 1s 108ms/step - loss: 202.5181 - val_loss: 204.7314\n",
      "Epoch 42/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 201.6497 - val_loss: 205.5502\n",
      "Epoch 43/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 199.4940 - val_loss: 198.5010\n",
      "Epoch 44/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 196.6072 - val_loss: 198.1320\n",
      "Epoch 45/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 193.9815 - val_loss: 194.0532\n",
      "Epoch 46/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 191.4522 - val_loss: 191.3499\n",
      "Epoch 47/2000\n",
      "7/7 [==============================] - 1s 107ms/step - loss: 190.1930 - val_loss: 191.5600\n",
      "Epoch 48/2000\n",
      "7/7 [==============================] - 1s 107ms/step - loss: 188.7504 - val_loss: 189.5572\n",
      "Epoch 49/2000\n",
      "7/7 [==============================] - 1s 108ms/step - loss: 188.5465 - val_loss: 186.9257\n",
      "Epoch 50/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 185.1056 - val_loss: 185.1293\n",
      "Epoch 51/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 182.4902 - val_loss: 184.9392\n",
      "Epoch 52/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 180.9196 - val_loss: 181.3998\n",
      "Epoch 53/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 180.0375 - val_loss: 180.6260\n",
      "Epoch 54/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 177.5428 - val_loss: 179.7534\n",
      "Epoch 55/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 176.0430 - val_loss: 176.2139\n",
      "Epoch 56/2000\n",
      "7/7 [==============================] - 1s 107ms/step - loss: 173.2861 - val_loss: 174.9027\n",
      "Epoch 57/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 171.5678 - val_loss: 173.7734\n",
      "Epoch 58/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 169.7462 - val_loss: 171.0749\n",
      "Epoch 59/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 167.8867 - val_loss: 170.1382\n",
      "Epoch 60/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 166.6799 - val_loss: 168.8035\n",
      "Epoch 61/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 165.7509 - val_loss: 166.1245\n",
      "Epoch 62/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 164.1258 - val_loss: 163.5699\n",
      "Epoch 63/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 161.9623 - val_loss: 163.5347\n",
      "Epoch 64/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 160.4485 - val_loss: 163.3706\n",
      "Epoch 65/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 159.0557 - val_loss: 160.3442\n",
      "Epoch 66/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 158.8003 - val_loss: 158.3387\n",
      "Epoch 67/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 156.4626 - val_loss: 158.5380\n",
      "Epoch 68/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 155.0825 - val_loss: 156.4015\n",
      "Epoch 69/2000\n",
      "7/7 [==============================] - 1s 107ms/step - loss: 153.5853 - val_loss: 154.4863\n",
      "Epoch 70/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 152.7384 - val_loss: 153.5027\n",
      "Epoch 71/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 151.5357 - val_loss: 152.8512\n",
      "Epoch 72/2000\n",
      "7/7 [==============================] - 1s 107ms/step - loss: 150.4682 - val_loss: 153.5742\n",
      "Epoch 73/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 149.6087 - val_loss: 151.4142\n",
      "Epoch 74/2000\n",
      "7/7 [==============================] - 1s 107ms/step - loss: 148.2398 - val_loss: 147.8025\n",
      "Epoch 75/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 146.1973 - val_loss: 145.8987\n",
      "Epoch 76/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 144.5339 - val_loss: 145.7641\n",
      "Epoch 77/2000\n",
      "7/7 [==============================] - 1s 107ms/step - loss: 143.3235 - val_loss: 144.2312\n",
      "Epoch 78/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 141.7454 - val_loss: 142.4376\n",
      "Epoch 79/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 140.9169 - val_loss: 142.9721\n",
      "Epoch 80/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 141.0542 - val_loss: 142.7471\n",
      "Epoch 81/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 140.3180 - val_loss: 139.7744\n",
      "Epoch 82/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 138.0830 - val_loss: 138.9767\n",
      "Epoch 83/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 135.9339 - val_loss: 137.1561\n",
      "Epoch 84/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 135.9389 - val_loss: 135.4718\n",
      "Epoch 85/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 135.1122 - val_loss: 136.0044\n",
      "Epoch 86/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 133.4940 - val_loss: 135.0625\n",
      "Epoch 87/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 132.1291 - val_loss: 132.0297\n",
      "Epoch 88/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 130.4209 - val_loss: 131.3128\n",
      "Epoch 89/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 129.5429 - val_loss: 130.5219\n",
      "Epoch 90/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 128.0247 - val_loss: 129.2104\n",
      "Epoch 91/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 127.4499 - val_loss: 127.9823\n",
      "Epoch 92/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 126.2167 - val_loss: 127.2301\n",
      "Epoch 93/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 125.2543 - val_loss: 126.5849\n",
      "Epoch 94/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 124.0748 - val_loss: 127.2095\n",
      "Epoch 95/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 123.2872 - val_loss: 124.2446\n",
      "Epoch 96/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 122.8954 - val_loss: 124.3188\n",
      "Epoch 97/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 121.4756 - val_loss: 123.8573\n",
      "Epoch 98/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 120.9870 - val_loss: 120.9556\n",
      "Epoch 99/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 119.5272 - val_loss: 120.6822\n",
      "Epoch 100/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 118.5456 - val_loss: 118.7470\n",
      "Epoch 101/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 117.6862 - val_loss: 118.3766\n",
      "Epoch 102/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 116.4034 - val_loss: 118.1097\n",
      "Epoch 103/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 116.0411 - val_loss: 116.2539\n",
      "Epoch 104/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 114.9852 - val_loss: 115.6063\n",
      "Epoch 105/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 113.9107 - val_loss: 115.7388\n",
      "Epoch 106/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 113.2556 - val_loss: 113.6992\n",
      "Epoch 107/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 112.5540 - val_loss: 113.7678\n",
      "Epoch 108/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 111.1240 - val_loss: 112.1509\n",
      "Epoch 109/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 110.3555 - val_loss: 110.7238\n",
      "Epoch 110/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 109.6605 - val_loss: 111.3148\n",
      "Epoch 111/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 108.8355 - val_loss: 110.0751\n",
      "Epoch 112/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 108.0020 - val_loss: 108.9455\n",
      "Epoch 113/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 107.4641 - val_loss: 107.2963\n",
      "Epoch 114/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 106.5258 - val_loss: 106.6113\n",
      "Epoch 115/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 105.8821 - val_loss: 105.5699\n",
      "Epoch 116/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 104.7626 - val_loss: 105.4979\n",
      "Epoch 117/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 103.8296 - val_loss: 105.4223\n",
      "Epoch 118/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 103.0910 - val_loss: 104.6722\n",
      "Epoch 119/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 102.3352 - val_loss: 103.0575\n",
      "Epoch 120/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 101.3588 - val_loss: 101.7268\n",
      "Epoch 121/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 101.0531 - val_loss: 101.5692\n",
      "Epoch 122/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 99.9267 - val_loss: 102.1836\n",
      "Epoch 123/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 99.8077 - val_loss: 101.7619\n",
      "Epoch 124/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 99.4596 - val_loss: 100.6061\n",
      "Epoch 125/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 97.8535 - val_loss: 97.7845\n",
      "Epoch 126/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 97.6382 - val_loss: 97.5458\n",
      "Epoch 127/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 97.0106 - val_loss: 97.0465\n",
      "Epoch 128/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 95.9326 - val_loss: 96.1018\n",
      "Epoch 129/2000\n",
      "7/7 [==============================] - 1s 107ms/step - loss: 95.0384 - val_loss: 96.5731\n",
      "Epoch 130/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 94.4465 - val_loss: 95.6867\n",
      "Epoch 131/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 94.1271 - val_loss: 95.1786\n",
      "Epoch 132/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 93.3116 - val_loss: 94.5536\n",
      "Epoch 133/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 92.6616 - val_loss: 92.7581\n",
      "Epoch 134/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 91.6928 - val_loss: 92.5755\n",
      "Epoch 135/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 91.0311 - val_loss: 94.9184\n",
      "Epoch 136/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 91.0463 - val_loss: 91.9892\n",
      "Epoch 137/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 90.1350 - val_loss: 92.2650\n",
      "Epoch 138/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 89.7561 - val_loss: 90.7104\n",
      "Epoch 139/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 89.4529 - val_loss: 89.7765\n",
      "Epoch 140/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 88.3812 - val_loss: 88.8952\n",
      "Epoch 141/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 87.9273 - val_loss: 88.0401\n",
      "Epoch 142/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 87.3837 - val_loss: 88.8241\n",
      "Epoch 143/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 86.6577 - val_loss: 86.5785\n",
      "Epoch 144/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 86.1090 - val_loss: 89.1149\n",
      "Epoch 145/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 86.1516 - val_loss: 88.2675\n",
      "Epoch 146/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 86.4688 - val_loss: 86.2604\n",
      "Epoch 147/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 84.9002 - val_loss: 84.4755\n",
      "Epoch 148/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 83.7755 - val_loss: 85.5151\n",
      "Epoch 149/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 84.1912 - val_loss: 86.7844\n",
      "Epoch 150/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 82.9720 - val_loss: 83.1000\n",
      "Epoch 151/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 81.9274 - val_loss: 83.5879\n",
      "Epoch 152/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 81.8269 - val_loss: 82.0748\n",
      "Epoch 153/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 80.9121 - val_loss: 82.0013\n",
      "Epoch 154/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 80.9651 - val_loss: 81.0384\n",
      "Epoch 155/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 79.8149 - val_loss: 80.8509\n",
      "Epoch 156/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 79.7119 - val_loss: 80.7993\n",
      "Epoch 157/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 79.1773 - val_loss: 79.9043\n",
      "Epoch 158/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 78.6067 - val_loss: 79.4102\n",
      "Epoch 159/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 78.7000 - val_loss: 78.8729\n",
      "Epoch 160/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 78.3703 - val_loss: 79.0517\n",
      "Epoch 161/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 78.1011 - val_loss: 80.4420\n",
      "Epoch 162/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 77.0192 - val_loss: 77.8319\n",
      "Epoch 163/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 76.3245 - val_loss: 77.0532\n",
      "Epoch 164/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 75.8784 - val_loss: 77.7843\n",
      "Epoch 165/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 75.3088 - val_loss: 76.3478\n",
      "Epoch 166/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 75.3034 - val_loss: 75.6628\n",
      "Epoch 167/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 74.7424 - val_loss: 76.3176\n",
      "Epoch 168/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 74.6101 - val_loss: 75.3253\n",
      "Epoch 169/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 73.6490 - val_loss: 74.7537\n",
      "Epoch 170/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 73.6422 - val_loss: 74.1738\n",
      "Epoch 171/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 72.8322 - val_loss: 74.1314\n",
      "Epoch 172/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 72.4319 - val_loss: 74.5510\n",
      "Epoch 173/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 72.2532 - val_loss: 74.4285\n",
      "Epoch 174/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 72.4876 - val_loss: 72.8641\n",
      "Epoch 175/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 71.4973 - val_loss: 71.8063\n",
      "Epoch 176/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 71.2485 - val_loss: 71.3325\n",
      "Epoch 177/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 70.3767 - val_loss: 71.1610\n",
      "Epoch 178/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 69.7407 - val_loss: 71.4236\n",
      "Epoch 179/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 69.6425 - val_loss: 70.7217\n",
      "Epoch 180/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 69.2426 - val_loss: 70.5446\n",
      "Epoch 181/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 68.8864 - val_loss: 71.0846\n",
      "Epoch 182/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 68.7231 - val_loss: 70.0172\n",
      "Epoch 183/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 68.6199 - val_loss: 69.2340\n",
      "Epoch 184/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 68.2709 - val_loss: 68.4276\n",
      "Epoch 185/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 67.2406 - val_loss: 69.3907\n",
      "Epoch 186/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 67.3893 - val_loss: 72.0151\n",
      "Epoch 187/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 67.6112 - val_loss: 68.9271\n",
      "Epoch 188/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 66.7874 - val_loss: 67.7876\n",
      "Epoch 189/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 66.4161 - val_loss: 68.9214\n",
      "Epoch 190/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 66.1301 - val_loss: 68.3512\n",
      "Epoch 191/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 66.2448 - val_loss: 68.2163\n",
      "Epoch 192/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 65.7289 - val_loss: 66.8142\n",
      "Epoch 193/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 65.3247 - val_loss: 65.4075\n",
      "Epoch 194/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 64.9749 - val_loss: 65.0983\n",
      "Epoch 195/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 64.3646 - val_loss: 65.6383\n",
      "Epoch 196/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 64.0349 - val_loss: 64.9783\n",
      "Epoch 197/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 63.3085 - val_loss: 65.0882\n",
      "Epoch 198/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 62.9600 - val_loss: 64.2167\n",
      "Epoch 199/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 63.0343 - val_loss: 63.8023\n",
      "Epoch 200/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 62.4528\n",
      "Epoch 00200: saving model to saved_models/latent16/cp-0200.h5\n",
      "7/7 [==============================] - 1s 144ms/step - loss: 62.4528 - val_loss: 63.9719\n",
      "Epoch 201/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 62.5085 - val_loss: 63.7681\n",
      "Epoch 202/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 62.0910 - val_loss: 63.1763\n",
      "Epoch 203/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 62.5577 - val_loss: 64.3128\n",
      "Epoch 204/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 61.7303 - val_loss: 62.8008\n",
      "Epoch 205/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 61.3206 - val_loss: 63.4141\n",
      "Epoch 206/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 61.4071 - val_loss: 61.4796\n",
      "Epoch 207/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 60.6395 - val_loss: 61.5522\n",
      "Epoch 208/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 60.1993 - val_loss: 61.5571\n",
      "Epoch 209/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 60.1488 - val_loss: 61.4368\n",
      "Epoch 210/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 59.6489 - val_loss: 60.7824\n",
      "Epoch 211/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 59.4931 - val_loss: 60.7409\n",
      "Epoch 212/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 59.6171 - val_loss: 60.5861\n",
      "Epoch 213/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 58.9141 - val_loss: 59.7754\n",
      "Epoch 214/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 59.0421 - val_loss: 61.3621\n",
      "Epoch 215/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 59.0673 - val_loss: 59.6214\n",
      "Epoch 216/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 58.0424 - val_loss: 59.9116\n",
      "Epoch 217/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 57.7377 - val_loss: 58.7225\n",
      "Epoch 218/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 57.2381 - val_loss: 58.4933\n",
      "Epoch 219/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 57.5546 - val_loss: 57.8783\n",
      "Epoch 220/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 57.2636 - val_loss: 58.3276\n",
      "Epoch 221/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 57.0597 - val_loss: 56.7324\n",
      "Epoch 222/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 56.6472 - val_loss: 58.1721\n",
      "Epoch 223/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 56.6418 - val_loss: 58.4224\n",
      "Epoch 224/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 55.8873 - val_loss: 56.5391\n",
      "Epoch 225/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 55.6885 - val_loss: 58.0042\n",
      "Epoch 226/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 55.6634 - val_loss: 57.0913\n",
      "Epoch 227/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 55.4007 - val_loss: 56.5684\n",
      "Epoch 228/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 55.1522 - val_loss: 56.9588\n",
      "Epoch 229/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 54.6928 - val_loss: 56.3681\n",
      "Epoch 230/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 54.2743 - val_loss: 55.6509\n",
      "Epoch 231/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 54.3099 - val_loss: 55.8777\n",
      "Epoch 232/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 54.1559 - val_loss: 55.2162\n",
      "Epoch 233/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 53.8698 - val_loss: 55.0866\n",
      "Epoch 234/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 53.7321 - val_loss: 54.5095\n",
      "Epoch 235/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 54.3713 - val_loss: 55.1521\n",
      "Epoch 236/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 53.1942 - val_loss: 53.9011\n",
      "Epoch 237/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 53.0209 - val_loss: 54.1071\n",
      "Epoch 238/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 52.8523 - val_loss: 54.2896\n",
      "Epoch 239/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 52.5233 - val_loss: 53.9094\n",
      "Epoch 240/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 52.3717 - val_loss: 54.6006\n",
      "Epoch 241/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 52.4082 - val_loss: 53.3678\n",
      "Epoch 242/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 52.1506 - val_loss: 53.7054\n",
      "Epoch 243/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 52.0782 - val_loss: 53.1008\n",
      "Epoch 244/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 52.8711 - val_loss: 53.3758\n",
      "Epoch 245/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 53.1471 - val_loss: 53.8156\n",
      "Epoch 246/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 51.7820 - val_loss: 52.3074\n",
      "Epoch 247/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 51.1070 - val_loss: 52.2663\n",
      "Epoch 248/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 50.6719 - val_loss: 52.4508\n",
      "Epoch 249/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 50.3935 - val_loss: 52.2437\n",
      "Epoch 250/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 50.5916 - val_loss: 54.2534\n",
      "Epoch 251/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 50.4804 - val_loss: 51.2453\n",
      "Epoch 252/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 50.4544 - val_loss: 51.6820\n",
      "Epoch 253/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 49.8401 - val_loss: 51.5504\n",
      "Epoch 254/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 49.7766 - val_loss: 50.8552\n",
      "Epoch 255/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 49.4704 - val_loss: 50.2347\n",
      "Epoch 256/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 49.2108 - val_loss: 50.6550\n",
      "Epoch 257/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 49.5425 - val_loss: 51.0634\n",
      "Epoch 258/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 49.0355 - val_loss: 50.3876\n",
      "Epoch 259/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 48.7355 - val_loss: 49.7094\n",
      "Epoch 260/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 48.3749 - val_loss: 50.2807\n",
      "Epoch 261/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 48.5390 - val_loss: 49.9423\n",
      "Epoch 262/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 48.1781 - val_loss: 49.9596\n",
      "Epoch 263/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 47.9308 - val_loss: 49.4660\n",
      "Epoch 264/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 48.0100 - val_loss: 49.7593\n",
      "Epoch 265/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 47.6711 - val_loss: 48.7776\n",
      "Epoch 266/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 47.4419 - val_loss: 49.1708\n",
      "Epoch 267/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 47.6256 - val_loss: 48.8868\n",
      "Epoch 268/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 47.2609 - val_loss: 49.4254\n",
      "Epoch 269/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 47.5777 - val_loss: 50.0313\n",
      "Epoch 270/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 47.4762 - val_loss: 49.4559\n",
      "Epoch 271/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 47.3201 - val_loss: 48.1165\n",
      "Epoch 272/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 46.7713 - val_loss: 49.3788\n",
      "Epoch 273/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 46.9361 - val_loss: 50.0392\n",
      "Epoch 274/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 46.7531 - val_loss: 48.1629\n",
      "Epoch 275/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 47.2347 - val_loss: 48.7143\n",
      "Epoch 276/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 47.0421 - val_loss: 46.9687\n",
      "Epoch 277/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 46.0823 - val_loss: 47.8339\n",
      "Epoch 278/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 45.5939 - val_loss: 47.0977\n",
      "Epoch 279/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.7701 - val_loss: 46.9319\n",
      "Epoch 280/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 45.5639 - val_loss: 47.2989\n",
      "Epoch 281/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 45.2270 - val_loss: 47.0901\n",
      "Epoch 282/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.6164 - val_loss: 46.9446\n",
      "Epoch 283/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 45.0946 - val_loss: 47.7172\n",
      "Epoch 284/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 45.4143 - val_loss: 47.0016\n",
      "Epoch 285/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 45.0340 - val_loss: 46.6567\n",
      "Epoch 286/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 44.8933 - val_loss: 46.1590\n",
      "Epoch 287/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 44.6749 - val_loss: 47.0755\n",
      "Epoch 288/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 44.7971 - val_loss: 46.4000\n",
      "Epoch 289/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 44.4816 - val_loss: 45.7292\n",
      "Epoch 290/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 44.2186 - val_loss: 45.4710\n",
      "Epoch 291/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 44.4879 - val_loss: 45.5318\n",
      "Epoch 292/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 43.8587 - val_loss: 45.4739\n",
      "Epoch 293/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 43.6850 - val_loss: 45.2339\n",
      "Epoch 294/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 43.7019 - val_loss: 45.6191\n",
      "Epoch 295/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 43.6738 - val_loss: 44.8787\n",
      "Epoch 296/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 43.2656 - val_loss: 44.9184\n",
      "Epoch 297/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.2667 - val_loss: 45.6982\n",
      "Epoch 298/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.6206 - val_loss: 44.8175\n",
      "Epoch 299/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 43.2358 - val_loss: 46.6590\n",
      "Epoch 300/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.5270 - val_loss: 43.7949\n",
      "Epoch 301/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 43.9549 - val_loss: 45.0078\n",
      "Epoch 302/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 43.8978 - val_loss: 45.6377\n",
      "Epoch 303/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 43.6506 - val_loss: 44.6901\n",
      "Epoch 304/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 42.8624 - val_loss: 43.8925\n",
      "Epoch 305/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 42.4932 - val_loss: 44.1925\n",
      "Epoch 306/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 42.4252 - val_loss: 43.9709\n",
      "Epoch 307/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 42.3666 - val_loss: 43.9417\n",
      "Epoch 308/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 41.9076 - val_loss: 44.1845\n",
      "Epoch 309/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 41.8024 - val_loss: 42.9643\n",
      "Epoch 310/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 42.4614 - val_loss: 44.7878\n",
      "Epoch 311/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 41.9417 - val_loss: 43.1994\n",
      "Epoch 312/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 41.7809 - val_loss: 43.5655\n",
      "Epoch 313/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 41.6982 - val_loss: 43.3687\n",
      "Epoch 314/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 41.4363 - val_loss: 42.9205\n",
      "Epoch 315/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 41.4013 - val_loss: 42.4402\n",
      "Epoch 316/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 41.5939 - val_loss: 42.7534\n",
      "Epoch 317/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 41.3074 - val_loss: 42.6868\n",
      "Epoch 318/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 40.9617 - val_loss: 42.1274\n",
      "Epoch 319/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 40.4936 - val_loss: 42.3641\n",
      "Epoch 320/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 40.5973 - val_loss: 42.5976\n",
      "Epoch 321/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 40.4561 - val_loss: 41.5046\n",
      "Epoch 322/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 40.5566 - val_loss: 42.6904\n",
      "Epoch 323/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 40.4387 - val_loss: 41.0420\n",
      "Epoch 324/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 40.1653 - val_loss: 41.8434\n",
      "Epoch 325/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 40.1965 - val_loss: 42.4490\n",
      "Epoch 326/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 39.9864 - val_loss: 41.1486\n",
      "Epoch 327/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 39.7271 - val_loss: 40.6384\n",
      "Epoch 328/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 39.7050 - val_loss: 40.9810\n",
      "Epoch 329/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 39.3242 - val_loss: 40.9035\n",
      "Epoch 330/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 39.5158 - val_loss: 40.5689\n",
      "Epoch 331/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 39.1178 - val_loss: 40.5729\n",
      "Epoch 332/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 39.2330 - val_loss: 40.7486\n",
      "Epoch 333/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 39.2591 - val_loss: 41.0117\n",
      "Epoch 334/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 39.0156 - val_loss: 41.0901\n",
      "Epoch 335/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 39.1839 - val_loss: 39.9426\n",
      "Epoch 336/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 38.8405 - val_loss: 40.5313\n",
      "Epoch 337/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 38.8583 - val_loss: 40.2648\n",
      "Epoch 338/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 38.7601 - val_loss: 40.2289\n",
      "Epoch 339/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 38.8394 - val_loss: 40.1327\n",
      "Epoch 340/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 38.6081 - val_loss: 39.6599\n",
      "Epoch 341/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 38.6115 - val_loss: 40.5977\n",
      "Epoch 342/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 38.3205 - val_loss: 39.9505\n",
      "Epoch 343/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 38.8780 - val_loss: 43.3726\n",
      "Epoch 344/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 39.4046 - val_loss: 40.6709\n",
      "Epoch 345/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 39.2284 - val_loss: 40.2760\n",
      "Epoch 346/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 38.7402 - val_loss: 40.4567\n",
      "Epoch 347/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 39.0634 - val_loss: 40.1521\n",
      "Epoch 348/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 38.0024 - val_loss: 39.4183\n",
      "Epoch 349/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 38.0908 - val_loss: 39.2483\n",
      "Epoch 350/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 38.0069 - val_loss: 39.9820\n",
      "Epoch 351/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 37.6099 - val_loss: 39.4712\n",
      "Epoch 352/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 37.9860 - val_loss: 38.4636\n",
      "Epoch 353/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 37.2567 - val_loss: 38.9274\n",
      "Epoch 354/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 37.4040 - val_loss: 38.7465\n",
      "Epoch 355/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 37.1336 - val_loss: 38.9513\n",
      "Epoch 356/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 37.2909 - val_loss: 38.6520\n",
      "Epoch 357/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 37.1106 - val_loss: 38.3599\n",
      "Epoch 358/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 37.2116 - val_loss: 38.8732\n",
      "Epoch 359/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 37.0963 - val_loss: 38.8815\n",
      "Epoch 360/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 37.2585 - val_loss: 38.0017\n",
      "Epoch 361/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 37.2750 - val_loss: 39.4796\n",
      "Epoch 362/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 37.0299 - val_loss: 38.3862\n",
      "Epoch 363/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 36.8545 - val_loss: 38.3814\n",
      "Epoch 364/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 36.9144 - val_loss: 37.5300\n",
      "Epoch 365/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 36.5339 - val_loss: 37.2732\n",
      "Epoch 366/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 36.6495 - val_loss: 37.5190\n",
      "Epoch 367/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 36.4715 - val_loss: 37.3246\n",
      "Epoch 368/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 36.3994 - val_loss: 37.5001\n",
      "Epoch 369/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 36.5842 - val_loss: 37.9569\n",
      "Epoch 370/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 36.5428 - val_loss: 37.4925\n",
      "Epoch 371/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 36.7827 - val_loss: 38.1395\n",
      "Epoch 372/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 37.0445 - val_loss: 37.7032\n",
      "Epoch 373/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 36.0809 - val_loss: 36.9217\n",
      "Epoch 374/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 35.7165 - val_loss: 37.0479\n",
      "Epoch 375/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 35.6551 - val_loss: 37.5316\n",
      "Epoch 376/2000\n",
      "7/7 [==============================] - 1s 107ms/step - loss: 35.4958 - val_loss: 37.3909\n",
      "Epoch 377/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 35.6724 - val_loss: 36.4653\n",
      "Epoch 378/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 35.8060 - val_loss: 36.9485\n",
      "Epoch 379/2000\n",
      "7/7 [==============================] - 1s 107ms/step - loss: 35.2501 - val_loss: 36.5006\n",
      "Epoch 380/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 35.2683 - val_loss: 36.4316\n",
      "Epoch 381/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 35.3243 - val_loss: 37.5365\n",
      "Epoch 382/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 35.2875 - val_loss: 37.5587\n",
      "Epoch 383/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 35.3537 - val_loss: 36.7973\n",
      "Epoch 384/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 35.0096 - val_loss: 37.1833\n",
      "Epoch 385/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 35.0732 - val_loss: 37.0411\n",
      "Epoch 386/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 35.0401 - val_loss: 36.4125\n",
      "Epoch 387/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 34.8650 - val_loss: 36.4855\n",
      "Epoch 388/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 34.7160 - val_loss: 36.9377\n",
      "Epoch 389/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 34.7662 - val_loss: 35.6761\n",
      "Epoch 390/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 34.5246 - val_loss: 36.0052\n",
      "Epoch 391/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 34.7828 - val_loss: 36.1420\n",
      "Epoch 392/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 34.7956 - val_loss: 36.8313\n",
      "Epoch 393/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 35.3725 - val_loss: 36.9405\n",
      "Epoch 394/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 35.1249 - val_loss: 36.5208\n",
      "Epoch 395/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 34.4571 - val_loss: 35.9901\n",
      "Epoch 396/2000\n",
      "7/7 [==============================] - 1s 107ms/step - loss: 34.4204 - val_loss: 35.6756\n",
      "Epoch 397/2000\n",
      "7/7 [==============================] - 1s 107ms/step - loss: 34.3349 - val_loss: 36.1208\n",
      "Epoch 398/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 34.6939 - val_loss: 36.1952\n",
      "Epoch 399/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 34.8611 - val_loss: 35.8134\n",
      "Epoch 400/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 34.2334\n",
      "Epoch 00400: saving model to saved_models/latent16/cp-0400.h5\n",
      "7/7 [==============================] - 1s 170ms/step - loss: 34.2334 - val_loss: 35.1548\n",
      "Epoch 401/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 33.9793 - val_loss: 35.9698\n",
      "Epoch 402/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 34.1239 - val_loss: 35.8784\n",
      "Epoch 403/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 34.2881 - val_loss: 35.9639\n",
      "Epoch 404/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 34.1948 - val_loss: 36.1341\n",
      "Epoch 405/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 34.2961 - val_loss: 34.8563\n",
      "Epoch 406/2000\n",
      "7/7 [==============================] - 1s 107ms/step - loss: 33.7654 - val_loss: 35.8368\n",
      "Epoch 407/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 33.7839 - val_loss: 35.3840\n",
      "Epoch 408/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 34.0849 - val_loss: 35.8214\n",
      "Epoch 409/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 34.2705 - val_loss: 35.9506\n",
      "Epoch 410/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 34.2712 - val_loss: 35.1474\n",
      "Epoch 411/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 33.5398 - val_loss: 35.3199\n",
      "Epoch 412/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 33.6239 - val_loss: 35.2756\n",
      "Epoch 413/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 33.5177 - val_loss: 35.7590\n",
      "Epoch 414/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 33.3825 - val_loss: 35.4552\n",
      "Epoch 415/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 33.3783 - val_loss: 35.0372\n",
      "Epoch 416/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 33.3471 - val_loss: 35.7189\n",
      "Epoch 417/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 33.4542 - val_loss: 35.2560\n",
      "Epoch 418/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 33.4922 - val_loss: 35.1209\n",
      "Epoch 419/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 33.3045 - val_loss: 34.2235\n",
      "Epoch 420/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 33.2332 - val_loss: 35.2938\n",
      "Epoch 421/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 33.0773 - val_loss: 34.6892\n",
      "Epoch 422/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 32.7986 - val_loss: 34.9366\n",
      "Epoch 423/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 33.4913 - val_loss: 35.4014\n",
      "Epoch 424/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 33.0558 - val_loss: 34.7127\n",
      "Epoch 425/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 32.7472 - val_loss: 34.4775\n",
      "Epoch 426/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 32.8751 - val_loss: 34.7036\n",
      "Epoch 427/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.9826 - val_loss: 34.5854\n",
      "Epoch 428/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 32.8672 - val_loss: 33.8859\n",
      "Epoch 429/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 33.0958 - val_loss: 34.2434\n",
      "Epoch 430/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 33.5488 - val_loss: 34.4224\n",
      "Epoch 431/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 33.0136 - val_loss: 35.2575\n",
      "Epoch 432/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 33.3265 - val_loss: 34.7566\n",
      "Epoch 433/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 33.6128 - val_loss: 36.2609\n",
      "Epoch 434/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 33.4303 - val_loss: 34.7276\n",
      "Epoch 435/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 33.3941 - val_loss: 34.6701\n",
      "Epoch 436/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 32.7150 - val_loss: 34.1269\n",
      "Epoch 437/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 32.5744 - val_loss: 33.7912\n",
      "Epoch 438/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 32.4497 - val_loss: 34.0140\n",
      "Epoch 439/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.7672 - val_loss: 34.0923\n",
      "Epoch 440/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 32.5565 - val_loss: 34.6009\n",
      "Epoch 441/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 32.3972 - val_loss: 33.7839\n",
      "Epoch 442/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 32.1876 - val_loss: 33.9137\n",
      "Epoch 443/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 31.9069 - val_loss: 33.5490\n",
      "Epoch 444/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.8890 - val_loss: 33.7380\n",
      "Epoch 445/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 31.7619 - val_loss: 33.1417\n",
      "Epoch 446/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.0058 - val_loss: 33.7326\n",
      "Epoch 447/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.9578 - val_loss: 34.2416\n",
      "Epoch 448/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.9077 - val_loss: 33.7419\n",
      "Epoch 449/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.8795 - val_loss: 34.6166\n",
      "Epoch 450/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 32.0805 - val_loss: 34.0546\n",
      "Epoch 451/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.8692 - val_loss: 33.6933\n",
      "Epoch 452/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.9847 - val_loss: 34.3812\n",
      "Epoch 453/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.9049 - val_loss: 34.6517\n",
      "Epoch 454/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.9321 - val_loss: 33.5371\n",
      "Epoch 455/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 31.5919 - val_loss: 33.6442\n",
      "Epoch 456/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.6496 - val_loss: 35.1657\n",
      "Epoch 457/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 32.4513 - val_loss: 34.3135\n",
      "Epoch 458/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.0812 - val_loss: 34.8047\n",
      "Epoch 459/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 32.5284 - val_loss: 33.5396\n",
      "Epoch 460/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.8706 - val_loss: 34.0379\n",
      "Epoch 461/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.6804 - val_loss: 33.9239\n",
      "Epoch 462/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.7072 - val_loss: 32.7396\n",
      "Epoch 463/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 31.5503 - val_loss: 33.2349\n",
      "Epoch 464/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 31.4169 - val_loss: 33.5380\n",
      "Epoch 465/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.4471 - val_loss: 34.1018\n",
      "Epoch 466/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.8063 - val_loss: 33.2407\n",
      "Epoch 467/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.1885 - val_loss: 33.0693\n",
      "Epoch 468/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.3527 - val_loss: 33.2073\n",
      "Epoch 469/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 30.9911 - val_loss: 32.9533\n",
      "Epoch 470/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.1796 - val_loss: 33.0411\n",
      "Epoch 471/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.2383 - val_loss: 32.9647\n",
      "Epoch 472/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.4018 - val_loss: 32.3823\n",
      "Epoch 473/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 30.8840 - val_loss: 32.5072\n",
      "Epoch 474/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.8676 - val_loss: 32.4444\n",
      "Epoch 475/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.0688 - val_loss: 32.9278\n",
      "Epoch 476/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.1943 - val_loss: 32.8600\n",
      "Epoch 477/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 31.0484 - val_loss: 32.7870\n",
      "Epoch 478/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 30.7524 - val_loss: 32.7925\n",
      "Epoch 479/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.7603 - val_loss: 32.7301\n",
      "Epoch 480/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 30.7327 - val_loss: 32.4343\n",
      "Epoch 481/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.7487 - val_loss: 33.8897\n",
      "Epoch 482/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.9468 - val_loss: 33.2939\n",
      "Epoch 483/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.9336 - val_loss: 32.8238\n",
      "Epoch 484/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 30.6115 - val_loss: 33.6870\n",
      "Epoch 485/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 31.5209 - val_loss: 33.2711\n",
      "Epoch 486/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.7593 - val_loss: 33.2088\n",
      "Epoch 487/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.6261 - val_loss: 32.7355\n",
      "Epoch 488/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.7297 - val_loss: 32.3528\n",
      "Epoch 489/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.7525 - val_loss: 31.8662\n",
      "Epoch 490/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.4617 - val_loss: 33.4289\n",
      "Epoch 491/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.9762 - val_loss: 32.3888\n",
      "Epoch 492/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.4859 - val_loss: 31.6112\n",
      "Epoch 493/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.4863 - val_loss: 32.1894\n",
      "Epoch 494/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 30.3243 - val_loss: 32.5957\n",
      "Epoch 495/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 30.2896 - val_loss: 32.0301\n",
      "Epoch 496/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.3965 - val_loss: 31.6735\n",
      "Epoch 497/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.3968 - val_loss: 33.2095\n",
      "Epoch 498/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.9726 - val_loss: 33.3509\n",
      "Epoch 499/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.4031 - val_loss: 32.0285\n",
      "Epoch 500/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.5704 - val_loss: 31.7056\n",
      "Epoch 501/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.5477 - val_loss: 31.9607\n",
      "Epoch 502/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.4021 - val_loss: 32.5111\n",
      "Epoch 503/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.3346 - val_loss: 32.4943\n",
      "Epoch 504/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.2173 - val_loss: 31.7644\n",
      "Epoch 505/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 30.2064 - val_loss: 32.8245\n",
      "Epoch 506/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.3479 - val_loss: 32.2733\n",
      "Epoch 507/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 30.1184 - val_loss: 31.9262\n",
      "Epoch 508/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 29.9803 - val_loss: 32.5298\n",
      "Epoch 509/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.0173 - val_loss: 31.7795\n",
      "Epoch 510/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 29.9977 - val_loss: 32.2032\n",
      "Epoch 511/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.0849 - val_loss: 32.6113\n",
      "Epoch 512/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.0048 - val_loss: 31.9587\n",
      "Epoch 513/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 29.7063 - val_loss: 32.3569\n",
      "Epoch 514/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.0398 - val_loss: 31.6379\n",
      "Epoch 515/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.0643 - val_loss: 32.3798\n",
      "Epoch 516/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 29.8765 - val_loss: 31.7570\n",
      "Epoch 517/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 29.9163 - val_loss: 32.2108\n",
      "Epoch 518/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 29.8973 - val_loss: 32.5619\n",
      "Epoch 519/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 29.8839 - val_loss: 32.9884\n",
      "Epoch 520/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 30.1527 - val_loss: 31.4054\n",
      "Epoch 521/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 29.6600 - val_loss: 32.2523\n",
      "Epoch 522/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.0045 - val_loss: 31.4077\n",
      "Epoch 523/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 29.8323 - val_loss: 32.0019\n",
      "Epoch 524/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 29.6006 - val_loss: 31.5205\n",
      "Epoch 525/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 29.6164 - val_loss: 31.8977\n",
      "Epoch 526/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 29.7999 - val_loss: 32.6402\n",
      "Epoch 527/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 29.8675 - val_loss: 32.6335\n",
      "Epoch 528/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 29.7840 - val_loss: 31.6115\n",
      "Epoch 529/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 30.1103 - val_loss: 32.5936\n",
      "Epoch 530/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 29.8269 - val_loss: 31.3419\n",
      "Epoch 531/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 29.5671 - val_loss: 31.4573\n",
      "Epoch 532/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 29.5047 - val_loss: 31.7275\n",
      "Epoch 533/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 29.3387 - val_loss: 31.2490\n",
      "Epoch 534/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 29.3459 - val_loss: 31.4803\n",
      "Epoch 535/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 29.2525 - val_loss: 31.1724\n",
      "Epoch 536/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 29.4747 - val_loss: 32.3686\n",
      "Epoch 537/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 29.6158 - val_loss: 32.6659\n",
      "Epoch 538/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 29.7976 - val_loss: 31.2292\n",
      "Epoch 539/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 29.4420 - val_loss: 31.3764\n",
      "Epoch 540/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 29.0677 - val_loss: 31.6372\n",
      "Epoch 541/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 29.3598 - val_loss: 31.7918\n",
      "Epoch 542/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 29.0839 - val_loss: 31.1344\n",
      "Epoch 543/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 29.1064 - val_loss: 31.5038\n",
      "Epoch 544/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 29.2888 - val_loss: 31.2133\n",
      "Epoch 545/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 29.5312 - val_loss: 31.6776\n",
      "Epoch 546/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 29.9353 - val_loss: 31.3653\n",
      "Epoch 547/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 29.5352 - val_loss: 31.3838\n",
      "Epoch 548/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 29.3786 - val_loss: 31.8956\n",
      "Epoch 549/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 29.0789 - val_loss: 31.8138\n",
      "Epoch 550/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 29.3192 - val_loss: 30.9488\n",
      "Epoch 551/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 29.0212 - val_loss: 31.8558\n",
      "Epoch 552/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 29.4231 - val_loss: 32.3069\n",
      "Epoch 553/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 29.2712 - val_loss: 31.4665\n",
      "Epoch 554/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 29.2798 - val_loss: 31.1460\n",
      "Epoch 555/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 29.0542 - val_loss: 31.3636\n",
      "Epoch 556/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 28.9648 - val_loss: 31.4105\n",
      "Epoch 557/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 28.9349 - val_loss: 32.9508\n",
      "Epoch 558/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 29.4865 - val_loss: 30.9580\n",
      "Epoch 559/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 29.4400 - val_loss: 31.1790\n",
      "Epoch 560/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 29.0121 - val_loss: 32.0554\n",
      "Epoch 561/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 29.0703 - val_loss: 31.2473\n",
      "Epoch 562/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 28.9552 - val_loss: 31.4509\n",
      "Epoch 563/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 29.0708 - val_loss: 31.4791\n",
      "Epoch 564/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 29.1829 - val_loss: 30.9803\n",
      "Epoch 565/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 28.9856 - val_loss: 31.7139\n",
      "Epoch 566/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 28.7513 - val_loss: 31.4767\n",
      "Epoch 567/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 28.5966 - val_loss: 30.5144\n",
      "Epoch 568/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 28.7810 - val_loss: 30.8920\n",
      "Epoch 569/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 28.8974 - val_loss: 30.9569\n",
      "Epoch 570/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 28.8337 - val_loss: 31.3108\n",
      "Epoch 571/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.9430 - val_loss: 32.3159\n",
      "Epoch 572/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 29.0727 - val_loss: 31.8327\n",
      "Epoch 573/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 28.6418 - val_loss: 31.0382\n",
      "Epoch 574/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 28.4892 - val_loss: 31.0418\n",
      "Epoch 575/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 28.6417 - val_loss: 31.3049\n",
      "Epoch 576/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 28.5562 - val_loss: 30.8892\n",
      "Epoch 577/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 28.7309 - val_loss: 31.3222\n",
      "Epoch 578/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 28.7360 - val_loss: 30.7867\n",
      "Epoch 579/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 28.4950 - val_loss: 30.7334\n",
      "Epoch 580/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.6181 - val_loss: 31.2444\n",
      "Epoch 581/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.6733 - val_loss: 31.8703\n",
      "Epoch 582/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.6767 - val_loss: 31.2662\n",
      "Epoch 583/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.8238 - val_loss: 31.5222\n",
      "Epoch 584/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.7980 - val_loss: 31.4823\n",
      "Epoch 585/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 28.5292 - val_loss: 30.7372\n",
      "Epoch 586/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 28.3651 - val_loss: 30.8282\n",
      "Epoch 587/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.6364 - val_loss: 30.8553\n",
      "Epoch 588/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 28.6161 - val_loss: 32.3125\n",
      "Epoch 589/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 29.0522 - val_loss: 31.2243\n",
      "Epoch 590/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 28.4988 - val_loss: 30.9122\n",
      "Epoch 591/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 28.4283 - val_loss: 30.6993\n",
      "Epoch 592/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.4353 - val_loss: 30.7314\n",
      "Epoch 593/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 28.6643 - val_loss: 30.6092\n",
      "Epoch 594/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 28.6349 - val_loss: 30.2875\n",
      "Epoch 595/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 28.3022 - val_loss: 30.8832\n",
      "Epoch 596/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.3114 - val_loss: 31.3539\n",
      "Epoch 597/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 28.6332 - val_loss: 32.1643\n",
      "Epoch 598/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.6937 - val_loss: 31.5880\n",
      "Epoch 599/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 28.4544 - val_loss: 30.4210\n",
      "Epoch 600/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 28.1505\n",
      "Epoch 00600: saving model to saved_models/latent16/cp-0600.h5\n",
      "7/7 [==============================] - 1s 166ms/step - loss: 28.1505 - val_loss: 30.4141\n",
      "Epoch 601/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 28.0502 - val_loss: 30.5797\n",
      "Epoch 602/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.2199 - val_loss: 30.4886\n",
      "Epoch 603/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.2741 - val_loss: 31.3049\n",
      "Epoch 604/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.9558 - val_loss: 31.9547\n",
      "Epoch 605/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 28.9030 - val_loss: 31.1116\n",
      "Epoch 606/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 28.3111 - val_loss: 32.0451\n",
      "Epoch 607/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 28.5558 - val_loss: 30.6651\n",
      "Epoch 608/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.2676 - val_loss: 30.7927\n",
      "Epoch 609/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 28.0239 - val_loss: 31.2533\n",
      "Epoch 610/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 28.3042 - val_loss: 31.4591\n",
      "Epoch 611/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 28.2402 - val_loss: 30.9512\n",
      "Epoch 612/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.0710 - val_loss: 30.9690\n",
      "Epoch 613/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 28.3679 - val_loss: 30.3612\n",
      "Epoch 614/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 28.4517 - val_loss: 30.7512\n",
      "Epoch 615/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.6600 - val_loss: 31.0727\n",
      "Epoch 616/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 28.2064 - val_loss: 30.6071\n",
      "Epoch 617/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 27.9838 - val_loss: 30.4272\n",
      "Epoch 618/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 27.9384 - val_loss: 30.4099\n",
      "Epoch 619/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.9976 - val_loss: 30.8818\n",
      "Epoch 620/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.0438 - val_loss: 30.9014\n",
      "Epoch 621/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 28.0323 - val_loss: 30.1210\n",
      "Epoch 622/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.9920 - val_loss: 31.3099\n",
      "Epoch 623/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 27.9267 - val_loss: 30.6820\n",
      "Epoch 624/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 27.9085 - val_loss: 30.6481\n",
      "Epoch 625/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 27.7990 - val_loss: 30.3966\n",
      "Epoch 626/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.8981 - val_loss: 30.6756\n",
      "Epoch 627/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 28.0989 - val_loss: 30.6331\n",
      "Epoch 628/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 27.7561 - val_loss: 30.2582\n",
      "Epoch 629/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 28.2150 - val_loss: 30.2799\n",
      "Epoch 630/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 27.6703 - val_loss: 30.4837\n",
      "Epoch 631/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 28.1178 - val_loss: 31.7487\n",
      "Epoch 632/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 28.3909 - val_loss: 30.5245\n",
      "Epoch 633/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.8199 - val_loss: 30.0760\n",
      "Epoch 634/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.6566 - val_loss: 30.0415\n",
      "Epoch 635/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.8836 - val_loss: 30.5585\n",
      "Epoch 636/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 27.5874 - val_loss: 29.7863\n",
      "Epoch 637/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.8431 - val_loss: 30.6805\n",
      "Epoch 638/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.9830 - val_loss: 31.4579\n",
      "Epoch 639/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.9104 - val_loss: 30.6586\n",
      "Epoch 640/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.6086 - val_loss: 29.5930\n",
      "Epoch 641/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.7105 - val_loss: 30.6448\n",
      "Epoch 642/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.4810 - val_loss: 32.2574\n",
      "Epoch 643/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.5301 - val_loss: 30.9142\n",
      "Epoch 644/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 28.3546 - val_loss: 30.4846\n",
      "Epoch 645/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 28.0570 - val_loss: 31.2086\n",
      "Epoch 646/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 28.0626 - val_loss: 31.1520\n",
      "Epoch 647/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.7359 - val_loss: 30.5138\n",
      "Epoch 648/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.6508 - val_loss: 29.8317\n",
      "Epoch 649/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.5965 - val_loss: 29.9101\n",
      "Epoch 650/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.7203 - val_loss: 30.1545\n",
      "Epoch 651/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.9634 - val_loss: 30.2503\n",
      "Epoch 652/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 27.5278 - val_loss: 30.2800\n",
      "Epoch 653/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 27.3865 - val_loss: 29.7073\n",
      "Epoch 654/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.6498 - val_loss: 29.9851\n",
      "Epoch 655/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.4299 - val_loss: 30.6979\n",
      "Epoch 656/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.4280 - val_loss: 29.8266\n",
      "Epoch 657/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.5026 - val_loss: 31.4494\n",
      "Epoch 658/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.9909 - val_loss: 30.7524\n",
      "Epoch 659/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.6753 - val_loss: 29.7471\n",
      "Epoch 660/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.5143 - val_loss: 30.2072\n",
      "Epoch 661/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.6610 - val_loss: 30.2382\n",
      "Epoch 662/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.2399 - val_loss: 30.1320\n",
      "Epoch 663/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.4340 - val_loss: 30.2640\n",
      "Epoch 664/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.4041 - val_loss: 30.2055\n",
      "Epoch 665/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.2734 - val_loss: 30.5830\n",
      "Epoch 666/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 28.1829 - val_loss: 30.8379\n",
      "Epoch 667/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.8717 - val_loss: 31.4343\n",
      "Epoch 668/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.6758 - val_loss: 30.3264\n",
      "Epoch 669/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.3658 - val_loss: 29.8125\n",
      "Epoch 670/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.2903 - val_loss: 30.0521\n",
      "Epoch 671/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.5485 - val_loss: 29.9077\n",
      "Epoch 672/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.4700 - val_loss: 30.2061\n",
      "Epoch 673/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.4631 - val_loss: 30.7706\n",
      "Epoch 674/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 27.1044 - val_loss: 30.5901\n",
      "Epoch 675/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.4694 - val_loss: 29.9258\n",
      "Epoch 676/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.6470 - val_loss: 30.4569\n",
      "Epoch 677/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 28.1006 - val_loss: 29.7182\n",
      "Epoch 678/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.6259 - val_loss: 29.7408\n",
      "Epoch 679/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.2251 - val_loss: 30.1851\n",
      "Epoch 680/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.6828 - val_loss: 30.1733\n",
      "Epoch 681/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.4106 - val_loss: 30.6825\n",
      "Epoch 682/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.3251 - val_loss: 30.9442\n",
      "Epoch 683/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.6441 - val_loss: 30.1823\n",
      "Epoch 684/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.3213 - val_loss: 30.1076\n",
      "Epoch 685/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.6769 - val_loss: 30.2177\n",
      "Epoch 686/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.1361 - val_loss: 29.8006\n",
      "Epoch 687/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.2832 - val_loss: 29.8006\n",
      "Epoch 688/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.2749 - val_loss: 30.5984\n",
      "Epoch 689/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.5413 - val_loss: 30.0337\n",
      "Epoch 690/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.2279 - val_loss: 30.0110\n",
      "Epoch 691/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.5553 - val_loss: 30.1357\n",
      "Epoch 692/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.1597 - val_loss: 29.7311\n",
      "Epoch 693/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.0749 - val_loss: 30.1217\n",
      "Epoch 694/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.2568 - val_loss: 29.8950\n",
      "Epoch 695/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.3462 - val_loss: 30.2167\n",
      "Epoch 696/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.3737 - val_loss: 30.6069\n",
      "Epoch 697/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.6832 - val_loss: 29.8438\n",
      "Epoch 698/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.2091 - val_loss: 29.9989\n",
      "Epoch 699/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.1236 - val_loss: 29.8833\n",
      "Epoch 700/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.2245 - val_loss: 29.5971\n",
      "Epoch 701/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 26.9939 - val_loss: 29.5817\n",
      "Epoch 702/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 26.9349 - val_loss: 29.7773\n",
      "Epoch 703/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.2894 - val_loss: 29.8691\n",
      "Epoch 704/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.0950 - val_loss: 30.7246\n",
      "Epoch 705/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.1067 - val_loss: 30.2404\n",
      "Epoch 706/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.1063 - val_loss: 29.3864\n",
      "Epoch 707/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.0589 - val_loss: 30.4329\n",
      "Epoch 708/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.2342 - val_loss: 30.2250\n",
      "Epoch 709/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.8107 - val_loss: 30.3754\n",
      "Epoch 710/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.0834 - val_loss: 30.8553\n",
      "Epoch 711/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.6325 - val_loss: 30.5675\n",
      "Epoch 712/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.1100 - val_loss: 29.7502\n",
      "Epoch 713/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.1787 - val_loss: 30.0750\n",
      "Epoch 714/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.1353 - val_loss: 30.3561\n",
      "Epoch 715/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.9768 - val_loss: 29.6890\n",
      "Epoch 716/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.1394 - val_loss: 29.6521\n",
      "Epoch 717/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.0935 - val_loss: 29.5307\n",
      "Epoch 718/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.8138 - val_loss: 30.2290\n",
      "Epoch 719/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.0300 - val_loss: 30.3809\n",
      "Epoch 720/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.0504 - val_loss: 29.9364\n",
      "Epoch 721/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.1882 - val_loss: 29.7597\n",
      "Epoch 722/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 26.9575 - val_loss: 29.7340\n",
      "Epoch 723/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.1391 - val_loss: 30.1858\n",
      "Epoch 724/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 26.8036 - val_loss: 29.7503\n",
      "Epoch 725/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.0248 - val_loss: 30.1335\n",
      "Epoch 726/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.3695 - val_loss: 29.6976\n",
      "Epoch 727/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.1914 - val_loss: 29.5320\n",
      "Epoch 728/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.1825 - val_loss: 29.7391\n",
      "Epoch 729/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.4790 - val_loss: 29.8735\n",
      "Epoch 730/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.9426 - val_loss: 30.0355\n",
      "Epoch 731/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.1149 - val_loss: 29.8043\n",
      "Epoch 732/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.8799 - val_loss: 29.8847\n",
      "Epoch 733/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 26.7771 - val_loss: 30.1416\n",
      "Epoch 734/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.9560 - val_loss: 30.4646\n",
      "Epoch 735/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.0951 - val_loss: 29.9555\n",
      "Epoch 736/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.0394 - val_loss: 30.2335\n",
      "Epoch 737/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.9743 - val_loss: 30.8292\n",
      "Epoch 738/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.9173 - val_loss: 31.0019\n",
      "Epoch 739/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.0547 - val_loss: 31.0332\n",
      "Epoch 740/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.1509 - val_loss: 30.2119\n",
      "Epoch 741/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.8870 - val_loss: 30.3028\n",
      "Epoch 742/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 26.6879 - val_loss: 30.2537\n",
      "Epoch 743/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.6952 - val_loss: 29.3710\n",
      "Epoch 744/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.9039 - val_loss: 30.2330\n",
      "Epoch 745/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.9605 - val_loss: 29.4258\n",
      "Epoch 746/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.0178 - val_loss: 29.5202\n",
      "Epoch 747/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.7731 - val_loss: 29.7386\n",
      "Epoch 748/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 26.5554 - val_loss: 29.4113\n",
      "Epoch 749/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.6791 - val_loss: 30.1416\n",
      "Epoch 750/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.6158 - val_loss: 30.0757\n",
      "Epoch 751/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.6281 - val_loss: 29.7467\n",
      "Epoch 752/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.7657 - val_loss: 29.2523\n",
      "Epoch 753/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.6568 - val_loss: 29.3518\n",
      "Epoch 754/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.7295 - val_loss: 29.6857\n",
      "Epoch 755/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.9218 - val_loss: 29.8198\n",
      "Epoch 756/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.7227 - val_loss: 29.4873\n",
      "Epoch 757/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.7972 - val_loss: 30.4129\n",
      "Epoch 758/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.5834 - val_loss: 29.8481\n",
      "Epoch 759/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 26.5140 - val_loss: 29.7487\n",
      "Epoch 760/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.6485 - val_loss: 29.4923\n",
      "Epoch 761/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.5722 - val_loss: 30.0276\n",
      "Epoch 762/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.8020 - val_loss: 30.9499\n",
      "Epoch 763/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.8975 - val_loss: 29.4246\n",
      "Epoch 764/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.8001 - val_loss: 29.1955\n",
      "Epoch 765/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.8206 - val_loss: 29.1576\n",
      "Epoch 766/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.3774 - val_loss: 29.6394\n",
      "Epoch 767/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.1114 - val_loss: 30.6415\n",
      "Epoch 768/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.4063 - val_loss: 29.8447\n",
      "Epoch 769/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.2393 - val_loss: 28.8868\n",
      "Epoch 770/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.4313 - val_loss: 31.2170\n",
      "Epoch 771/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.5270 - val_loss: 31.3253\n",
      "Epoch 772/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.3877 - val_loss: 29.9691\n",
      "Epoch 773/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.5742 - val_loss: 29.7221\n",
      "Epoch 774/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.6175 - val_loss: 32.4488\n",
      "Epoch 775/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.6061 - val_loss: 30.5062\n",
      "Epoch 776/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.5093 - val_loss: 29.9690\n",
      "Epoch 777/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.7662 - val_loss: 29.4126\n",
      "Epoch 778/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.6519 - val_loss: 30.2760\n",
      "Epoch 779/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.5697 - val_loss: 29.2587\n",
      "Epoch 780/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.3698 - val_loss: 29.9029\n",
      "Epoch 781/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.4497 - val_loss: 30.2190\n",
      "Epoch 782/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 26.3294 - val_loss: 29.3132\n",
      "Epoch 783/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.4547 - val_loss: 30.3368\n",
      "Epoch 784/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.5362 - val_loss: 29.1029\n",
      "Epoch 785/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.3912 - val_loss: 29.3138\n",
      "Epoch 786/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.4665 - val_loss: 29.2843\n",
      "Epoch 787/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.4797 - val_loss: 30.2132\n",
      "Epoch 788/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.6573 - val_loss: 29.5008\n",
      "Epoch 789/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.4679 - val_loss: 29.1990\n",
      "Epoch 790/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 26.2980 - val_loss: 30.2274\n",
      "Epoch 791/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.6732 - val_loss: 29.4270\n",
      "Epoch 792/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.5095 - val_loss: 29.7347\n",
      "Epoch 793/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.4132 - val_loss: 29.2238\n",
      "Epoch 794/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.3066 - val_loss: 29.1959\n",
      "Epoch 795/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.5269 - val_loss: 29.6928\n",
      "Epoch 796/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 26.2428 - val_loss: 29.0335\n",
      "Epoch 797/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.2622 - val_loss: 29.6441\n",
      "Epoch 798/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 26.1993 - val_loss: 29.3415\n",
      "Epoch 799/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.2691 - val_loss: 29.8032\n",
      "Epoch 800/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 26.3068\n",
      "Epoch 00800: saving model to saved_models/latent16/cp-0800.h5\n",
      "7/7 [==============================] - 1s 143ms/step - loss: 26.3068 - val_loss: 29.3269\n",
      "Epoch 801/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.2730 - val_loss: 29.1199\n",
      "Epoch 802/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.3330 - val_loss: 29.9677\n",
      "Epoch 803/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.5358 - val_loss: 30.8821\n",
      "Epoch 804/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.7342 - val_loss: 29.7187\n",
      "Epoch 805/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.3895 - val_loss: 29.3162\n",
      "Epoch 806/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.5995 - val_loss: 30.0957\n",
      "Epoch 807/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.4995 - val_loss: 29.2275\n",
      "Epoch 808/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.3533 - val_loss: 29.0719\n",
      "Epoch 809/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.5286 - val_loss: 29.1924\n",
      "Epoch 810/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.4651 - val_loss: 29.4007\n",
      "Epoch 811/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.2480 - val_loss: 30.1714\n",
      "Epoch 812/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.3050 - val_loss: 29.2875\n",
      "Epoch 813/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.4028 - val_loss: 29.9524\n",
      "Epoch 814/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.4061 - val_loss: 28.8553\n",
      "Epoch 815/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 26.1922 - val_loss: 29.7242\n",
      "Epoch 816/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.3730 - val_loss: 29.2871\n",
      "Epoch 817/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.2836 - val_loss: 29.0154\n",
      "Epoch 818/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.3894 - val_loss: 29.4973\n",
      "Epoch 819/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.2569 - val_loss: 29.7860\n",
      "Epoch 820/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.1991 - val_loss: 29.6551\n",
      "Epoch 821/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.3147 - val_loss: 29.7311\n",
      "Epoch 822/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.4417 - val_loss: 29.3200\n",
      "Epoch 823/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.3692 - val_loss: 30.3232\n",
      "Epoch 824/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.6144 - val_loss: 30.5895\n",
      "Epoch 825/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.7260 - val_loss: 30.1002\n",
      "Epoch 826/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.5174 - val_loss: 30.5263\n",
      "Epoch 827/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.7198 - val_loss: 31.9684\n",
      "Epoch 828/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.3816 - val_loss: 30.8154\n",
      "Epoch 829/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 27.0902 - val_loss: 29.2486\n",
      "Epoch 830/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.7813 - val_loss: 29.8506\n",
      "Epoch 831/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 26.3520 - val_loss: 29.7529\n",
      "Epoch 832/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.3748 - val_loss: 29.4480\n",
      "Epoch 833/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.4461 - val_loss: 29.2944\n",
      "Epoch 834/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.2151 - val_loss: 29.7952\n",
      "Epoch 835/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.1934 - val_loss: 29.7589\n",
      "Epoch 836/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.2831 - val_loss: 29.6728\n",
      "Epoch 837/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.4480 - val_loss: 29.4299\n",
      "Epoch 838/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.3710 - val_loss: 29.7212\n",
      "Epoch 839/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.1931 - val_loss: 29.4695\n",
      "Epoch 840/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.2950 - val_loss: 29.6263\n",
      "Epoch 841/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.2987 - val_loss: 29.4044\n",
      "Epoch 842/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.7171 - val_loss: 29.7987\n",
      "Epoch 843/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.9520 - val_loss: 30.0945\n",
      "Epoch 844/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.0104 - val_loss: 31.7173\n",
      "Epoch 845/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 27.3262 - val_loss: 29.8489\n",
      "Epoch 846/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.7810 - val_loss: 30.4907\n",
      "Epoch 847/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.5650 - val_loss: 29.4042\n",
      "Epoch 848/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.3366 - val_loss: 29.4236\n",
      "Epoch 849/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.2148 - val_loss: 30.3578\n",
      "Epoch 850/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.2260 - val_loss: 29.3928\n",
      "Epoch 851/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.3039 - val_loss: 29.5701\n",
      "Epoch 852/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.1978 - val_loss: 28.9266\n",
      "Epoch 853/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.2668 - val_loss: 29.6687\n",
      "Epoch 854/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 26.1405 - val_loss: 29.1760\n",
      "Epoch 855/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.1891 - val_loss: 29.4259\n",
      "Epoch 856/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.1777 - val_loss: 30.3360\n",
      "Epoch 857/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.5720 - val_loss: 30.2453\n",
      "Epoch 858/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.3173 - val_loss: 29.9533\n",
      "Epoch 859/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 26.1027 - val_loss: 29.6265\n",
      "Epoch 860/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.1523 - val_loss: 29.8083\n",
      "Epoch 861/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.2889 - val_loss: 29.3327\n",
      "Epoch 862/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.3651 - val_loss: 29.5345\n",
      "Epoch 863/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.2306 - val_loss: 29.9696\n",
      "Epoch 864/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.2109 - val_loss: 29.5658\n",
      "Epoch 865/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.9762 - val_loss: 29.9484\n",
      "Epoch 866/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.0860 - val_loss: 30.0882\n",
      "Epoch 867/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.9301 - val_loss: 29.9276\n",
      "Epoch 868/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.0704 - val_loss: 29.8357\n",
      "Epoch 869/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.1502 - val_loss: 29.8256\n",
      "Epoch 870/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.2223 - val_loss: 28.9743\n",
      "Epoch 871/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.4328 - val_loss: 28.8920\n",
      "Epoch 872/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.1820 - val_loss: 29.1595\n",
      "Epoch 873/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.0512 - val_loss: 29.6378\n",
      "Epoch 874/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.0144 - val_loss: 29.9409\n",
      "Epoch 875/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.1344 - val_loss: 29.3236\n",
      "Epoch 876/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.1706 - val_loss: 30.1204\n",
      "Epoch 877/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.2774 - val_loss: 29.4407\n",
      "Epoch 878/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.2045 - val_loss: 29.6159\n",
      "Epoch 879/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.9817 - val_loss: 28.8585\n",
      "Epoch 880/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.0205 - val_loss: 29.4087\n",
      "Epoch 881/2000\n",
      "7/7 [==============================] - 1s 107ms/step - loss: 25.9353 - val_loss: 29.4358\n",
      "Epoch 882/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.9759 - val_loss: 29.3747\n",
      "Epoch 883/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 25.8596 - val_loss: 28.4793\n",
      "Epoch 884/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.0991 - val_loss: 29.0480\n",
      "Epoch 885/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.1406 - val_loss: 29.4553\n",
      "Epoch 886/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.2289 - val_loss: 28.8609\n",
      "Epoch 887/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.9717 - val_loss: 29.7584\n",
      "Epoch 888/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.0049 - val_loss: 29.4077\n",
      "Epoch 889/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.1106 - val_loss: 29.5969\n",
      "Epoch 890/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.1077 - val_loss: 30.2114\n",
      "Epoch 891/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.9382 - val_loss: 29.7242\n",
      "Epoch 892/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.0242 - val_loss: 29.8971\n",
      "Epoch 893/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 26.0728 - val_loss: 29.8174\n",
      "Epoch 894/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.0526 - val_loss: 29.8512\n",
      "Epoch 895/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.0177 - val_loss: 29.6056\n",
      "Epoch 896/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.9245 - val_loss: 29.4781\n",
      "Epoch 897/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 25.7812 - val_loss: 29.6231\n",
      "Epoch 898/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.7916 - val_loss: 29.6081\n",
      "Epoch 899/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.0649 - val_loss: 29.9347\n",
      "Epoch 900/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.2657 - val_loss: 29.6676\n",
      "Epoch 901/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.9136 - val_loss: 29.5791\n",
      "Epoch 902/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 25.7739 - val_loss: 29.3899\n",
      "Epoch 903/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.6203 - val_loss: 29.2160\n",
      "Epoch 904/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.7965 - val_loss: 29.8766\n",
      "Epoch 905/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.1592 - val_loss: 29.5277\n",
      "Epoch 906/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.9015 - val_loss: 30.1696\n",
      "Epoch 907/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.9310 - val_loss: 30.1418\n",
      "Epoch 908/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.2721 - val_loss: 29.2380\n",
      "Epoch 909/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.2551 - val_loss: 29.2022\n",
      "Epoch 910/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.2201 - val_loss: 29.6737\n",
      "Epoch 911/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.0902 - val_loss: 29.8283\n",
      "Epoch 912/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.1278 - val_loss: 30.1570\n",
      "Epoch 913/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.6418 - val_loss: 30.1230\n",
      "Epoch 914/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.1362 - val_loss: 29.0867\n",
      "Epoch 915/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.2358 - val_loss: 29.9542\n",
      "Epoch 916/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 25.7614 - val_loss: 29.6103\n",
      "Epoch 917/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.7730 - val_loss: 29.8840\n",
      "Epoch 918/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.1914 - val_loss: 29.7065\n",
      "Epoch 919/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.0503 - val_loss: 29.7071\n",
      "Epoch 920/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.8742 - val_loss: 29.8235\n",
      "Epoch 921/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.1085 - val_loss: 30.0840\n",
      "Epoch 922/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.0719 - val_loss: 29.7674\n",
      "Epoch 923/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.9848 - val_loss: 29.3943\n",
      "Epoch 924/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.2571 - val_loss: 29.4202\n",
      "Epoch 925/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.8451 - val_loss: 29.4636\n",
      "Epoch 926/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.9829 - val_loss: 29.3383\n",
      "Epoch 927/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.8135 - val_loss: 29.7058\n",
      "Epoch 928/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.0505 - val_loss: 29.0266\n",
      "Epoch 929/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.7890 - val_loss: 29.7466\n",
      "Epoch 930/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.1098 - val_loss: 30.1560\n",
      "Epoch 931/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.0428 - val_loss: 29.2477\n",
      "Epoch 932/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.1684 - val_loss: 29.2014\n",
      "Epoch 933/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.8729 - val_loss: 29.1461\n",
      "Epoch 934/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.9011 - val_loss: 29.8621\n",
      "Epoch 935/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.0946 - val_loss: 30.8546\n",
      "Epoch 936/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.1105 - val_loss: 29.6062\n",
      "Epoch 937/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.0336 - val_loss: 30.8975\n",
      "Epoch 938/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.4097 - val_loss: 29.7424\n",
      "Epoch 939/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.0297 - val_loss: 29.1313\n",
      "Epoch 940/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.5005 - val_loss: 30.3128\n",
      "Epoch 941/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.2401 - val_loss: 29.7325\n",
      "Epoch 942/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.0377 - val_loss: 29.3147\n",
      "Epoch 943/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.9488 - val_loss: 29.2413\n",
      "Epoch 944/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.9313 - val_loss: 29.7454\n",
      "Epoch 945/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 25.6560 - val_loss: 29.3783\n",
      "Epoch 946/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.9343 - val_loss: 29.8707\n",
      "Epoch 947/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.7323 - val_loss: 29.2486\n",
      "Epoch 948/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.8668 - val_loss: 29.4112\n",
      "Epoch 949/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.0992 - val_loss: 29.6034\n",
      "Epoch 950/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.9349 - val_loss: 29.3248\n",
      "Epoch 951/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.9828 - val_loss: 29.5068\n",
      "Epoch 952/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.9258 - val_loss: 28.7864\n",
      "Epoch 953/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.6828 - val_loss: 29.3934\n",
      "Epoch 954/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 25.8696 - val_loss: 29.9164\n",
      "Epoch 955/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.0346 - val_loss: 30.5905\n",
      "Epoch 956/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.1762 - val_loss: 29.8286\n",
      "Epoch 957/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.9225 - val_loss: 29.2898\n",
      "Epoch 958/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.7798 - val_loss: 29.8923\n",
      "Epoch 959/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.7935 - val_loss: 29.1590\n",
      "Epoch 960/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.7220 - val_loss: 30.4614\n",
      "Epoch 961/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.8948 - val_loss: 29.5022\n",
      "Epoch 962/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.8258 - val_loss: 29.4223\n",
      "Epoch 963/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.6664 - val_loss: 29.6489\n",
      "Epoch 964/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.7430 - val_loss: 29.3390\n",
      "Epoch 965/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 25.6103 - val_loss: 29.2077\n",
      "Epoch 966/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.5148 - val_loss: 29.1431\n",
      "Epoch 967/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.7312 - val_loss: 29.6795\n",
      "Epoch 968/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.6264 - val_loss: 29.3336\n",
      "Epoch 969/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.6166 - val_loss: 29.7475\n",
      "Epoch 970/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.7894 - val_loss: 28.8915\n",
      "Epoch 971/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.7831 - val_loss: 29.3283\n",
      "Epoch 972/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.6486 - val_loss: 29.1217\n",
      "Epoch 973/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.7049 - val_loss: 29.5794\n",
      "Epoch 974/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.6514 - val_loss: 30.0638\n",
      "Epoch 975/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.9101 - val_loss: 30.2008\n",
      "Epoch 976/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.8036 - val_loss: 29.5228\n",
      "Epoch 977/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.1736 - val_loss: 30.0595\n",
      "Epoch 978/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.7059 - val_loss: 29.1780\n",
      "Epoch 979/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.6161 - val_loss: 29.1099\n",
      "Epoch 980/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 25.5048 - val_loss: 29.6932\n",
      "Epoch 981/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 25.4391 - val_loss: 29.0053\n",
      "Epoch 982/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 25.4098 - val_loss: 29.4585\n",
      "Epoch 983/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.9030 - val_loss: 29.2569\n",
      "Epoch 984/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.8186 - val_loss: 29.3163\n",
      "Epoch 985/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.5373 - val_loss: 29.7603\n",
      "Epoch 986/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.8298 - val_loss: 28.8828\n",
      "Epoch 987/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.7780 - val_loss: 29.4426\n",
      "Epoch 988/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.0273 - val_loss: 29.6658\n",
      "Epoch 989/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.8246 - val_loss: 29.1253\n",
      "Epoch 990/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.6917 - val_loss: 28.6470\n",
      "Epoch 991/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.5971 - val_loss: 29.2442\n",
      "Epoch 992/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.5436 - val_loss: 28.5766\n",
      "Epoch 993/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4737 - val_loss: 28.7422\n",
      "Epoch 994/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.8453 - val_loss: 29.0655\n",
      "Epoch 995/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.1910 - val_loss: 29.4432\n",
      "Epoch 996/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5715 - val_loss: 28.8742\n",
      "Epoch 997/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.7803 - val_loss: 29.8830\n",
      "Epoch 998/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.0491 - val_loss: 31.2229\n",
      "Epoch 999/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.3927 - val_loss: 30.6036\n",
      "Epoch 1000/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 25.6805\n",
      "Epoch 01000: saving model to saved_models/latent16/cp-1000.h5\n",
      "7/7 [==============================] - 1s 148ms/step - loss: 25.6805 - val_loss: 28.7418\n",
      "Epoch 1001/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.7081 - val_loss: 28.9192\n",
      "Epoch 1002/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.5729 - val_loss: 29.6340\n",
      "Epoch 1003/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.7462 - val_loss: 29.4698\n",
      "Epoch 1004/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.6427 - val_loss: 29.4510\n",
      "Epoch 1005/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.7593 - val_loss: 28.8222\n",
      "Epoch 1006/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.7776 - val_loss: 29.3055\n",
      "Epoch 1007/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.7937 - val_loss: 29.4891\n",
      "Epoch 1008/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.9523 - val_loss: 29.9271\n",
      "Epoch 1009/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.7941 - val_loss: 29.8893\n",
      "Epoch 1010/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.4609 - val_loss: 29.1022\n",
      "Epoch 1011/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.9006 - val_loss: 28.7138\n",
      "Epoch 1012/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.7312 - val_loss: 30.0512\n",
      "Epoch 1013/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.7058 - val_loss: 29.4731\n",
      "Epoch 1014/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.4173 - val_loss: 29.1613\n",
      "Epoch 1015/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.5558 - val_loss: 29.0118\n",
      "Epoch 1016/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.4456 - val_loss: 29.0606\n",
      "Epoch 1017/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.6046 - val_loss: 29.9907\n",
      "Epoch 1018/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.7087 - val_loss: 29.5123\n",
      "Epoch 1019/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.6045 - val_loss: 30.0916\n",
      "Epoch 1020/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.6405 - val_loss: 29.2714\n",
      "Epoch 1021/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.7410 - val_loss: 29.4622\n",
      "Epoch 1022/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.6001 - val_loss: 29.6087\n",
      "Epoch 1023/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.6347 - val_loss: 29.2543\n",
      "Epoch 1024/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5793 - val_loss: 29.0675\n",
      "Epoch 1025/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.6336 - val_loss: 29.7120\n",
      "Epoch 1026/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.5374 - val_loss: 29.2270\n",
      "Epoch 1027/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.4446 - val_loss: 29.1111\n",
      "Epoch 1028/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.5708 - val_loss: 29.7535\n",
      "Epoch 1029/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4974 - val_loss: 29.0664\n",
      "Epoch 1030/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.5338 - val_loss: 29.7475\n",
      "Epoch 1031/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5736 - val_loss: 29.2989\n",
      "Epoch 1032/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.6329 - val_loss: 29.2274\n",
      "Epoch 1033/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5310 - val_loss: 29.5773\n",
      "Epoch 1034/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.6780 - val_loss: 30.1061\n",
      "Epoch 1035/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.4711 - val_loss: 30.0572\n",
      "Epoch 1036/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.9299 - val_loss: 28.9695\n",
      "Epoch 1037/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.5770 - val_loss: 29.5303\n",
      "Epoch 1038/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.4564 - val_loss: 29.6652\n",
      "Epoch 1039/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5929 - val_loss: 30.1175\n",
      "Epoch 1040/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.5717 - val_loss: 29.2528\n",
      "Epoch 1041/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.6812 - val_loss: 29.3736\n",
      "Epoch 1042/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.8353 - val_loss: 29.6541\n",
      "Epoch 1043/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.9392 - val_loss: 29.9939\n",
      "Epoch 1044/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.6977 - val_loss: 29.4879\n",
      "Epoch 1045/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.6780 - val_loss: 29.8602\n",
      "Epoch 1046/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.6206 - val_loss: 29.5312\n",
      "Epoch 1047/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.4185 - val_loss: 29.3393\n",
      "Epoch 1048/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.7620 - val_loss: 30.5249\n",
      "Epoch 1049/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.2066 - val_loss: 30.4498\n",
      "Epoch 1050/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.9329 - val_loss: 29.8576\n",
      "Epoch 1051/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.9886 - val_loss: 29.8962\n",
      "Epoch 1052/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.0235 - val_loss: 30.1509\n",
      "Epoch 1053/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.7536 - val_loss: 29.5545\n",
      "Epoch 1054/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.8855 - val_loss: 30.6406\n",
      "Epoch 1055/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.9112 - val_loss: 29.6645\n",
      "Epoch 1056/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.0475 - val_loss: 29.1538\n",
      "Epoch 1057/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5100 - val_loss: 29.3311\n",
      "Epoch 1058/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.4488 - val_loss: 30.0520\n",
      "Epoch 1059/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.8331 - val_loss: 29.7646\n",
      "Epoch 1060/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5824 - val_loss: 28.6247\n",
      "Epoch 1061/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.3175 - val_loss: 29.0656\n",
      "Epoch 1062/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3255 - val_loss: 30.1097\n",
      "Epoch 1063/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3865 - val_loss: 29.5562\n",
      "Epoch 1064/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.4778 - val_loss: 29.8327\n",
      "Epoch 1065/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.7431 - val_loss: 29.3525\n",
      "Epoch 1066/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.4395 - val_loss: 29.2101\n",
      "Epoch 1067/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3602 - val_loss: 29.0204\n",
      "Epoch 1068/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.7328 - val_loss: 30.0780\n",
      "Epoch 1069/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5950 - val_loss: 30.1169\n",
      "Epoch 1070/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.7186 - val_loss: 30.2905\n",
      "Epoch 1071/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.4924 - val_loss: 29.1530\n",
      "Epoch 1072/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4219 - val_loss: 29.2642\n",
      "Epoch 1073/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4699 - val_loss: 30.3383\n",
      "Epoch 1074/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5790 - val_loss: 29.3693\n",
      "Epoch 1075/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.4809 - val_loss: 29.6693\n",
      "Epoch 1076/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4298 - val_loss: 29.0249\n",
      "Epoch 1077/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.5065 - val_loss: 29.4727\n",
      "Epoch 1078/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.5724 - val_loss: 29.5888\n",
      "Epoch 1079/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.6310 - val_loss: 29.0657\n",
      "Epoch 1080/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.7679 - val_loss: 29.6261\n",
      "Epoch 1081/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.8731 - val_loss: 29.3095\n",
      "Epoch 1082/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.0956 - val_loss: 29.1611\n",
      "Epoch 1083/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.8743 - val_loss: 30.8248\n",
      "Epoch 1084/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.9217 - val_loss: 30.2786\n",
      "Epoch 1085/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5395 - val_loss: 29.9720\n",
      "Epoch 1086/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.6534 - val_loss: 30.3226\n",
      "Epoch 1087/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.8929 - val_loss: 29.9225\n",
      "Epoch 1088/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.6222 - val_loss: 29.4381\n",
      "Epoch 1089/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.7542 - val_loss: 30.2003\n",
      "Epoch 1090/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.6458 - val_loss: 29.5937\n",
      "Epoch 1091/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.3420 - val_loss: 29.4010\n",
      "Epoch 1092/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.3285 - val_loss: 29.0977\n",
      "Epoch 1093/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.3809 - val_loss: 29.1851\n",
      "Epoch 1094/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5907 - val_loss: 28.9542\n",
      "Epoch 1095/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.4476 - val_loss: 29.1799\n",
      "Epoch 1096/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.2919 - val_loss: 29.1558\n",
      "Epoch 1097/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4893 - val_loss: 29.0667\n",
      "Epoch 1098/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.2295 - val_loss: 29.7926\n",
      "Epoch 1099/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.4561 - val_loss: 29.5768\n",
      "Epoch 1100/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.6643 - val_loss: 29.9870\n",
      "Epoch 1101/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5411 - val_loss: 30.1333\n",
      "Epoch 1102/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.7186 - val_loss: 30.2891\n",
      "Epoch 1103/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.7516 - val_loss: 29.3592\n",
      "Epoch 1104/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5888 - val_loss: 28.9364\n",
      "Epoch 1105/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3459 - val_loss: 29.4405\n",
      "Epoch 1106/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5117 - val_loss: 30.0452\n",
      "Epoch 1107/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.8226 - val_loss: 29.8752\n",
      "Epoch 1108/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.7495 - val_loss: 29.6636\n",
      "Epoch 1109/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.9240 - val_loss: 30.2437\n",
      "Epoch 1110/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.7682 - val_loss: 29.0663\n",
      "Epoch 1111/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4359 - val_loss: 28.8822\n",
      "Epoch 1112/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3218 - val_loss: 29.5724\n",
      "Epoch 1113/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.8037 - val_loss: 29.5070\n",
      "Epoch 1114/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2856 - val_loss: 28.4832\n",
      "Epoch 1115/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.3652 - val_loss: 28.8214\n",
      "Epoch 1116/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.5715 - val_loss: 30.2161\n",
      "Epoch 1117/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.4691 - val_loss: 29.6226\n",
      "Epoch 1118/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4791 - val_loss: 29.4772\n",
      "Epoch 1119/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5384 - val_loss: 30.5769\n",
      "Epoch 1120/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.8517 - val_loss: 31.1622\n",
      "Epoch 1121/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.8522 - val_loss: 31.1097\n",
      "Epoch 1122/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.8402 - val_loss: 30.3580\n",
      "Epoch 1123/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.8395 - val_loss: 29.2818\n",
      "Epoch 1124/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4852 - val_loss: 29.0499\n",
      "Epoch 1125/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4095 - val_loss: 29.1988\n",
      "Epoch 1126/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5720 - val_loss: 29.4298\n",
      "Epoch 1127/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.2703 - val_loss: 29.4451\n",
      "Epoch 1128/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4124 - val_loss: 29.5666\n",
      "Epoch 1129/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2337 - val_loss: 29.2832\n",
      "Epoch 1130/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.4463 - val_loss: 29.6705\n",
      "Epoch 1131/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.4303 - val_loss: 29.9648\n",
      "Epoch 1132/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 25.4606 - val_loss: 29.8460\n",
      "Epoch 1133/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.3164 - val_loss: 29.2775\n",
      "Epoch 1134/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.6659 - val_loss: 28.7150\n",
      "Epoch 1135/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.7371 - val_loss: 30.2564\n",
      "Epoch 1136/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.4432 - val_loss: 30.0571\n",
      "Epoch 1137/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.6196 - val_loss: 29.9136\n",
      "Epoch 1138/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5287 - val_loss: 29.6799\n",
      "Epoch 1139/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4013 - val_loss: 29.7647\n",
      "Epoch 1140/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.3446 - val_loss: 29.1118\n",
      "Epoch 1141/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.2418 - val_loss: 29.1705\n",
      "Epoch 1142/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3194 - val_loss: 29.4250\n",
      "Epoch 1143/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.4498 - val_loss: 30.0121\n",
      "Epoch 1144/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.4729 - val_loss: 28.7275\n",
      "Epoch 1145/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.5293 - val_loss: 29.5339\n",
      "Epoch 1146/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.6359 - val_loss: 29.8144\n",
      "Epoch 1147/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.3484 - val_loss: 29.3114\n",
      "Epoch 1148/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.2845 - val_loss: 30.5786\n",
      "Epoch 1149/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.4931 - val_loss: 29.9013\n",
      "Epoch 1150/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.6930 - val_loss: 28.7457\n",
      "Epoch 1151/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.2553 - val_loss: 29.4700\n",
      "Epoch 1152/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.6037 - val_loss: 30.6491\n",
      "Epoch 1153/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.6329 - val_loss: 28.9294\n",
      "Epoch 1154/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2444 - val_loss: 29.4432\n",
      "Epoch 1155/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.2130 - val_loss: 29.7510\n",
      "Epoch 1156/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4016 - val_loss: 28.7759\n",
      "Epoch 1157/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.2876 - val_loss: 29.3876\n",
      "Epoch 1158/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 25.2079 - val_loss: 29.1872\n",
      "Epoch 1159/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.3254 - val_loss: 28.9536\n",
      "Epoch 1160/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.1859 - val_loss: 28.7565\n",
      "Epoch 1161/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.2329 - val_loss: 29.4827\n",
      "Epoch 1162/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.4555 - val_loss: 28.9757\n",
      "Epoch 1163/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4191 - val_loss: 28.9631\n",
      "Epoch 1164/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.6912 - val_loss: 29.3249\n",
      "Epoch 1165/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.6844 - val_loss: 29.7410\n",
      "Epoch 1166/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4515 - val_loss: 29.1901\n",
      "Epoch 1167/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4213 - val_loss: 29.7934\n",
      "Epoch 1168/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.3904 - val_loss: 29.2997\n",
      "Epoch 1169/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 25.1248 - val_loss: 29.5242\n",
      "Epoch 1170/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.3853 - val_loss: 29.4767\n",
      "Epoch 1171/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.3317 - val_loss: 29.8058\n",
      "Epoch 1172/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.5420 - val_loss: 29.4962\n",
      "Epoch 1173/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.1509 - val_loss: 28.3193\n",
      "Epoch 1174/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4550 - val_loss: 29.6528\n",
      "Epoch 1175/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4109 - val_loss: 29.5910\n",
      "Epoch 1176/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.3962 - val_loss: 29.4166\n",
      "Epoch 1177/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5899 - val_loss: 29.0260\n",
      "Epoch 1178/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3186 - val_loss: 29.6836\n",
      "Epoch 1179/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4520 - val_loss: 30.0064\n",
      "Epoch 1180/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.4289 - val_loss: 29.5425\n",
      "Epoch 1181/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.5093 - val_loss: 29.8301\n",
      "Epoch 1182/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5008 - val_loss: 29.2151\n",
      "Epoch 1183/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.3175 - val_loss: 29.0576\n",
      "Epoch 1184/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.2815 - val_loss: 29.0914\n",
      "Epoch 1185/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.2731 - val_loss: 29.8077\n",
      "Epoch 1186/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.2649 - val_loss: 28.8949\n",
      "Epoch 1187/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4507 - val_loss: 29.9426\n",
      "Epoch 1188/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4303 - val_loss: 29.7025\n",
      "Epoch 1189/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5707 - val_loss: 29.4158\n",
      "Epoch 1190/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.5429 - val_loss: 29.2460\n",
      "Epoch 1191/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.7228 - val_loss: 29.9220\n",
      "Epoch 1192/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.5046 - val_loss: 29.8654\n",
      "Epoch 1193/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.4828 - val_loss: 29.3762\n",
      "Epoch 1194/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.1829 - val_loss: 29.1058\n",
      "Epoch 1195/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.1363 - val_loss: 28.6900\n",
      "Epoch 1196/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.3743 - val_loss: 29.7938\n",
      "Epoch 1197/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.3156 - val_loss: 29.4962\n",
      "Epoch 1198/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2876 - val_loss: 30.1994\n",
      "Epoch 1199/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4948 - val_loss: 29.6861\n",
      "Epoch 1200/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 25.3910\n",
      "Epoch 01200: saving model to saved_models/latent16/cp-1200.h5\n",
      "7/7 [==============================] - 1s 154ms/step - loss: 25.3910 - val_loss: 29.7752\n",
      "Epoch 1201/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 25.9255 - val_loss: 30.3730\n",
      "Epoch 1202/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.1803 - val_loss: 29.8489\n",
      "Epoch 1203/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.4917 - val_loss: 30.1061\n",
      "Epoch 1204/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.2218 - val_loss: 29.4783\n",
      "Epoch 1205/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.2656 - val_loss: 29.1933\n",
      "Epoch 1206/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.2039 - val_loss: 29.3456\n",
      "Epoch 1207/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1498 - val_loss: 29.5491\n",
      "Epoch 1208/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1263 - val_loss: 29.1663\n",
      "Epoch 1209/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 25.0826 - val_loss: 30.3379\n",
      "Epoch 1210/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.4686 - val_loss: 29.5442\n",
      "Epoch 1211/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.4285 - val_loss: 30.1977\n",
      "Epoch 1212/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5193 - val_loss: 29.1938\n",
      "Epoch 1213/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5830 - val_loss: 30.0423\n",
      "Epoch 1214/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.8933 - val_loss: 31.8554\n",
      "Epoch 1215/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.2067 - val_loss: 30.3873\n",
      "Epoch 1216/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.9902 - val_loss: 29.6844\n",
      "Epoch 1217/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3540 - val_loss: 28.9921\n",
      "Epoch 1218/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.7123 - val_loss: 30.5876\n",
      "Epoch 1219/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.5118 - val_loss: 29.2134\n",
      "Epoch 1220/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1645 - val_loss: 29.6307\n",
      "Epoch 1221/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1788 - val_loss: 29.7257\n",
      "Epoch 1222/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1666 - val_loss: 29.6447\n",
      "Epoch 1223/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2836 - val_loss: 29.5509\n",
      "Epoch 1224/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.1196 - val_loss: 28.7342\n",
      "Epoch 1225/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1283 - val_loss: 29.4456\n",
      "Epoch 1226/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2565 - val_loss: 28.9542\n",
      "Epoch 1227/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1592 - val_loss: 29.8304\n",
      "Epoch 1228/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3668 - val_loss: 30.1163\n",
      "Epoch 1229/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.6627 - val_loss: 30.8804\n",
      "Epoch 1230/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.5871 - val_loss: 29.6690\n",
      "Epoch 1231/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1775 - val_loss: 29.0294\n",
      "Epoch 1232/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.0310 - val_loss: 29.0400\n",
      "Epoch 1233/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.9935 - val_loss: 29.6767\n",
      "Epoch 1234/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.2504 - val_loss: 29.8976\n",
      "Epoch 1235/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0213 - val_loss: 29.4307\n",
      "Epoch 1236/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.1267 - val_loss: 28.9807\n",
      "Epoch 1237/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.4284 - val_loss: 29.4696\n",
      "Epoch 1238/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.2154 - val_loss: 29.8567\n",
      "Epoch 1239/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.2786 - val_loss: 28.7860\n",
      "Epoch 1240/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1577 - val_loss: 29.4761\n",
      "Epoch 1241/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.9676 - val_loss: 29.7862\n",
      "Epoch 1242/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.1765 - val_loss: 28.8636\n",
      "Epoch 1243/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1232 - val_loss: 29.6973\n",
      "Epoch 1244/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4504 - val_loss: 30.3984\n",
      "Epoch 1245/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.6901 - val_loss: 29.0850\n",
      "Epoch 1246/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2485 - val_loss: 29.5407\n",
      "Epoch 1247/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.2324 - val_loss: 28.9338\n",
      "Epoch 1248/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.2323 - val_loss: 28.9307\n",
      "Epoch 1249/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1368 - val_loss: 28.9874\n",
      "Epoch 1250/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1789 - val_loss: 29.4659\n",
      "Epoch 1251/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.1109 - val_loss: 28.9356\n",
      "Epoch 1252/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2524 - val_loss: 29.4079\n",
      "Epoch 1253/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.2264 - val_loss: 29.3511\n",
      "Epoch 1254/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4048 - val_loss: 29.9260\n",
      "Epoch 1255/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.3695 - val_loss: 30.4940\n",
      "Epoch 1256/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.6155 - val_loss: 29.7987\n",
      "Epoch 1257/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.3881 - val_loss: 29.0477\n",
      "Epoch 1258/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.6537 - val_loss: 29.6295\n",
      "Epoch 1259/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0558 - val_loss: 28.7683\n",
      "Epoch 1260/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.1991 - val_loss: 29.2115\n",
      "Epoch 1261/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.4133 - val_loss: 30.3513\n",
      "Epoch 1262/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3102 - val_loss: 29.9447\n",
      "Epoch 1263/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3340 - val_loss: 29.4300\n",
      "Epoch 1264/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0767 - val_loss: 29.2993\n",
      "Epoch 1265/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 24.9666 - val_loss: 29.7994\n",
      "Epoch 1266/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1425 - val_loss: 29.5263\n",
      "Epoch 1267/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2191 - val_loss: 29.7458\n",
      "Epoch 1268/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3972 - val_loss: 29.4762\n",
      "Epoch 1269/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2598 - val_loss: 30.2662\n",
      "Epoch 1270/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.3290 - val_loss: 29.7411\n",
      "Epoch 1271/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.0460 - val_loss: 29.5391\n",
      "Epoch 1272/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4048 - val_loss: 30.1432\n",
      "Epoch 1273/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4349 - val_loss: 29.2361\n",
      "Epoch 1274/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1802 - val_loss: 29.7786\n",
      "Epoch 1275/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1840 - val_loss: 29.3968\n",
      "Epoch 1276/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.1338 - val_loss: 28.7753\n",
      "Epoch 1277/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0641 - val_loss: 29.5842\n",
      "Epoch 1278/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.0190 - val_loss: 29.3425\n",
      "Epoch 1279/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.1150 - val_loss: 29.3384\n",
      "Epoch 1280/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9744 - val_loss: 29.1546\n",
      "Epoch 1281/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1453 - val_loss: 28.8406\n",
      "Epoch 1282/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.6600 - val_loss: 29.4288\n",
      "Epoch 1283/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.2031 - val_loss: 29.2037\n",
      "Epoch 1284/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.6557 - val_loss: 30.1787\n",
      "Epoch 1285/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.6926 - val_loss: 29.7619\n",
      "Epoch 1286/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2601 - val_loss: 29.6778\n",
      "Epoch 1287/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.9503 - val_loss: 29.5897\n",
      "Epoch 1288/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.0197 - val_loss: 29.4174\n",
      "Epoch 1289/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0496 - val_loss: 29.8428\n",
      "Epoch 1290/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.8448 - val_loss: 29.2997\n",
      "Epoch 1291/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.0665 - val_loss: 28.9209\n",
      "Epoch 1292/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0746 - val_loss: 29.8268\n",
      "Epoch 1293/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2107 - val_loss: 28.9393\n",
      "Epoch 1294/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0178 - val_loss: 29.8667\n",
      "Epoch 1295/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1360 - val_loss: 28.8413\n",
      "Epoch 1296/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9427 - val_loss: 28.9266\n",
      "Epoch 1297/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9641 - val_loss: 28.9972\n",
      "Epoch 1298/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0575 - val_loss: 29.2160\n",
      "Epoch 1299/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.1243 - val_loss: 30.2089\n",
      "Epoch 1300/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4314 - val_loss: 30.2716\n",
      "Epoch 1301/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4441 - val_loss: 29.3934\n",
      "Epoch 1302/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2933 - val_loss: 28.5124\n",
      "Epoch 1303/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.3929 - val_loss: 29.6563\n",
      "Epoch 1304/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.9189 - val_loss: 29.5290\n",
      "Epoch 1305/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.2417 - val_loss: 29.5570\n",
      "Epoch 1306/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.1720 - val_loss: 29.2036\n",
      "Epoch 1307/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0656 - val_loss: 29.0787\n",
      "Epoch 1308/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3209 - val_loss: 29.5120\n",
      "Epoch 1309/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1657 - val_loss: 30.0869\n",
      "Epoch 1310/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.3030 - val_loss: 28.5085\n",
      "Epoch 1311/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.2359 - val_loss: 30.2607\n",
      "Epoch 1312/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.2319 - val_loss: 29.3277\n",
      "Epoch 1313/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2221 - val_loss: 29.2197\n",
      "Epoch 1314/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4099 - val_loss: 29.4257\n",
      "Epoch 1315/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.7411 - val_loss: 31.2880\n",
      "Epoch 1316/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 26.0200 - val_loss: 30.0252\n",
      "Epoch 1317/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.4662 - val_loss: 30.4200\n",
      "Epoch 1318/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.2323 - val_loss: 29.0422\n",
      "Epoch 1319/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.0206 - val_loss: 28.9567\n",
      "Epoch 1320/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0758 - val_loss: 28.9584\n",
      "Epoch 1321/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.0063 - val_loss: 29.1567\n",
      "Epoch 1322/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.1781 - val_loss: 28.9908\n",
      "Epoch 1323/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.0158 - val_loss: 29.2171\n",
      "Epoch 1324/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.1176 - val_loss: 28.5664\n",
      "Epoch 1325/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.1086 - val_loss: 29.5044\n",
      "Epoch 1326/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1847 - val_loss: 29.1519\n",
      "Epoch 1327/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.1223 - val_loss: 29.5055\n",
      "Epoch 1328/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.0377 - val_loss: 29.0025\n",
      "Epoch 1329/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0547 - val_loss: 29.7306\n",
      "Epoch 1330/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3848 - val_loss: 29.1663\n",
      "Epoch 1331/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3693 - val_loss: 28.9121\n",
      "Epoch 1332/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.5504 - val_loss: 30.3539\n",
      "Epoch 1333/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3633 - val_loss: 29.1873\n",
      "Epoch 1334/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1101 - val_loss: 29.7023\n",
      "Epoch 1335/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.1632 - val_loss: 29.8722\n",
      "Epoch 1336/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1934 - val_loss: 29.2633\n",
      "Epoch 1337/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1046 - val_loss: 28.9885\n",
      "Epoch 1338/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1793 - val_loss: 29.2481\n",
      "Epoch 1339/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 25.0331 - val_loss: 29.7849\n",
      "Epoch 1340/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1158 - val_loss: 28.8520\n",
      "Epoch 1341/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.3235 - val_loss: 29.1749\n",
      "Epoch 1342/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1311 - val_loss: 29.3091\n",
      "Epoch 1343/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2463 - val_loss: 30.1546\n",
      "Epoch 1344/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1182 - val_loss: 29.6193\n",
      "Epoch 1345/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3280 - val_loss: 29.4782\n",
      "Epoch 1346/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1617 - val_loss: 28.6675\n",
      "Epoch 1347/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1022 - val_loss: 29.0502\n",
      "Epoch 1348/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.1159 - val_loss: 29.5280\n",
      "Epoch 1349/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1184 - val_loss: 29.3552\n",
      "Epoch 1350/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0638 - val_loss: 29.2912\n",
      "Epoch 1351/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0008 - val_loss: 28.8150\n",
      "Epoch 1352/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9641 - val_loss: 28.8760\n",
      "Epoch 1353/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1905 - val_loss: 28.7230\n",
      "Epoch 1354/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9554 - val_loss: 29.1216\n",
      "Epoch 1355/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.0425 - val_loss: 29.5278\n",
      "Epoch 1356/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9471 - val_loss: 30.1287\n",
      "Epoch 1357/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1484 - val_loss: 29.8792\n",
      "Epoch 1358/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0093 - val_loss: 28.9571\n",
      "Epoch 1359/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.7805 - val_loss: 29.2321\n",
      "Epoch 1360/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2499 - val_loss: 30.0203\n",
      "Epoch 1361/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.2512 - val_loss: 30.0202\n",
      "Epoch 1362/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2956 - val_loss: 29.4282\n",
      "Epoch 1363/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0435 - val_loss: 30.0180\n",
      "Epoch 1364/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4101 - val_loss: 29.4889\n",
      "Epoch 1365/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.0305 - val_loss: 29.8836\n",
      "Epoch 1366/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5411 - val_loss: 29.1763\n",
      "Epoch 1367/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.0108 - val_loss: 29.8834\n",
      "Epoch 1368/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.3184 - val_loss: 29.6596\n",
      "Epoch 1369/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3696 - val_loss: 29.3215\n",
      "Epoch 1370/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.9403 - val_loss: 29.2243\n",
      "Epoch 1371/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9492 - val_loss: 30.0261\n",
      "Epoch 1372/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.2593 - val_loss: 28.8148\n",
      "Epoch 1373/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.3694 - val_loss: 29.0250\n",
      "Epoch 1374/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3971 - val_loss: 29.4606\n",
      "Epoch 1375/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3718 - val_loss: 28.5747\n",
      "Epoch 1376/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.3020 - val_loss: 30.2885\n",
      "Epoch 1377/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2681 - val_loss: 28.8931\n",
      "Epoch 1378/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9076 - val_loss: 29.6890\n",
      "Epoch 1379/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0682 - val_loss: 30.0769\n",
      "Epoch 1380/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.0602 - val_loss: 29.3358\n",
      "Epoch 1381/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.0426 - val_loss: 30.4472\n",
      "Epoch 1382/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.1242 - val_loss: 28.9868\n",
      "Epoch 1383/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.0609 - val_loss: 29.2580\n",
      "Epoch 1384/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.8497 - val_loss: 29.0154\n",
      "Epoch 1385/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.9316 - val_loss: 28.9822\n",
      "Epoch 1386/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9763 - val_loss: 28.9606\n",
      "Epoch 1387/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.0924 - val_loss: 29.0494\n",
      "Epoch 1388/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1931 - val_loss: 29.5150\n",
      "Epoch 1389/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.3702 - val_loss: 29.4240\n",
      "Epoch 1390/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.1596 - val_loss: 29.3973\n",
      "Epoch 1391/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8118 - val_loss: 28.9004\n",
      "Epoch 1392/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.8246 - val_loss: 29.2376\n",
      "Epoch 1393/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0861 - val_loss: 29.5214\n",
      "Epoch 1394/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.0084 - val_loss: 29.3826\n",
      "Epoch 1395/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.0236 - val_loss: 29.0608\n",
      "Epoch 1396/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9196 - val_loss: 29.8589\n",
      "Epoch 1397/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.2335 - val_loss: 29.6280\n",
      "Epoch 1398/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.8620 - val_loss: 28.4893\n",
      "Epoch 1399/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.9197 - val_loss: 28.8440\n",
      "Epoch 1400/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 25.0612\n",
      "Epoch 01400: saving model to saved_models/latent16/cp-1400.h5\n",
      "7/7 [==============================] - 1s 159ms/step - loss: 25.0612 - val_loss: 29.6986\n",
      "Epoch 1401/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.0535 - val_loss: 29.6436\n",
      "Epoch 1402/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.2053 - val_loss: 29.4762\n",
      "Epoch 1403/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2759 - val_loss: 29.6965\n",
      "Epoch 1404/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5206 - val_loss: 29.8710\n",
      "Epoch 1405/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0900 - val_loss: 29.8996\n",
      "Epoch 1406/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.0116 - val_loss: 28.8959\n",
      "Epoch 1407/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.9718 - val_loss: 29.1308\n",
      "Epoch 1408/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.9001 - val_loss: 29.3360\n",
      "Epoch 1409/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.8457 - val_loss: 29.1281\n",
      "Epoch 1410/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.9566 - val_loss: 29.1244\n",
      "Epoch 1411/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9577 - val_loss: 29.0377\n",
      "Epoch 1412/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.9951 - val_loss: 28.9565\n",
      "Epoch 1413/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9928 - val_loss: 28.9187\n",
      "Epoch 1414/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8360 - val_loss: 28.8908\n",
      "Epoch 1415/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 24.7111 - val_loss: 29.0243\n",
      "Epoch 1416/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9531 - val_loss: 30.5168\n",
      "Epoch 1417/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4068 - val_loss: 30.3010\n",
      "Epoch 1418/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3202 - val_loss: 29.2344\n",
      "Epoch 1419/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.2792 - val_loss: 28.8478\n",
      "Epoch 1420/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.1905 - val_loss: 29.7834\n",
      "Epoch 1421/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.3947 - val_loss: 29.2216\n",
      "Epoch 1422/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.1880 - val_loss: 29.6801\n",
      "Epoch 1423/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.2642 - val_loss: 29.3566\n",
      "Epoch 1424/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1540 - val_loss: 29.8133\n",
      "Epoch 1425/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2094 - val_loss: 30.2455\n",
      "Epoch 1426/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1990 - val_loss: 28.9101\n",
      "Epoch 1427/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9986 - val_loss: 29.8059\n",
      "Epoch 1428/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0301 - val_loss: 29.9404\n",
      "Epoch 1429/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.8356 - val_loss: 29.8930\n",
      "Epoch 1430/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1520 - val_loss: 29.3790\n",
      "Epoch 1431/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.9792 - val_loss: 29.5789\n",
      "Epoch 1432/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0797 - val_loss: 29.0280\n",
      "Epoch 1433/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.1459 - val_loss: 29.6054\n",
      "Epoch 1434/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.0165 - val_loss: 29.1593\n",
      "Epoch 1435/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.4471 - val_loss: 31.2555\n",
      "Epoch 1436/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.6805 - val_loss: 28.6126\n",
      "Epoch 1437/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0939 - val_loss: 30.2970\n",
      "Epoch 1438/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.2103 - val_loss: 29.4841\n",
      "Epoch 1439/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.7276 - val_loss: 29.6744\n",
      "Epoch 1440/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.9250 - val_loss: 29.0623\n",
      "Epoch 1441/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8588 - val_loss: 29.8085\n",
      "Epoch 1442/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9522 - val_loss: 29.5376\n",
      "Epoch 1443/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.9549 - val_loss: 29.7843\n",
      "Epoch 1444/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.9565 - val_loss: 28.8801\n",
      "Epoch 1445/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.7275 - val_loss: 29.2353\n",
      "Epoch 1446/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.7706 - val_loss: 28.9299\n",
      "Epoch 1447/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.7907 - val_loss: 29.7348\n",
      "Epoch 1448/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.7554 - val_loss: 29.6087\n",
      "Epoch 1449/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0479 - val_loss: 29.7759\n",
      "Epoch 1450/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0661 - val_loss: 29.8065\n",
      "Epoch 1451/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9156 - val_loss: 29.9387\n",
      "Epoch 1452/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9393 - val_loss: 29.1844\n",
      "Epoch 1453/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8705 - val_loss: 29.0673\n",
      "Epoch 1454/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9467 - val_loss: 29.5222\n",
      "Epoch 1455/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1977 - val_loss: 29.3394\n",
      "Epoch 1456/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.2284 - val_loss: 29.5509\n",
      "Epoch 1457/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0626 - val_loss: 29.7477\n",
      "Epoch 1458/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8870 - val_loss: 29.7527\n",
      "Epoch 1459/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9688 - val_loss: 29.5567\n",
      "Epoch 1460/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1159 - val_loss: 29.3881\n",
      "Epoch 1461/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.1158 - val_loss: 29.7661\n",
      "Epoch 1462/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1347 - val_loss: 28.8242\n",
      "Epoch 1463/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.7782 - val_loss: 29.0547\n",
      "Epoch 1464/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.7775 - val_loss: 29.3939\n",
      "Epoch 1465/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.9159 - val_loss: 28.5907\n",
      "Epoch 1466/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9265 - val_loss: 28.3977\n",
      "Epoch 1467/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8734 - val_loss: 28.8125\n",
      "Epoch 1468/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8657 - val_loss: 29.3740\n",
      "Epoch 1469/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8524 - val_loss: 29.1977\n",
      "Epoch 1470/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.0514 - val_loss: 29.8226\n",
      "Epoch 1471/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.1073 - val_loss: 29.4774\n",
      "Epoch 1472/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.9942 - val_loss: 29.5738\n",
      "Epoch 1473/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0014 - val_loss: 30.2756\n",
      "Epoch 1474/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.8895 - val_loss: 28.8108\n",
      "Epoch 1475/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9988 - val_loss: 29.5649\n",
      "Epoch 1476/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9046 - val_loss: 29.4767\n",
      "Epoch 1477/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9502 - val_loss: 29.2406\n",
      "Epoch 1478/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.9377 - val_loss: 28.9070\n",
      "Epoch 1479/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.8717 - val_loss: 29.3148\n",
      "Epoch 1480/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.7921 - val_loss: 29.4858\n",
      "Epoch 1481/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8059 - val_loss: 29.6902\n",
      "Epoch 1482/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.8980 - val_loss: 29.8617\n",
      "Epoch 1483/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.7673 - val_loss: 29.6851\n",
      "Epoch 1484/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9490 - val_loss: 28.7456\n",
      "Epoch 1485/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8572 - val_loss: 29.3313\n",
      "Epoch 1486/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8725 - val_loss: 30.4088\n",
      "Epoch 1487/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9493 - val_loss: 30.1065\n",
      "Epoch 1488/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3462 - val_loss: 30.9261\n",
      "Epoch 1489/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 26.0389 - val_loss: 29.9400\n",
      "Epoch 1490/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3834 - val_loss: 30.5138\n",
      "Epoch 1491/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.3278 - val_loss: 28.6869\n",
      "Epoch 1492/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4062 - val_loss: 29.7907\n",
      "Epoch 1493/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.9770 - val_loss: 29.2793\n",
      "Epoch 1494/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9531 - val_loss: 29.4371\n",
      "Epoch 1495/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 24.6600 - val_loss: 29.8370\n",
      "Epoch 1496/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.9983 - val_loss: 29.5596\n",
      "Epoch 1497/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.7194 - val_loss: 29.5054\n",
      "Epoch 1498/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.8106 - val_loss: 29.0594\n",
      "Epoch 1499/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.8947 - val_loss: 29.4433\n",
      "Epoch 1500/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.8931 - val_loss: 28.5469\n",
      "Epoch 1501/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.0512 - val_loss: 29.4354\n",
      "Epoch 1502/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.7302 - val_loss: 29.3878\n",
      "Epoch 1503/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.7483 - val_loss: 30.0150\n",
      "Epoch 1504/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9239 - val_loss: 29.3043\n",
      "Epoch 1505/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0393 - val_loss: 29.5812\n",
      "Epoch 1506/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.7784 - val_loss: 29.2123\n",
      "Epoch 1507/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.7266 - val_loss: 29.4289\n",
      "Epoch 1508/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.7534 - val_loss: 29.8577\n",
      "Epoch 1509/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9616 - val_loss: 29.5846\n",
      "Epoch 1510/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0721 - val_loss: 29.6494\n",
      "Epoch 1511/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.5463 - val_loss: 29.9815\n",
      "Epoch 1512/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.1743 - val_loss: 29.3842\n",
      "Epoch 1513/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.2245 - val_loss: 29.7273\n",
      "Epoch 1514/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.5349 - val_loss: 29.5457\n",
      "Epoch 1515/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1423 - val_loss: 29.4177\n",
      "Epoch 1516/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1018 - val_loss: 29.3381\n",
      "Epoch 1517/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8640 - val_loss: 29.4744\n",
      "Epoch 1518/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8441 - val_loss: 29.9408\n",
      "Epoch 1519/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8342 - val_loss: 29.0907\n",
      "Epoch 1520/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9625 - val_loss: 29.5476\n",
      "Epoch 1521/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0243 - val_loss: 30.0217\n",
      "Epoch 1522/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1598 - val_loss: 30.6229\n",
      "Epoch 1523/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.0276 - val_loss: 29.7651\n",
      "Epoch 1524/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.7423 - val_loss: 29.1136\n",
      "Epoch 1525/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 24.6101 - val_loss: 29.7125\n",
      "Epoch 1526/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.7578 - val_loss: 29.7985\n",
      "Epoch 1527/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.9847 - val_loss: 30.5597\n",
      "Epoch 1528/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.1383 - val_loss: 29.7761\n",
      "Epoch 1529/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1350 - val_loss: 29.1728\n",
      "Epoch 1530/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9559 - val_loss: 29.6414\n",
      "Epoch 1531/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8641 - val_loss: 29.8214\n",
      "Epoch 1532/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.6248 - val_loss: 29.9877\n",
      "Epoch 1533/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.7907 - val_loss: 29.4605\n",
      "Epoch 1534/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.7275 - val_loss: 28.5842\n",
      "Epoch 1535/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9076 - val_loss: 29.7519\n",
      "Epoch 1536/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0232 - val_loss: 28.7875\n",
      "Epoch 1537/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8387 - val_loss: 29.1235\n",
      "Epoch 1538/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2124 - val_loss: 29.5707\n",
      "Epoch 1539/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0872 - val_loss: 29.1007\n",
      "Epoch 1540/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3075 - val_loss: 30.1580\n",
      "Epoch 1541/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8886 - val_loss: 29.2263\n",
      "Epoch 1542/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.8157 - val_loss: 29.3891\n",
      "Epoch 1543/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.8409 - val_loss: 28.9954\n",
      "Epoch 1544/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8073 - val_loss: 29.7490\n",
      "Epoch 1545/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9271 - val_loss: 29.4468\n",
      "Epoch 1546/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0019 - val_loss: 30.3220\n",
      "Epoch 1547/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8687 - val_loss: 30.1115\n",
      "Epoch 1548/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.9463 - val_loss: 29.5523\n",
      "Epoch 1549/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.7815 - val_loss: 29.9368\n",
      "Epoch 1550/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.7291 - val_loss: 28.9203\n",
      "Epoch 1551/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8953 - val_loss: 29.4672\n",
      "Epoch 1552/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8011 - val_loss: 29.7860\n",
      "Epoch 1553/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.7852 - val_loss: 29.2149\n",
      "Epoch 1554/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.9652 - val_loss: 28.8046\n",
      "Epoch 1555/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.7768 - val_loss: 29.3085\n",
      "Epoch 1556/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8051 - val_loss: 30.1191\n",
      "Epoch 1557/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8108 - val_loss: 29.6266\n",
      "Epoch 1558/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1576 - val_loss: 29.9578\n",
      "Epoch 1559/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9856 - val_loss: 28.9143\n",
      "Epoch 1560/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.1251 - val_loss: 29.2190\n",
      "Epoch 1561/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.8592 - val_loss: 29.9986\n",
      "Epoch 1562/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.8566 - val_loss: 30.0232\n",
      "Epoch 1563/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.8786 - val_loss: 29.7934\n",
      "Epoch 1564/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.7147 - val_loss: 28.6199\n",
      "Epoch 1565/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.8566 - val_loss: 29.7496\n",
      "Epoch 1566/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9985 - val_loss: 28.6730\n",
      "Epoch 1567/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.8740 - val_loss: 29.2610\n",
      "Epoch 1568/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8658 - val_loss: 29.5516\n",
      "Epoch 1569/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8254 - val_loss: 28.7422\n",
      "Epoch 1570/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.6332 - val_loss: 29.1451\n",
      "Epoch 1571/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8039 - val_loss: 29.1325\n",
      "Epoch 1572/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.7995 - val_loss: 29.3100\n",
      "Epoch 1573/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.0903 - val_loss: 29.5990\n",
      "Epoch 1574/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8343 - val_loss: 29.1427\n",
      "Epoch 1575/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9190 - val_loss: 30.1591\n",
      "Epoch 1576/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.5578 - val_loss: 31.6942\n",
      "Epoch 1577/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4298 - val_loss: 29.9105\n",
      "Epoch 1578/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.1211 - val_loss: 29.0537\n",
      "Epoch 1579/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8542 - val_loss: 29.1549\n",
      "Epoch 1580/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.8647 - val_loss: 28.7322\n",
      "Epoch 1581/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.7892 - val_loss: 30.2469\n",
      "Epoch 1582/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8623 - val_loss: 30.0524\n",
      "Epoch 1583/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8506 - val_loss: 29.4757\n",
      "Epoch 1584/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2540 - val_loss: 30.1835\n",
      "Epoch 1585/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.0145 - val_loss: 29.0168\n",
      "Epoch 1586/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 24.6000 - val_loss: 30.0309\n",
      "Epoch 1587/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.6016 - val_loss: 29.3316\n",
      "Epoch 1588/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.7059 - val_loss: 29.0493\n",
      "Epoch 1589/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9709 - val_loss: 30.0492\n",
      "Epoch 1590/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8141 - val_loss: 28.8906\n",
      "Epoch 1591/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.7828 - val_loss: 29.1525\n",
      "Epoch 1592/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.8331 - val_loss: 29.4651\n",
      "Epoch 1593/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9363 - val_loss: 29.7979\n",
      "Epoch 1594/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8707 - val_loss: 29.2352\n",
      "Epoch 1595/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.7870 - val_loss: 29.5614\n",
      "Epoch 1596/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.7670 - val_loss: 28.4685\n",
      "Epoch 1597/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.6371 - val_loss: 28.8256\n",
      "Epoch 1598/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.7179 - val_loss: 29.5763\n",
      "Epoch 1599/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.7740 - val_loss: 29.3467\n",
      "Epoch 1600/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 24.8392\n",
      "Epoch 01600: saving model to saved_models/latent16/cp-1600.h5\n",
      "7/7 [==============================] - 1s 159ms/step - loss: 24.8392 - val_loss: 29.4542\n",
      "Epoch 1601/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.6961 - val_loss: 29.1134\n",
      "Epoch 1602/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8984 - val_loss: 30.9110\n",
      "Epoch 1603/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2260 - val_loss: 28.9656\n",
      "Epoch 1604/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9377 - val_loss: 29.4057\n",
      "Epoch 1605/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.9292 - val_loss: 29.7350\n",
      "Epoch 1606/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9751 - val_loss: 29.3760\n",
      "Epoch 1607/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.7527 - val_loss: 29.3885\n",
      "Epoch 1608/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.9287 - val_loss: 30.2851\n",
      "Epoch 1609/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.2539 - val_loss: 29.8697\n",
      "Epoch 1610/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.8432 - val_loss: 29.1231\n",
      "Epoch 1611/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.6591 - val_loss: 28.7415\n",
      "Epoch 1612/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 24.5597 - val_loss: 28.7167\n",
      "Epoch 1613/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.5301 - val_loss: 29.6015\n",
      "Epoch 1614/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.7759 - val_loss: 29.0032\n",
      "Epoch 1615/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.6103 - val_loss: 29.5293\n",
      "Epoch 1616/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.7874 - val_loss: 29.3516\n",
      "Epoch 1617/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.6101 - val_loss: 29.1421\n",
      "Epoch 1618/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.7028 - val_loss: 29.5997\n",
      "Epoch 1619/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.5550 - val_loss: 29.8287\n",
      "Epoch 1620/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.7876 - val_loss: 29.6674\n",
      "Epoch 1621/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9157 - val_loss: 28.8855\n",
      "Epoch 1622/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.6544 - val_loss: 29.1820\n",
      "Epoch 1623/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.5901 - val_loss: 28.5729\n",
      "Epoch 1624/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.6494 - val_loss: 29.4177\n",
      "Epoch 1625/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8286 - val_loss: 29.7594\n",
      "Epoch 1626/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8125 - val_loss: 28.8464\n",
      "Epoch 1627/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.7449 - val_loss: 29.4629\n",
      "Epoch 1628/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9363 - val_loss: 30.1026\n",
      "Epoch 1629/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0544 - val_loss: 30.4061\n",
      "Epoch 1630/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.0147 - val_loss: 29.8590\n",
      "Epoch 1631/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8815 - val_loss: 29.6546\n",
      "Epoch 1632/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1338 - val_loss: 30.6372\n",
      "Epoch 1633/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.8527 - val_loss: 29.0977\n",
      "Epoch 1634/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.1634 - val_loss: 29.4368\n",
      "Epoch 1635/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.0061 - val_loss: 28.9726\n",
      "Epoch 1636/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.7268 - val_loss: 29.8872\n",
      "Epoch 1637/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.9707 - val_loss: 29.1082\n",
      "Epoch 1638/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.8464 - val_loss: 29.4672\n",
      "Epoch 1639/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.7217 - val_loss: 29.6225\n",
      "Epoch 1640/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.6075 - val_loss: 29.5833\n",
      "Epoch 1641/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.8016 - val_loss: 29.1608\n",
      "Epoch 1642/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8506 - val_loss: 29.2536\n",
      "Epoch 1643/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8937 - val_loss: 30.0819\n",
      "Epoch 1644/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.0948 - val_loss: 29.8604\n",
      "Epoch 1645/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.8762 - val_loss: 30.0121\n",
      "Epoch 1646/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1313 - val_loss: 29.5680\n",
      "Epoch 1647/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.7764 - val_loss: 30.0242\n",
      "Epoch 1648/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.7352 - val_loss: 29.4791\n",
      "Epoch 1649/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8078 - val_loss: 28.9012\n",
      "Epoch 1650/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.6895 - val_loss: 29.1723\n",
      "Epoch 1651/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8193 - val_loss: 29.5433\n",
      "Epoch 1652/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.5167 - val_loss: 29.9994\n",
      "Epoch 1653/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.7542 - val_loss: 29.1344\n",
      "Epoch 1654/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.7832 - val_loss: 28.9337\n",
      "Epoch 1655/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.7435 - val_loss: 29.6841\n",
      "Epoch 1656/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3459 - val_loss: 29.3921\n",
      "Epoch 1657/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.0169 - val_loss: 30.0893\n",
      "Epoch 1658/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9244 - val_loss: 30.1486\n",
      "Epoch 1659/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.7616 - val_loss: 29.4984\n",
      "Epoch 1660/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.7310 - val_loss: 29.3054\n",
      "Epoch 1661/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.7525 - val_loss: 28.9000\n",
      "Epoch 1662/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.8429 - val_loss: 30.2717\n",
      "Epoch 1663/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.1100 - val_loss: 29.3225\n",
      "Epoch 1664/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0222 - val_loss: 29.1737\n",
      "Epoch 1665/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9051 - val_loss: 29.2081\n",
      "Epoch 1666/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8542 - val_loss: 28.7616\n",
      "Epoch 1667/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.6976 - val_loss: 30.0433\n",
      "Epoch 1668/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.1503 - val_loss: 30.4493\n",
      "Epoch 1669/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4243 - val_loss: 29.8219\n",
      "Epoch 1670/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9741 - val_loss: 29.7798\n",
      "Epoch 1671/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0931 - val_loss: 29.9399\n",
      "Epoch 1672/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9836 - val_loss: 30.1726\n",
      "Epoch 1673/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.7564 - val_loss: 30.4355\n",
      "Epoch 1674/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.5116 - val_loss: 29.3436\n",
      "Epoch 1675/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.7020 - val_loss: 29.3411\n",
      "Epoch 1676/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.6979 - val_loss: 29.6656\n",
      "Epoch 1677/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.7843 - val_loss: 29.4153\n",
      "Epoch 1678/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.5866 - val_loss: 28.8734\n",
      "Epoch 1679/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.5815 - val_loss: 29.6310\n",
      "Epoch 1680/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.8798 - val_loss: 30.2823\n",
      "Epoch 1681/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.0153 - val_loss: 29.1910\n",
      "Epoch 1682/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.9709 - val_loss: 30.1300\n",
      "Epoch 1683/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.6683 - val_loss: 29.6635\n",
      "Epoch 1684/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.6103 - val_loss: 29.9766\n",
      "Epoch 1685/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.7575 - val_loss: 29.4025\n",
      "Epoch 1686/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.7281 - val_loss: 29.8464\n",
      "Epoch 1687/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8050 - val_loss: 29.8424\n",
      "Epoch 1688/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8862 - val_loss: 29.3350\n",
      "Epoch 1689/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.6166 - val_loss: 29.8159\n",
      "Epoch 1690/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.7365 - val_loss: 29.6616\n",
      "Epoch 1691/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8634 - val_loss: 29.1070\n",
      "Epoch 1692/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.6086 - val_loss: 29.3585\n",
      "Epoch 1693/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.7143 - val_loss: 29.5174\n",
      "Epoch 1694/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.8065 - val_loss: 29.7582\n",
      "Epoch 1695/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.7728 - val_loss: 29.5720\n",
      "Epoch 1696/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.7696 - val_loss: 29.1966\n",
      "Epoch 1697/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.6945 - val_loss: 29.8598\n",
      "Epoch 1698/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.6475 - val_loss: 29.9941\n",
      "Epoch 1699/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.6675 - val_loss: 29.4410\n",
      "Epoch 1700/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.7965 - val_loss: 30.4455\n",
      "Epoch 1701/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8890 - val_loss: 29.7330\n",
      "Epoch 1702/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.6371 - val_loss: 29.3825\n",
      "Epoch 1703/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.8382 - val_loss: 29.4567\n",
      "Epoch 1704/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.6381 - val_loss: 28.9330\n",
      "Epoch 1705/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.7392 - val_loss: 30.0334\n",
      "Epoch 1706/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9292 - val_loss: 29.2559\n",
      "Epoch 1707/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.5619 - val_loss: 29.1271\n",
      "Epoch 1708/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.8315 - val_loss: 29.1504\n",
      "Epoch 1709/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.5493 - val_loss: 29.1709\n",
      "Epoch 1710/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.5568 - val_loss: 30.5666\n",
      "Epoch 1711/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.6865 - val_loss: 29.8516\n",
      "Epoch 1712/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.6679 - val_loss: 29.6506\n",
      "Epoch 1713/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.5487 - val_loss: 28.3798\n",
      "Epoch 1714/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.5879 - val_loss: 29.8209\n",
      "Epoch 1715/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8359 - val_loss: 29.3064\n",
      "Epoch 1716/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.7350 - val_loss: 29.0197\n",
      "Epoch 1717/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.7083 - val_loss: 29.3318\n",
      "Epoch 1718/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.5994 - val_loss: 29.2760\n",
      "Epoch 1719/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.5554 - val_loss: 28.8319\n",
      "Epoch 1720/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.7115 - val_loss: 29.6208\n",
      "Epoch 1721/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.9788 - val_loss: 30.6021\n",
      "Epoch 1722/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.7907 - val_loss: 29.3550\n",
      "Epoch 1723/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.7808 - val_loss: 29.3770\n",
      "Epoch 1724/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.8831 - val_loss: 29.3248\n",
      "Epoch 1725/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.8817 - val_loss: 29.3960\n",
      "Epoch 1726/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.6191 - val_loss: 29.5455\n",
      "Epoch 1727/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.7236 - val_loss: 29.5789\n",
      "Epoch 1728/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.7802 - val_loss: 29.1180\n",
      "Epoch 1729/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.7057 - val_loss: 28.5596\n",
      "Epoch 1730/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.5807 - val_loss: 29.2975\n",
      "Epoch 1731/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.6285 - val_loss: 29.6803\n",
      "Epoch 1732/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.7095 - val_loss: 30.2504\n",
      "Epoch 1733/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.2471 - val_loss: 31.6002\n",
      "Epoch 1734/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.5462 - val_loss: 29.9960\n",
      "Epoch 1735/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.3164 - val_loss: 29.3401\n",
      "Epoch 1736/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8160 - val_loss: 29.9727\n",
      "Epoch 1737/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.5878 - val_loss: 29.2790\n",
      "Epoch 1738/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.7555 - val_loss: 29.0179\n",
      "Epoch 1739/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.7262 - val_loss: 30.0378\n",
      "Epoch 1740/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.0656 - val_loss: 29.3763\n",
      "Epoch 1741/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.0017 - val_loss: 29.2965\n",
      "Epoch 1742/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8419 - val_loss: 29.2616\n",
      "Epoch 1743/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.7562 - val_loss: 29.1622\n",
      "Epoch 1744/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.6611 - val_loss: 29.6627\n",
      "Epoch 1745/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.6807 - val_loss: 29.1841\n",
      "Epoch 1746/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.6215 - val_loss: 28.6754\n",
      "Epoch 1747/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.6395 - val_loss: 29.2900\n",
      "Epoch 1748/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.7856 - val_loss: 30.0819\n",
      "Epoch 1749/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.7097 - val_loss: 29.5795\n",
      "Epoch 1750/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.6326 - val_loss: 29.3984\n",
      "Epoch 1751/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.6024 - val_loss: 28.9882\n",
      "Epoch 1752/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.6924 - val_loss: 29.3606\n",
      "Epoch 1753/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.7105 - val_loss: 29.3721\n",
      "Epoch 1754/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.6791 - val_loss: 29.3769\n",
      "Epoch 1755/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.6121 - val_loss: 29.8054\n",
      "Epoch 1756/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.5210 - val_loss: 29.6813\n",
      "Epoch 1757/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.7228 - val_loss: 29.4470\n",
      "Epoch 1758/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.5851 - val_loss: 28.5355\n",
      "Epoch 1759/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.7378 - val_loss: 30.0304\n",
      "Epoch 1760/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 24.5022 - val_loss: 29.9870\n",
      "Epoch 1761/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.8993 - val_loss: 28.9617\n",
      "Epoch 1762/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.6775 - val_loss: 29.2689\n",
      "Epoch 1763/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.6977 - val_loss: 29.2684\n",
      "Epoch 1764/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.8458 - val_loss: 28.9728\n",
      "Epoch 1765/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.6586 - val_loss: 29.6046\n",
      "Epoch 1766/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 24.3891 - val_loss: 29.4230\n",
      "Epoch 1767/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.5476 - val_loss: 29.6635\n",
      "Epoch 1768/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.7683 - val_loss: 30.1297\n",
      "Epoch 1769/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.7156 - val_loss: 29.4939\n",
      "Epoch 1770/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.6320 - val_loss: 30.0962\n",
      "Epoch 1771/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.6562 - val_loss: 29.7865\n",
      "Epoch 1772/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.6601 - val_loss: 29.0325\n",
      "Epoch 1773/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.6349 - val_loss: 29.7781\n",
      "Epoch 1774/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8920 - val_loss: 29.9021\n",
      "Epoch 1775/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.8388 - val_loss: 29.7939\n",
      "Epoch 1776/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.7569 - val_loss: 29.2956\n",
      "Epoch 1777/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.6284 - val_loss: 29.6762\n",
      "Epoch 1778/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.5781 - val_loss: 30.2379\n",
      "Epoch 1779/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.7302 - val_loss: 29.1599\n",
      "Epoch 1780/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.6290 - val_loss: 29.6161\n",
      "Epoch 1781/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.4696 - val_loss: 29.6527\n",
      "Epoch 1782/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.5151 - val_loss: 29.5270\n",
      "Epoch 1783/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.5494 - val_loss: 29.3561\n",
      "Epoch 1784/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.6605 - val_loss: 30.0357\n",
      "Epoch 1785/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.5765 - val_loss: 29.0552\n",
      "Epoch 1786/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.6107 - val_loss: 29.3472\n",
      "Epoch 1787/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.6960 - val_loss: 29.4263\n",
      "Epoch 1788/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.6553 - val_loss: 29.6273\n",
      "Epoch 1789/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.6586 - val_loss: 29.6577\n",
      "Epoch 1790/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.6158 - val_loss: 29.5570\n",
      "Epoch 1791/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.3606 - val_loss: 29.3835\n",
      "Epoch 1792/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.6416 - val_loss: 29.4426\n",
      "Epoch 1793/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.6456 - val_loss: 29.0648\n",
      "Epoch 1794/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.5959 - val_loss: 28.8123\n",
      "Epoch 1795/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.5898 - val_loss: 29.8635\n",
      "Epoch 1796/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.6856 - val_loss: 30.2012\n",
      "Epoch 1797/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.8222 - val_loss: 31.2634\n",
      "Epoch 1798/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1523 - val_loss: 30.4807\n",
      "Epoch 1799/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.1637 - val_loss: 29.6997\n",
      "Epoch 1800/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 24.7393\n",
      "Epoch 01800: saving model to saved_models/latent16/cp-1800.h5\n",
      "7/7 [==============================] - 1s 167ms/step - loss: 24.7393 - val_loss: 30.1834\n",
      "Epoch 1801/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.9411 - val_loss: 30.4469\n",
      "Epoch 1802/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.8621 - val_loss: 30.4107\n",
      "Epoch 1803/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.9243 - val_loss: 29.9258\n",
      "Epoch 1804/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.6366 - val_loss: 30.8927\n",
      "Epoch 1805/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.3798 - val_loss: 29.5231\n",
      "Epoch 1806/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.7835 - val_loss: 30.0552\n",
      "Epoch 1807/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.6355 - val_loss: 29.3891\n",
      "Epoch 1808/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.6933 - val_loss: 29.8224\n",
      "Epoch 1809/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.5437 - val_loss: 29.9675\n",
      "Epoch 1810/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.7123 - val_loss: 29.6803\n",
      "Epoch 1811/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.8103 - val_loss: 29.4684\n",
      "Epoch 1812/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8177 - val_loss: 29.2024\n",
      "Epoch 1813/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.4872 - val_loss: 29.9418\n",
      "Epoch 1814/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.5119 - val_loss: 30.1888\n",
      "Epoch 1815/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.6662 - val_loss: 29.8593\n",
      "Epoch 1816/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.6297 - val_loss: 29.4150\n",
      "Epoch 1817/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.8582 - val_loss: 29.2714\n",
      "Epoch 1818/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.6690 - val_loss: 29.6086\n",
      "Epoch 1819/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.4321 - val_loss: 29.5328\n",
      "Epoch 1820/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.7270 - val_loss: 29.7088\n",
      "Epoch 1821/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.0692 - val_loss: 29.0015\n",
      "Epoch 1822/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.6472 - val_loss: 29.8838\n",
      "Epoch 1823/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.4244 - val_loss: 29.6542\n",
      "Epoch 1824/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.7972 - val_loss: 29.7986\n",
      "Epoch 1825/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8075 - val_loss: 29.5229\n",
      "Epoch 1826/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.6584 - val_loss: 29.8330\n",
      "Epoch 1827/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.6077 - val_loss: 29.4044\n",
      "Epoch 1828/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.4234 - val_loss: 29.1916\n",
      "Epoch 1829/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.4521 - val_loss: 29.7513\n",
      "Epoch 1830/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.5555 - val_loss: 29.2367\n",
      "Epoch 1831/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.5262 - val_loss: 30.0923\n",
      "Epoch 1832/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.6193 - val_loss: 30.8644\n",
      "Epoch 1833/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.6033 - val_loss: 29.4406\n",
      "Epoch 1834/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.5444 - val_loss: 29.7387\n",
      "Epoch 1835/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.5277 - val_loss: 28.4631\n",
      "Epoch 1836/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.3980 - val_loss: 29.6457\n",
      "Epoch 1837/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.4778 - val_loss: 29.5465\n",
      "Epoch 1838/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.4009 - val_loss: 29.5395\n",
      "Epoch 1839/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.5816 - val_loss: 29.7702\n",
      "Epoch 1840/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.6805 - val_loss: 29.8496\n",
      "Epoch 1841/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.6625 - val_loss: 30.0524\n",
      "Epoch 1842/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8921 - val_loss: 29.8411\n",
      "Epoch 1843/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.7908 - val_loss: 29.6980\n",
      "Epoch 1844/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.7543 - val_loss: 28.7862\n",
      "Epoch 1845/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.6144 - val_loss: 29.7510\n",
      "Epoch 1846/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8601 - val_loss: 29.0863\n",
      "Epoch 1847/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.7372 - val_loss: 29.8145\n",
      "Epoch 1848/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.6156 - val_loss: 30.1098\n",
      "Epoch 1849/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.6458 - val_loss: 30.0204\n",
      "Epoch 1850/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.7044 - val_loss: 29.8304\n",
      "Epoch 1851/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.6292 - val_loss: 28.8995\n",
      "Epoch 1852/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.4752 - val_loss: 29.3490\n",
      "Epoch 1853/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.6478 - val_loss: 29.4285\n",
      "Epoch 1854/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.7333 - val_loss: 29.1110\n",
      "Epoch 1855/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8432 - val_loss: 30.4043\n",
      "Epoch 1856/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.1154 - val_loss: 29.8027\n",
      "Epoch 1857/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.5288 - val_loss: 29.3203\n",
      "Epoch 1858/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.4714 - val_loss: 29.2130\n",
      "Epoch 1859/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.6132 - val_loss: 29.5799\n",
      "Epoch 1860/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.6539 - val_loss: 29.7517\n",
      "Epoch 1861/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.6976 - val_loss: 28.9391\n",
      "Epoch 1862/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.4322 - val_loss: 29.9307\n",
      "Epoch 1863/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.4412 - val_loss: 29.3984\n",
      "Epoch 1864/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.3943 - val_loss: 29.1251\n",
      "Epoch 1865/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.3749 - val_loss: 30.2129\n",
      "Epoch 1866/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.7619 - val_loss: 29.3043\n",
      "Epoch 1867/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.5697 - val_loss: 29.5079\n",
      "Epoch 1868/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.6363 - val_loss: 29.6757\n",
      "Epoch 1869/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.7122 - val_loss: 29.6126\n",
      "Epoch 1870/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.6141 - val_loss: 30.0797\n",
      "Epoch 1871/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.9990 - val_loss: 29.5164\n",
      "Epoch 1872/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8423 - val_loss: 29.6871\n",
      "Epoch 1873/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8413 - val_loss: 29.4929\n",
      "Epoch 1874/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.6137 - val_loss: 29.3593\n",
      "Epoch 1875/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.4163 - val_loss: 29.5650\n",
      "Epoch 1876/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8507 - val_loss: 29.7148\n",
      "Epoch 1877/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.4893 - val_loss: 30.4010\n",
      "Epoch 1878/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.5321 - val_loss: 29.7410\n",
      "Epoch 1879/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.4998 - val_loss: 29.8343\n",
      "Epoch 1880/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.5408 - val_loss: 29.6835\n",
      "Epoch 1881/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.6311 - val_loss: 30.7647\n",
      "Epoch 1882/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.4809 - val_loss: 29.4722\n",
      "Epoch 1883/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.6089 - val_loss: 29.4348\n",
      "Epoch 1884/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.3449 - val_loss: 29.2211\n",
      "Epoch 1885/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.4682 - val_loss: 29.7611\n",
      "Epoch 1886/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.5766 - val_loss: 29.6043\n",
      "Epoch 1887/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.8851 - val_loss: 29.9745\n",
      "Epoch 1888/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.8268 - val_loss: 29.7069\n",
      "Epoch 1889/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.4910 - val_loss: 29.9262\n",
      "Epoch 1890/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.4504 - val_loss: 29.8820\n",
      "Epoch 1891/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.4116 - val_loss: 30.8701\n",
      "Epoch 1892/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.6428 - val_loss: 29.4574\n",
      "Epoch 1893/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.5515 - val_loss: 29.9356\n",
      "Epoch 1894/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.6402 - val_loss: 30.3371\n",
      "Epoch 1895/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.5064 - val_loss: 29.0190\n",
      "Epoch 1896/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.3666 - val_loss: 28.8580\n",
      "Epoch 1897/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.4355 - val_loss: 29.3797\n",
      "Epoch 1898/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.7159 - val_loss: 29.1948\n",
      "Epoch 1899/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8865 - val_loss: 29.8216\n",
      "Epoch 1900/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.8399 - val_loss: 29.8614\n",
      "Epoch 1901/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.7995 - val_loss: 29.8151\n",
      "Epoch 1902/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8097 - val_loss: 28.9259\n",
      "Epoch 1903/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.7223 - val_loss: 29.7961\n",
      "Epoch 1904/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.4869 - val_loss: 29.3453\n",
      "Epoch 1905/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.5072 - val_loss: 29.2876\n",
      "Epoch 1906/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.5489 - val_loss: 29.4621\n",
      "Epoch 1907/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.5183 - val_loss: 29.2697\n",
      "Epoch 1908/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.7149 - val_loss: 29.7098\n",
      "Epoch 1909/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.6985 - val_loss: 30.0707\n",
      "Epoch 1910/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.6669 - val_loss: 28.7843\n",
      "Epoch 1911/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.8507 - val_loss: 30.5318\n",
      "Epoch 1912/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.7855 - val_loss: 31.4872\n",
      "Epoch 1913/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.0748 - val_loss: 29.7374\n",
      "Epoch 1914/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.8748 - val_loss: 30.8658\n",
      "Epoch 1915/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.7633 - val_loss: 29.5496\n",
      "Epoch 1916/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.7706 - val_loss: 29.0628\n",
      "Epoch 1917/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.7753 - val_loss: 29.0147\n",
      "Epoch 1918/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.6629 - val_loss: 29.7929\n",
      "Epoch 1919/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8416 - val_loss: 29.7091\n",
      "Epoch 1920/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.5857 - val_loss: 29.8819\n",
      "Epoch 1921/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.4635 - val_loss: 29.9058\n",
      "Epoch 1922/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.4504 - val_loss: 29.5109\n",
      "Epoch 1923/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.5719 - val_loss: 29.5803\n",
      "Epoch 1924/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 24.3271 - val_loss: 29.1214\n",
      "Epoch 1925/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9736 - val_loss: 31.9393\n",
      "Epoch 1926/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.5134 - val_loss: 29.6312\n",
      "Epoch 1927/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 25.1869 - val_loss: 30.0795\n",
      "Epoch 1928/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.6902 - val_loss: 29.2000\n",
      "Epoch 1929/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.5654 - val_loss: 29.6581\n",
      "Epoch 1930/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.6152 - val_loss: 29.4624\n",
      "Epoch 1931/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.4873 - val_loss: 29.3110\n",
      "Epoch 1932/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.4340 - val_loss: 29.1054\n",
      "Epoch 1933/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.6374 - val_loss: 29.2170\n",
      "Epoch 1934/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.7553 - val_loss: 29.4287\n",
      "Epoch 1935/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.5240 - val_loss: 29.2048\n",
      "Epoch 1936/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.5194 - val_loss: 29.9882\n",
      "Epoch 1937/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.4939 - val_loss: 30.7688\n",
      "Epoch 1938/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.6208 - val_loss: 29.5181\n",
      "Epoch 1939/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.6145 - val_loss: 29.4464\n",
      "Epoch 1940/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8059 - val_loss: 30.1896\n",
      "Epoch 1941/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.4505 - val_loss: 29.6600\n",
      "Epoch 1942/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.4298 - val_loss: 30.0497\n",
      "Epoch 1943/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.4828 - val_loss: 29.8569\n",
      "Epoch 1944/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.5349 - val_loss: 29.6437\n",
      "Epoch 1945/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.6091 - val_loss: 28.9789\n",
      "Epoch 1946/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.3839 - val_loss: 29.5130\n",
      "Epoch 1947/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.5602 - val_loss: 29.6997\n",
      "Epoch 1948/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.3765 - val_loss: 29.7433\n",
      "Epoch 1949/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.4260 - val_loss: 29.2690\n",
      "Epoch 1950/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.3725 - val_loss: 29.1585\n",
      "Epoch 1951/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.3748 - val_loss: 29.7508\n",
      "Epoch 1952/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 24.2689 - val_loss: 28.8794\n",
      "Epoch 1953/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.3316 - val_loss: 29.7226\n",
      "Epoch 1954/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.4554 - val_loss: 29.1288\n",
      "Epoch 1955/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.4670 - val_loss: 29.6352\n",
      "Epoch 1956/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.5517 - val_loss: 28.8101\n",
      "Epoch 1957/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.5889 - val_loss: 29.6090\n",
      "Epoch 1958/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.6119 - val_loss: 28.7838\n",
      "Epoch 1959/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.7346 - val_loss: 29.4345\n",
      "Epoch 1960/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.5203 - val_loss: 28.5093\n",
      "Epoch 1961/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.3550 - val_loss: 30.8736\n",
      "Epoch 1962/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.5866 - val_loss: 29.6110\n",
      "Epoch 1963/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.4253 - val_loss: 29.7819\n",
      "Epoch 1964/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.4065 - val_loss: 29.5711\n",
      "Epoch 1965/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.2723 - val_loss: 30.0828\n",
      "Epoch 1966/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.4406 - val_loss: 28.9823\n",
      "Epoch 1967/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.4054 - val_loss: 29.3340\n",
      "Epoch 1968/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.3995 - val_loss: 28.8123\n",
      "Epoch 1969/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.5341 - val_loss: 29.6918\n",
      "Epoch 1970/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.4178 - val_loss: 30.3600\n",
      "Epoch 1971/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.3863 - val_loss: 29.7760\n",
      "Epoch 1972/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.4544 - val_loss: 29.2249\n",
      "Epoch 1973/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.3960 - val_loss: 30.1109\n",
      "Epoch 1974/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8832 - val_loss: 29.8487\n",
      "Epoch 1975/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.9676 - val_loss: 30.3866\n",
      "Epoch 1976/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.9192 - val_loss: 29.3701\n",
      "Epoch 1977/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.5766 - val_loss: 29.5526\n",
      "Epoch 1978/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.4405 - val_loss: 29.0598\n",
      "Epoch 1979/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.5358 - val_loss: 29.2539\n",
      "Epoch 1980/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.4563 - val_loss: 29.7610\n",
      "Epoch 1981/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.4518 - val_loss: 29.5041\n",
      "Epoch 1982/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.3158 - val_loss: 30.1920\n",
      "Epoch 1983/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.5965 - val_loss: 29.2900\n",
      "Epoch 1984/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.7326 - val_loss: 30.3401\n",
      "Epoch 1985/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.5303 - val_loss: 30.1810\n",
      "Epoch 1986/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.8136 - val_loss: 29.4476\n",
      "Epoch 1987/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.7267 - val_loss: 30.4269\n",
      "Epoch 1988/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.7182 - val_loss: 29.2771\n",
      "Epoch 1989/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.5599 - val_loss: 29.9561\n",
      "Epoch 1990/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.4564 - val_loss: 29.6490\n",
      "Epoch 1991/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.5154 - val_loss: 29.5696\n",
      "Epoch 1992/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.3713 - val_loss: 29.5118\n",
      "Epoch 1993/2000\n",
      "7/7 [==============================] - 1s 106ms/step - loss: 24.2238 - val_loss: 28.7321\n",
      "Epoch 1994/2000\n",
      "7/7 [==============================] - 1s 105ms/step - loss: 24.3087 - val_loss: 29.6984\n",
      "Epoch 1995/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.3071 - val_loss: 30.0417\n",
      "Epoch 1996/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 24.7623 - val_loss: 30.9801\n",
      "Epoch 1997/2000\n",
      "7/7 [==============================] - 1s 104ms/step - loss: 25.4282 - val_loss: 29.9695\n",
      "Epoch 1998/2000\n",
      "7/7 [==============================] - 1s 103ms/step - loss: 24.6181 - val_loss: 28.7367\n",
      "Epoch 1999/2000\n",
      "7/7 [==============================] - 1s 103ms/step - loss: 24.4383 - val_loss: 29.5948\n",
      "Epoch 2000/2000\n",
      "7/7 [==============================] - ETA: 0s - loss: 24.4555\n",
      "Epoch 02000: saving model to saved_models/latent16/cp-2000.h5\n",
      "7/7 [==============================] - 1s 150ms/step - loss: 24.4555 - val_loss: 29.2378\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Train VAE model with callbacks \"\"\"\n",
    "history = vae.fit(trainX, trainX, \n",
    "                  batch_size = batch_size, \n",
    "                  epochs = epochs, \n",
    "                  callbacks=[initial_checkpoint, checkpoint, EarlyStoppingAtMinLoss()],\n",
    "                  #callbacks=[initial_checkpoint, checkpoint],\n",
    "                  shuffle=True,\n",
    "                  validation_data=(valX, valX)\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step size: 7.0\n"
     ]
    }
   ],
   "source": [
    "# In this GPU implementation, it shows the number of batches/iterations for one epoch (and not the training samples), which is:\n",
    "print (\"Step size:\", np.ceil(trainX.shape[0]/batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Save the entire model \"\"\"\n",
    "# Save notes about hyperparameters used:\n",
    "epochs = len(history.history['loss'])\n",
    "with open(f'{SAVE_FOLDER}/notes.txt', 'w') as f:\n",
    "    f.write(f'Epochs: {epochs} \\n')\n",
    "    f.write(f'Batch size: {batch_size} \\n')\n",
    "    f.write(f'Latent dimension: {latent_dim} \\n')\n",
    "    f.write(f'Filter sizes: {filters} \\n')\n",
    "    f.write(f'img_width: {img_width} \\n')\n",
    "    f.write(f'img_height: {img_height} \\n')\n",
    "    f.write(f'num_channels: {num_channels}')\n",
    "    f.close()\n",
    "\n",
    "# Save weights for encoder model:\n",
    "encoder.save_weights(f'{SAVE_FOLDER}/ENCODER_WEIGHTS.h5')\n",
    "\n",
    "# Save weights for decoder model:\n",
    "decoder.save_weights(f'{SAVE_FOLDER}/DECODER_WEIGHTS.h5')\n",
    "\n",
    "# Save weights for total VAE:\n",
    "vae.save_weights(f'{SAVE_FOLDER}/VAE_WEIGHTS.h5')\n",
    "\n",
    "# Save the history at each epochs (cost of train and validation sets):\n",
    "np.save(f'{SAVE_FOLDER}/HISTORY.npy', history.history)\n",
    "\n",
    "# Save exact training, validation and testing data set (as numpy arrays):\n",
    "np.save(f'{SAVE_FOLDER}/trainX.npy', trainX)\n",
    "np.save(f'{SAVE_FOLDER}/valX.npy', valX)\n",
    "np.save(f'{SAVE_FOLDER}/testAllX.npy', testAllX)\n",
    "np.save(f'{SAVE_FOLDER}/testAllY.npy', testAllY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate training process\n",
    "\n",
    "Now that the training process is complete, lets evaluate the average loss of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 936x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set_style(\"darkgrid\")\n",
    "plt.figure(figsize=(13, 6))\n",
    "\n",
    "# summarize history for loss:\n",
    "plt.plot(history.history['loss'], color='blue', label='train')\n",
    "plt.plot(history.history['val_loss'], color='blue', alpha=0.4, label='validation')\n",
    "plt.xlabel('Epoch', fontsize=16)\n",
    "plt.ylabel('Cost', fontsize=16)\n",
    "plt.tick_params(axis='both', which='major', labelsize=14)\n",
    "plt.title('Average Loss During Training', fontsize=18)\n",
    "#plt.xticks(range(0,epochs))\n",
    "plt.legend(loc='upper right', fontsize=16)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
