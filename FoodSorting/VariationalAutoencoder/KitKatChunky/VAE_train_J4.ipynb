{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Autoencoder: Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import axis, colorbar, imshow, show, figure, subplot\n",
    "from matplotlib import cm\n",
    "import matplotlib as mpl\n",
    "mpl.rc('axes', labelsize=18)\n",
    "mpl.rc('xtick', labelsize=16)\n",
    "mpl.rc('ytick', labelsize=16)\n",
    "%matplotlib inline\n",
    "\n",
    "import glob\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "import cv2\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "## ----- GPU ------------------------------------\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'    # use of GPU 0 (ERDA) (use before importing torch or tensorflow/keeas)\n",
    "## ----------------------------------------------\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.layers import Conv2D, Conv2DTranspose, Input, Flatten, Dense, Lambda, Reshape\n",
    "from keras.layers import BatchNormalization, ReLU, LeakyReLU\n",
    "from keras.models import Model\n",
    "from keras.losses import binary_crossentropy, mse\n",
    "from keras.activations import relu\n",
    "from keras.callbacks import Callback, ModelCheckpoint\n",
    "from keras import backend as K                         #contains calls for tensor manipulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n",
      "2.4.3\n"
     ]
    }
   ],
   "source": [
    "print (tf.__version__)\n",
    "print (keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NORMAL_TRAIN_AUG:       /home/jovyan/work/Speciale/FoodSorting/generated_dataset/KitKatChunky//DataAugmentation/NormalTrainAugmentations\n",
      "NORMAL_VALIDATION_AUG:  /home/jovyan/work/Speciale/FoodSorting/generated_dataset/KitKatChunky//DataAugmentation/NormalValidationAugmentations\n",
      "NORMAL_TEST_AUG:        /home/jovyan/work/Speciale/FoodSorting/generated_dataset/KitKatChunky//DataAugmentation/NormalTestAugmentations\n",
      "ANOMALY_AUG:            /home/jovyan/work/Speciale/FoodSorting/generated_dataset/KitKatChunky//DataAugmentation/AnomalyAugmentations\n"
     ]
    }
   ],
   "source": [
    "from utils_vae_kitkat_paths import *\n",
    "\n",
    "# Get work directions for augmentated data set:\n",
    "print (\"NORMAL_TRAIN_AUG:      \", NORMAL_TRAIN_AUG)\n",
    "print (\"NORMAL_VALIDATION_AUG: \", NORMAL_VAL_AUG)\n",
    "print (\"NORMAL_TEST_AUG:       \", NORMAL_TEST_AUG)\n",
    "print (\"ANOMALY_AUG:           \", ANOMALY_AUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Which Folder The Models Are Saved In: saved_models/latent4\n"
     ]
    }
   ],
   "source": [
    "# What latent dimension and filter size are we using?:\n",
    "latent_dim = 4\n",
    "filters    = 32\n",
    "\n",
    "# Get work direction for saving files:\n",
    "SAVE_FOLDER = f'saved_models/latent{latent_dim}'\n",
    "print (\"Which Folder The Models Are Saved In:\", SAVE_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] deleting existing files in folder...\n",
      "         done in 0.033 minutes\n"
     ]
    }
   ],
   "source": [
    "# Delete all existing files in folder where models are saved:\n",
    "print(\"[INFO] deleting existing files in folder...\")\n",
    "t0 = time()\n",
    "\n",
    "files = glob.glob(f'{SAVE_FOLDER}/*')\n",
    "for f in files:\n",
    "    os.remove(f)\n",
    "\n",
    "print (\"         done in %0.3f minutes\" % ((time() - t0)/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_VAE_AE import get_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Investigation of Ground Truths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of \"normal\" training images is:     1400\n",
      "Number of \"normal\" validation images is:   134\n",
      "Number of \"normal\" test images is:         266\n",
      "Number of \"anomaly\" images is:             600\n"
     ]
    }
   ],
   "source": [
    "# Glob the directories and get the lists of good and bad images\n",
    "good_train_fns = [f for f in os.listdir(NORMAL_TRAIN_AUG) if not f.startswith('.')]\n",
    "good_val_fns = [f for f in os.listdir(NORMAL_VAL_AUG) if not f.startswith('.')]\n",
    "good_test_fns = [f for f in os.listdir(NORMAL_TEST_AUG) if not f.startswith('.')]\n",
    "bad_fns = [f for f in os.listdir(ANOMALY_AUG) if not f.startswith('.')]\n",
    "\n",
    "print('Number of \"normal\" training images is:     {}'.format(len(good_train_fns)))\n",
    "print('Number of \"normal\" validation images is:   {}'.format(len(good_val_fns)))\n",
    "print('Number of \"normal\" test images is:         {}'.format(len(good_test_fns)))\n",
    "print('Number of \"anomaly\" images is:             {}'.format(len(bad_fns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Procentage of normal samples (ground truth):   75.00 %\n",
      "> Procentage of anomaly samples (ground truth):  25.00 %\n"
     ]
    }
   ],
   "source": [
    "# compute procentage of bad images vs good images:\n",
    "normal_fns = len(good_train_fns) + len(good_val_fns) + len(good_test_fns)\n",
    "anomaly_fns = len(bad_fns)\n",
    "print(f\"> Procentage of normal samples (ground truth):   {(normal_fns / (normal_fns + anomaly_fns)) * 100:.2f} %\")\n",
    "print(f\"> Procentage of anomaly samples (ground truth):  {(anomaly_fns / (normal_fns + anomaly_fns)) * 100:.2f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Normal Data\n",
    "\n",
    "Load normal samples in pre-splitted data sets: train, valdiation and test.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading normal training images...\n",
      "          done\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] loading normal training images...\")\n",
    "trainX = [cv2.imread(file, 0) for file in glob.glob(f\"{NORMAL_TRAIN_AUG}/*.png\")]  # read as grayscale\n",
    "trainX = np.array(trainX)\n",
    "print(\"          done\")\n",
    "\n",
    "# create a corresponding list of labels:\n",
    "trainY = get_labels(trainX, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading normal validation images...\n",
      "          done\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] loading normal validation images...\")\n",
    "x_normal_val = [cv2.imread(file, 0) for file in glob.glob(f\"{NORMAL_VAL_AUG}/*.png\")]  # read as grayscale\n",
    "x_normal_val = np.array(x_normal_val)\n",
    "print(\"          done\")\n",
    "\n",
    "# create a corresponding list of labels:\n",
    "y_normal_val = get_labels(x_normal_val, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading normal testing images...\n",
      "          done\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] loading normal testing images...\")\n",
    "x_normal_test = [cv2.imread(file, 0) for file in glob.glob(f\"{NORMAL_TEST_AUG}/*.png\")]  # read as grayscale\n",
    "x_normal_test = np.array(x_normal_test)\n",
    "print(\"          done\")\n",
    "\n",
    "# create a corresponding list of labels:\n",
    "y_normal_test = get_labels(x_normal_test, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Anomaly Data\n",
    "\n",
    "\n",
    "Load anomaly samples in its singular training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading anomaly images...\n",
      "          done\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] loading anomaly images...\")\n",
    "x_anomaly = [cv2.imread(file, 0) for file in glob.glob(f\"{ANOMALY_AUG}/*.png\")]  # read as grayscale\n",
    "x_anomaly = np.array(x_anomaly)\n",
    "print(\"          done\")\n",
    "\n",
    "# create a corresponding list of labels:\n",
    "y_anomaly = get_labels(x_anomaly, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine normal samples and anomaly samples and split them into training, validation and test sets\n",
    "\n",
    "`label 0` == Normal Samples\n",
    "\n",
    "`label 1` == Anomaly Samples\n",
    "\n",
    "Train and Validation sets only consists of `Normal Data`, aka. `label 0`.\n",
    "\n",
    "Testing sets consists of both  `Normal Data` (aka. `label 0`) and `Anomaly Data` (aka. `label 1`)\n",
    "\n",
    "________________\n",
    "\n",
    "Due to the very sparse original (preprossed) KitKat Chunky data, the validation set will be a mix of normal testing and validation data. The normal testing set will consists of all validation and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine pre-loaded validation and testing set into a new testing set:\n",
    "testvalX = np.vstack((x_normal_val, x_normal_test))\n",
    "testvalY = get_labels(testvalX, 0)\n",
    "\n",
    "# randomly split in order to make new validation set:\n",
    "NoUsageX, valX, NoUsageY, valY = train_test_split(testvalX, testvalY, test_size=0.25, random_state=4345672)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine pre-loaded validation and testing set into a new testing set:\n",
    "testNormalX = np.vstack((x_normal_val, x_normal_test))\n",
    "testNormalY = get_labels(testNormalX, 0)\n",
    "\n",
    "# anomaly class (only used for testing):\n",
    "testAnomalyX, testAnomalyY = x_anomaly, y_anomaly\n",
    "\n",
    "# Combine all testing data in one array:\n",
    "testAllX = np.vstack((testNormalX, testAnomalyX))\n",
    "testAllY = np.hstack((testNormalY, testAnomalyY))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Data Dimensions and Distributions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect data shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:\n",
      " > images: (1400, 128, 128)\n",
      " > labels: (1400,)\n",
      "\n",
      "Validation set:\n",
      " > images: (100, 128, 128)\n",
      " > labels: (100,)\n",
      "\n",
      "Test set:\n",
      " > images: (1000, 128, 128)\n",
      " > labels: (1000,)\n",
      "\t'Normal' test set:\n",
      "\t  > images: (400, 128, 128)\n",
      "\t  > labels: (400,)\n",
      "\t'Anomaly' test set:\n",
      "\t  > images: (600, 128, 128)\n",
      "\t  > labels: (600,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set:\")\n",
    "print(\" > images:\", trainX.shape)\n",
    "print(\" > labels:\", trainY.shape)\n",
    "print (\"\")\n",
    "\n",
    "print(\"Validation set:\")\n",
    "print(\" > images:\", valX.shape)\n",
    "print(\" > labels:\", valY.shape)\n",
    "print (\"\")\n",
    "\n",
    "print(\"Test set:\")\n",
    "print(\" > images:\", testAllX.shape)\n",
    "print(\" > labels:\", testAllY.shape)\n",
    "print(\"\\t'Normal' test set:\")\n",
    "print(\"\\t  > images:\", testNormalX.shape)\n",
    "print(\"\\t  > labels:\", testNormalY.shape)\n",
    "print(\"\\t'Anomaly' test set:\")\n",
    "print(\"\\t  > images:\", testAnomalyX.shape)\n",
    "print(\"\\t  > labels:\", testAnomalyY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procentage of training samples: 73.68 %\n",
      "Procentage of validation samples: 5.26 %\n",
      "Procentage of (only normal) testing samples: 21.05 %\n"
     ]
    }
   ],
   "source": [
    "# compute procentage between training, validation and test data sets (only normal data):\n",
    "print(f\"Procentage of training samples: {(len(trainX) / (len(trainX) + len(valX) + len(testNormalX))) * 100:.2f} %\")\n",
    "print(f\"Procentage of validation samples: {(len(valX) / (len(trainX) + len(valX) + len(testNormalX))) * 100:.2f} %\")\n",
    "print(f\"Procentage of (only normal) testing samples: {(len(testNormalX) / (len(trainX) + len(valX) + len(testNormalX))) * 100:.2f} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procentage of training samples: 56.00 %\n",
      "Procentage of validation samples: 4.00 %\n",
      "Procentage of (total) testing samples: 40.00 %\n"
     ]
    }
   ],
   "source": [
    "# compute procentage between training, validation and test data sets:\n",
    "print(f\"Procentage of training samples: {(len(trainX) / (len(trainX) + len(valX) + len(testAllX))) * 100:.2f} %\")\n",
    "print(f\"Procentage of validation samples: {(len(valX) / (len(trainX) + len(valX) + len(testAllX))) * 100:.2f} %\")\n",
    "print(f\"Procentage of (total) testing samples: {(len(testAllX) / (len(trainX) + len(valX) + len(testAllX))) * 100:.2f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for (un)balanced data\n",
    "\n",
    "In an ideal world the data should be balanced. However in the real world we expect there being around ~$99 \\%$ good samples and only ~$1 \\%$ bad samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels = ['0: normal', '1: anomaly']\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(16,4))\n",
    "ax[0].hist(trainY, align=\"left\"); ax[0].set_title('train', fontsize=18); ax[0].set_xticks([0,1]); ax[0].set_xticklabels(labels); ax[0].tick_params(axis='both', which='major', labelsize=16); ax[0].set_xlim(-0.5, 1.5);\n",
    "ax[1].hist(valY, align=\"left\"); ax[1].set_title('validation', fontsize=18); ax[1].set_xticks([0,1]); ax[1].set_xticklabels(labels); ax[1].tick_params(axis='both', which='major', labelsize=16); ax[1].set_xlim(-0.5, 1.5);\n",
    "ax[2].hist(testAllY, align=\"left\"); ax[2].set_title('test', fontsize=18); ax[2].set_xticks([0,1]); ax[2].set_xticklabels(labels); ax[2].tick_params(axis='both', which='major', labelsize=16); ax[2].set_xlim(-0.5, 1.5);\n",
    "\n",
    "ax[0].set_ylabel(\"Number of images\", fontsize=18)\n",
    "ax[1].set_xlabel(\"Class Labels\", fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procentage of normal test samples: 40.00 %\n",
      "Procentage of total anomaly test samples: 60.00 %\n"
     ]
    }
   ],
   "source": [
    "# compute procentage of normal test images vs total anomaly test images:\n",
    "print(f\"Procentage of normal test samples: {(len(testNormalX) / (len(testNormalX) + len(testAnomalyX))) * 100:.2f} %\")\n",
    "print(f\"Procentage of total anomaly test samples: {(len(testAnomalyX) / (len(testNormalX) + len(testAnomalyX))) * 100:.2f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling\n",
    "\n",
    "Only normalization has been used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data configuration\n",
    "img_width, img_height = trainX.shape[1], trainX.shape[2]      # input image dimensions\n",
    "num_channels = 1                                              # gray-channel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization\n",
    "\n",
    "Neural Networks learns faster with normalized data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize to be in  the [0, 1] range:\n",
    "trainX = trainX.astype('float32') / 255.\n",
    "valX = valX.astype('float32') / 255.\n",
    "testAllX = testAllX.astype('float32') / 255.\n",
    "\n",
    "# reshape data to keras inputs --> (n_samples, img_rows, img_cols, n_channels):\n",
    "trainX = trainX.reshape(trainX.shape[0], img_height, img_width, num_channels)\n",
    "valX = valX.reshape(valX.shape[0], img_height, img_width, num_channels)\n",
    "testAllX = testAllX.reshape(testAllX.shape[0], img_height, img_width, num_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture and Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import architecture of VAE model and its hyperparameters:\n",
    "from J4_VAE_model import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Encoder Part*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_input (InputLayer)      [(None, 128, 128, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 64, 64, 32)   320         encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)         (None, 64, 64, 32)   0           conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 32, 32, 32)   9248        leaky_re_lu[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)       (None, 32, 32, 32)   0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 16, 16, 32)   9248        leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)       (None, 16, 16, 32)   0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 8, 8, 32)     9248        leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)       (None, 8, 8, 32)     0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 4, 4, 32)     9248        leaky_re_lu_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)       (None, 4, 4, 32)     0           conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 512)          0           leaky_re_lu_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "latent (Dense)                  (None, 200)          102600      flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)       (None, 200)          0           latent[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "z_mean (Dense)                  (None, 4)            804         leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "z_log_mean (Dense)              (None, 4)            804         leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "z (Lambda)                      (None, 4)            0           z_mean[0][0]                     \n",
      "                                                                 z_log_mean[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 141,520\n",
      "Trainable params: 141,520\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Decoder Part*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "decoder_input (InputLayer)   [(None, 4)]               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               2560      \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 4, 4, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose (Conv2DTran (None, 8, 8, 32)          9248      \n",
      "_________________________________________________________________\n",
      "re_lu (ReLU)                 (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 16, 16, 32)        9248      \n",
      "_________________________________________________________________\n",
      "re_lu_1 (ReLU)               (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTr (None, 32, 32, 32)        9248      \n",
      "_________________________________________________________________\n",
      "re_lu_2 (ReLU)               (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTr (None, 64, 64, 32)        9248      \n",
      "_________________________________________________________________\n",
      "re_lu_3 (ReLU)               (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_4 (Conv2DTr (None, 128, 128, 32)      9248      \n",
      "_________________________________________________________________\n",
      "re_lu_4 (ReLU)               (None, 128, 128, 32)      0         \n",
      "_________________________________________________________________\n",
      "decoder_output (Conv2DTransp (None, 128, 128, 1)       289       \n",
      "=================================================================\n",
      "Total params: 49,089\n",
      "Trainable params: 49,089\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Total VAE Model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vae\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder_input (InputLayer)   [(None, 128, 128, 1)]     0         \n",
      "_________________________________________________________________\n",
      "encoder (Functional)         [(None, 4), (None, 4), (N 141520    \n",
      "_________________________________________________________________\n",
      "decoder (Functional)         (None, 128, 128, 1)       49089     \n",
      "=================================================================\n",
      "Total params: 190,609\n",
      "Trainable params: 190,609\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vae.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile and train model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Hyperparameters \"\"\"\n",
    "batch_size  = 256\n",
    "epochs      = 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Checkpoint callback options: save model at 1'st epoch (inital model)\n",
    "\n",
    "https://stackoverflow.com/questions/54323960/save-keras-model-at-specific-epochs\n",
    "\"\"\"\n",
    "\n",
    "class CustomSaver(keras.callbacks.Callback):\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        if epoch == 1:\n",
    "            self.model.save_weights(f\"{SAVE_FOLDER}/cp-0001.h5\")\n",
    "            \n",
    "# create and use callback:\n",
    "initial_checkpoint = CustomSaver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Checkpoint callback options: save model at every k'te epochs\n",
    "\"\"\"\n",
    "# Include the epoch in the file name (uses `str.format`):\n",
    "checkpoint_path = \"saved_models/latent4/cp-{epoch:04d}.h5\"\n",
    "\n",
    "# Create a callback that saves the model's weights every k'te epochs:\n",
    "checkpoint = ModelCheckpoint(filepath = checkpoint_path,\n",
    "                             monitor='loss',           # name of the metrics to monitor\n",
    "                             verbose=1, \n",
    "                             #save_best_only=False,     # if False, the best model will not be overridden.\n",
    "                             save_weights_only=True,   # if True, only the weights of the models will be saved.\n",
    "                                                        # if False, the whole models will be saved.\n",
    "                             #mode='auto', \n",
    "                             period=200                  # save after every k'te epochs\n",
    "                            )\n",
    "\n",
    "# Save the weights using the `checkpoint_path` format\n",
    "vae.save_weights(checkpoint_path.format(epoch=0))          # save for zero epoch (inital)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Checkpoint callback options: Early stopping\n",
    "https://www.tensorflow.org/guide/keras/custom_callback\n",
    "\"\"\"\n",
    "\n",
    "class EarlyStoppingAtMinLoss(keras.callbacks.Callback):\n",
    "    \"\"\"Stop training when the loss is at its min, i.e. the loss stops decreasing.\n",
    "\n",
    "  Arguments:\n",
    "      patience: Number of epochs to wait after min has been hit. After this\n",
    "      number of no improvement, training stops.\n",
    "  \"\"\"\n",
    "\n",
    "    def __init__(self, patience=100):\n",
    "        super(EarlyStoppingAtMinLoss, self).__init__()\n",
    "        self.patience = patience\n",
    "        # best_weights to store the weights at which the minimum loss occurs.\n",
    "        self.best_weights = None\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        # The number of epoch it has waited when loss is no longer minimum.\n",
    "        self.wait = 0\n",
    "        # The epoch the training stops at.\n",
    "        self.stopped_epoch = 0\n",
    "        # Initialize the best as infinity.\n",
    "        self.best = np.Inf\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        current = logs.get(\"loss\")\n",
    "        if np.less(current, self.best):\n",
    "            self.best = current\n",
    "            self.wait = 0\n",
    "            # Record the best weights if current results is better (less).\n",
    "            self.best_weights = self.model.get_weights()\n",
    "        else:\n",
    "            self.wait += 1\n",
    "            if self.wait >= self.patience:\n",
    "                self.stopped_epoch = epoch\n",
    "                self.model.stop_training = True\n",
    "                print(\"Restoring model weights from the end of the best epoch.\")\n",
    "                self.model.set_weights(self.best_weights)\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        if self.stopped_epoch > 0:\n",
    "            print(\"Epoch %05d: early stopping\" % (self.stopped_epoch + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Create and Compile Model\"\"\"\n",
    "# import architecture of VAE model and its hyperparameters. learning rate choosen from graph above.\n",
    "vae = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      "6/6 [==============================] - 1s 209ms/step - loss: 3528.2393 - val_loss: 3444.6733\n",
      "Epoch 2/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 3258.8691 - val_loss: 2865.6328\n",
      "Epoch 3/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 2268.7373 - val_loss: 1411.9243\n",
      "Epoch 4/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 1071.0322 - val_loss: 952.6407\n",
      "Epoch 5/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 852.9938 - val_loss: 904.7869\n",
      "Epoch 6/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 821.9099 - val_loss: 888.5553\n",
      "Epoch 7/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 800.9570 - val_loss: 839.2383\n",
      "Epoch 8/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 740.9453 - val_loss: 765.3344\n",
      "Epoch 9/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 663.2713 - val_loss: 693.0859\n",
      "Epoch 10/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 613.3854 - val_loss: 653.4674\n",
      "Epoch 11/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 593.6966 - val_loss: 638.4249\n",
      "Epoch 12/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 581.9068 - val_loss: 626.5959\n",
      "Epoch 13/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 571.0865 - val_loss: 616.1085\n",
      "Epoch 14/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 563.4727 - val_loss: 609.9208\n",
      "Epoch 15/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 556.0117 - val_loss: 605.5316\n",
      "Epoch 16/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 551.7792 - val_loss: 601.5505\n",
      "Epoch 17/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 549.3526 - val_loss: 599.4642\n",
      "Epoch 18/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 547.5919 - val_loss: 596.5546\n",
      "Epoch 19/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 543.1334 - val_loss: 594.5432\n",
      "Epoch 20/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 541.3323 - val_loss: 590.1581\n",
      "Epoch 21/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 539.1771 - val_loss: 587.9230\n",
      "Epoch 22/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 534.6896 - val_loss: 581.3841\n",
      "Epoch 23/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 527.6165 - val_loss: 566.4061\n",
      "Epoch 24/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 502.2255 - val_loss: 508.9283\n",
      "Epoch 25/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 451.1039 - val_loss: 433.8162\n",
      "Epoch 26/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 378.5880 - val_loss: 372.4262\n",
      "Epoch 27/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 322.3407 - val_loss: 330.3122\n",
      "Epoch 28/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 290.3871 - val_loss: 305.9531\n",
      "Epoch 29/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 269.6504 - val_loss: 285.8896\n",
      "Epoch 30/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 253.8065 - val_loss: 270.5457\n",
      "Epoch 31/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 242.6105 - val_loss: 259.4173\n",
      "Epoch 32/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 232.6526 - val_loss: 247.5145\n",
      "Epoch 33/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 223.2833 - val_loss: 236.5625\n",
      "Epoch 34/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 215.1235 - val_loss: 228.8049\n",
      "Epoch 35/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 206.6789 - val_loss: 218.8354\n",
      "Epoch 36/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 197.7236 - val_loss: 208.7250\n",
      "Epoch 37/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 187.3699 - val_loss: 198.8608\n",
      "Epoch 38/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 184.9047 - val_loss: 192.2618\n",
      "Epoch 39/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 175.0620 - val_loss: 186.8199\n",
      "Epoch 40/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 167.3255 - val_loss: 177.4958\n",
      "Epoch 41/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 158.7727 - val_loss: 170.5683\n",
      "Epoch 42/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 153.4667 - val_loss: 166.8851\n",
      "Epoch 43/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 146.3626 - val_loss: 161.8142\n",
      "Epoch 44/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 141.1559 - val_loss: 154.3078\n",
      "Epoch 45/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 135.4169 - val_loss: 148.9037\n",
      "Epoch 46/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 131.9986 - val_loss: 146.7468\n",
      "Epoch 47/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 127.6668 - val_loss: 141.9896\n",
      "Epoch 48/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 126.6417 - val_loss: 154.6884\n",
      "Epoch 49/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 129.3707 - val_loss: 140.3918\n",
      "Epoch 50/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 122.2461 - val_loss: 138.7906\n",
      "Epoch 51/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 119.2285 - val_loss: 134.5548\n",
      "Epoch 52/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 119.7189 - val_loss: 132.8640\n",
      "Epoch 53/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 115.6191 - val_loss: 130.9236\n",
      "Epoch 54/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 114.4512 - val_loss: 132.8262\n",
      "Epoch 55/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 114.4134 - val_loss: 129.4254\n",
      "Epoch 56/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 111.3048 - val_loss: 126.0768\n",
      "Epoch 57/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 110.0823 - val_loss: 125.9612\n",
      "Epoch 58/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 108.9100 - val_loss: 124.2281\n",
      "Epoch 59/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 107.7225 - val_loss: 124.2536\n",
      "Epoch 60/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 107.5199 - val_loss: 124.8910\n",
      "Epoch 61/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 105.6051 - val_loss: 121.3125\n",
      "Epoch 62/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 105.0425 - val_loss: 123.0188\n",
      "Epoch 63/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 105.4064 - val_loss: 120.8962\n",
      "Epoch 64/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 104.6326 - val_loss: 119.7026\n",
      "Epoch 65/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 103.9089 - val_loss: 123.3278\n",
      "Epoch 66/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 103.1247 - val_loss: 121.0980\n",
      "Epoch 67/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 102.5689 - val_loss: 118.1585\n",
      "Epoch 68/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 101.0090 - val_loss: 116.1026\n",
      "Epoch 69/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 100.2659 - val_loss: 119.7144\n",
      "Epoch 70/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 100.7176 - val_loss: 115.7606\n",
      "Epoch 71/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 99.0123 - val_loss: 116.0646\n",
      "Epoch 72/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 98.4147 - val_loss: 117.6223\n",
      "Epoch 73/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 98.8122 - val_loss: 115.0894\n",
      "Epoch 74/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 97.8680 - val_loss: 113.4060\n",
      "Epoch 75/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 98.5108 - val_loss: 117.2530\n",
      "Epoch 76/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 99.1531 - val_loss: 114.6063\n",
      "Epoch 77/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 96.8845 - val_loss: 113.4816\n",
      "Epoch 78/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 96.6302 - val_loss: 112.2137\n",
      "Epoch 79/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 95.8905 - val_loss: 112.1386\n",
      "Epoch 80/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 94.9598 - val_loss: 112.7933\n",
      "Epoch 81/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 94.4691 - val_loss: 112.1026\n",
      "Epoch 82/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 93.8158 - val_loss: 111.0481\n",
      "Epoch 83/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 94.7162 - val_loss: 115.2250\n",
      "Epoch 84/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 94.8418 - val_loss: 109.5377\n",
      "Epoch 85/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 93.3589 - val_loss: 107.8667\n",
      "Epoch 86/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 92.0370 - val_loss: 108.5771\n",
      "Epoch 87/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 91.5274 - val_loss: 108.0590\n",
      "Epoch 88/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 90.9162 - val_loss: 107.2238\n",
      "Epoch 89/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 89.8318 - val_loss: 107.0133\n",
      "Epoch 90/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 94.4544 - val_loss: 106.4220\n",
      "Epoch 91/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 92.1939 - val_loss: 106.9859\n",
      "Epoch 92/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 90.4737 - val_loss: 104.9369\n",
      "Epoch 93/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 89.0759 - val_loss: 103.8603\n",
      "Epoch 94/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 88.2288 - val_loss: 104.3358\n",
      "Epoch 95/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 88.0549 - val_loss: 103.6945\n",
      "Epoch 96/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 87.9791 - val_loss: 105.2496\n",
      "Epoch 97/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 87.1664 - val_loss: 101.8738\n",
      "Epoch 98/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 85.9646 - val_loss: 101.4892\n",
      "Epoch 99/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 85.5236 - val_loss: 100.3859\n",
      "Epoch 100/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 85.7682 - val_loss: 99.7809\n",
      "Epoch 101/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 85.5347 - val_loss: 101.6187\n",
      "Epoch 102/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 84.8998 - val_loss: 101.0842\n",
      "Epoch 103/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 85.0241 - val_loss: 99.4683\n",
      "Epoch 104/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 84.0735 - val_loss: 98.5054\n",
      "Epoch 105/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 83.3137 - val_loss: 101.1862\n",
      "Epoch 106/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 85.2291 - val_loss: 102.2172\n",
      "Epoch 107/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 85.2467 - val_loss: 96.4361\n",
      "Epoch 108/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 83.7582 - val_loss: 97.2045\n",
      "Epoch 109/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 82.8711 - val_loss: 97.6856\n",
      "Epoch 110/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 81.2952 - val_loss: 95.4807\n",
      "Epoch 111/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 80.6568 - val_loss: 94.4985\n",
      "Epoch 112/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 79.8195 - val_loss: 94.6836\n",
      "Epoch 113/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 79.8714 - val_loss: 95.2599\n",
      "Epoch 114/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 79.4767 - val_loss: 93.4372\n",
      "Epoch 115/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 79.3873 - val_loss: 93.0665\n",
      "Epoch 116/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 79.2601 - val_loss: 93.9393\n",
      "Epoch 117/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 78.8541 - val_loss: 92.8115\n",
      "Epoch 118/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 79.0796 - val_loss: 92.4337\n",
      "Epoch 119/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 80.5152 - val_loss: 94.8178\n",
      "Epoch 120/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 78.1497 - val_loss: 92.1691\n",
      "Epoch 121/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 76.9557 - val_loss: 91.7622\n",
      "Epoch 122/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 76.5484 - val_loss: 92.3860\n",
      "Epoch 123/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 77.2936 - val_loss: 89.6975\n",
      "Epoch 124/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 76.3914 - val_loss: 93.3923\n",
      "Epoch 125/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 75.9058 - val_loss: 89.4810\n",
      "Epoch 126/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 74.3548 - val_loss: 88.7042\n",
      "Epoch 127/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 74.7568 - val_loss: 89.8108\n",
      "Epoch 128/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 75.3046 - val_loss: 90.0675\n",
      "Epoch 129/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 74.0781 - val_loss: 88.1343\n",
      "Epoch 130/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 74.0140 - val_loss: 91.0676\n",
      "Epoch 131/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 74.3970 - val_loss: 87.4593\n",
      "Epoch 132/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 73.9425 - val_loss: 89.0394\n",
      "Epoch 133/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 73.8697 - val_loss: 87.9631\n",
      "Epoch 134/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 73.8567 - val_loss: 87.2595\n",
      "Epoch 135/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 73.1197 - val_loss: 86.3318\n",
      "Epoch 136/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 72.7233 - val_loss: 90.8579\n",
      "Epoch 137/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 72.4257 - val_loss: 86.5303\n",
      "Epoch 138/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 70.7899 - val_loss: 84.0479\n",
      "Epoch 139/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 70.4439 - val_loss: 85.0701\n",
      "Epoch 140/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 70.2903 - val_loss: 83.2007\n",
      "Epoch 141/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 69.4674 - val_loss: 83.2850\n",
      "Epoch 142/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 68.4394 - val_loss: 83.1124\n",
      "Epoch 143/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 68.3356 - val_loss: 82.8675\n",
      "Epoch 144/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 70.6287 - val_loss: 88.3061\n",
      "Epoch 145/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 70.3479 - val_loss: 84.4315\n",
      "Epoch 146/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 68.3612 - val_loss: 82.6695\n",
      "Epoch 147/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 68.8617 - val_loss: 81.3231\n",
      "Epoch 148/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 69.6811 - val_loss: 82.3952\n",
      "Epoch 149/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 68.7344 - val_loss: 79.6918\n",
      "Epoch 150/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 69.0828 - val_loss: 80.9303\n",
      "Epoch 151/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 68.8909 - val_loss: 79.7038\n",
      "Epoch 152/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 67.2866 - val_loss: 80.5316\n",
      "Epoch 153/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 66.3566 - val_loss: 79.8377\n",
      "Epoch 154/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 65.3772 - val_loss: 79.4456\n",
      "Epoch 155/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 65.5451 - val_loss: 78.9375\n",
      "Epoch 156/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 64.6905 - val_loss: 79.1231\n",
      "Epoch 157/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 64.8333 - val_loss: 78.0269\n",
      "Epoch 158/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 63.9762 - val_loss: 77.2121\n",
      "Epoch 159/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 63.5582 - val_loss: 77.7410\n",
      "Epoch 160/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 63.4322 - val_loss: 77.6242\n",
      "Epoch 161/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 63.0284 - val_loss: 77.6504\n",
      "Epoch 162/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 63.6433 - val_loss: 78.2901\n",
      "Epoch 163/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 63.5063 - val_loss: 76.8217\n",
      "Epoch 164/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 63.1525 - val_loss: 75.9577\n",
      "Epoch 165/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 61.9717 - val_loss: 75.3271\n",
      "Epoch 166/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 61.7370 - val_loss: 74.6784\n",
      "Epoch 167/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 61.6955 - val_loss: 74.9455\n",
      "Epoch 168/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 61.4855 - val_loss: 75.3310\n",
      "Epoch 169/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 62.2375 - val_loss: 76.1156\n",
      "Epoch 170/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 61.0824 - val_loss: 74.3812\n",
      "Epoch 171/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 60.9291 - val_loss: 75.4277\n",
      "Epoch 172/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 61.0111 - val_loss: 74.4263\n",
      "Epoch 173/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 60.6505 - val_loss: 73.3643\n",
      "Epoch 174/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 59.6084 - val_loss: 72.9022\n",
      "Epoch 175/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 59.4185 - val_loss: 72.3817\n",
      "Epoch 176/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 59.0836 - val_loss: 72.1251\n",
      "Epoch 177/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 59.1284 - val_loss: 72.5832\n",
      "Epoch 178/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 59.7401 - val_loss: 73.7598\n",
      "Epoch 179/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 59.6469 - val_loss: 73.8039\n",
      "Epoch 180/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 60.2169 - val_loss: 71.9190\n",
      "Epoch 181/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 60.9187 - val_loss: 74.0644\n",
      "Epoch 182/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 60.9599 - val_loss: 74.9971\n",
      "Epoch 183/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 60.5129 - val_loss: 73.5913\n",
      "Epoch 184/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 58.9257 - val_loss: 72.3677\n",
      "Epoch 185/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 58.2232 - val_loss: 70.9954\n",
      "Epoch 186/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 57.5528 - val_loss: 70.7684\n",
      "Epoch 187/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 57.4962 - val_loss: 70.5202\n",
      "Epoch 188/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 57.4400 - val_loss: 68.6944\n",
      "Epoch 189/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 56.8797 - val_loss: 69.4868\n",
      "Epoch 190/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 57.4947 - val_loss: 68.8058\n",
      "Epoch 191/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 56.7752 - val_loss: 70.1178\n",
      "Epoch 192/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 56.7191 - val_loss: 71.3679\n",
      "Epoch 193/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 56.7120 - val_loss: 68.7382\n",
      "Epoch 194/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 56.2946 - val_loss: 69.3955\n",
      "Epoch 195/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 56.3471 - val_loss: 68.7203\n",
      "Epoch 196/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 56.4195 - val_loss: 72.1971\n",
      "Epoch 197/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 56.4897 - val_loss: 70.5758\n",
      "Epoch 198/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 56.3610 - val_loss: 69.6786\n",
      "Epoch 199/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 56.1339 - val_loss: 67.1504\n",
      "Epoch 200/2000\n",
      "6/6 [==============================] - ETA: 0s - loss: 54.9837\n",
      "Epoch 00200: saving model to saved_models/latent4/cp-0200.h5\n",
      "6/6 [==============================] - 1s 157ms/step - loss: 54.9837 - val_loss: 67.1733\n",
      "Epoch 201/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 54.7689 - val_loss: 66.4834\n",
      "Epoch 202/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 54.3014 - val_loss: 66.4974\n",
      "Epoch 203/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 54.4184 - val_loss: 66.1820\n",
      "Epoch 204/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 54.0130 - val_loss: 65.3984\n",
      "Epoch 205/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 54.3309 - val_loss: 65.8198\n",
      "Epoch 206/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 53.6049 - val_loss: 66.1169\n",
      "Epoch 207/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 54.1059 - val_loss: 64.9137\n",
      "Epoch 208/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 53.1152 - val_loss: 64.9163\n",
      "Epoch 209/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 52.6023 - val_loss: 64.4727\n",
      "Epoch 210/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 52.5235 - val_loss: 63.7858\n",
      "Epoch 211/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 52.3693 - val_loss: 64.9871\n",
      "Epoch 212/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 52.8052 - val_loss: 64.1003\n",
      "Epoch 213/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 53.0264 - val_loss: 64.8181\n",
      "Epoch 214/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 52.6843 - val_loss: 65.2546\n",
      "Epoch 215/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 52.7661 - val_loss: 65.1629\n",
      "Epoch 216/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 52.5813 - val_loss: 63.1913\n",
      "Epoch 217/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 51.5192 - val_loss: 62.3847\n",
      "Epoch 218/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 50.9596 - val_loss: 62.4919\n",
      "Epoch 219/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 50.5462 - val_loss: 61.5038\n",
      "Epoch 220/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 50.6901 - val_loss: 62.0748\n",
      "Epoch 221/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 50.6229 - val_loss: 61.7085\n",
      "Epoch 222/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 50.2789 - val_loss: 62.0407\n",
      "Epoch 223/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 50.2048 - val_loss: 60.6721\n",
      "Epoch 224/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 49.8285 - val_loss: 61.7463\n",
      "Epoch 225/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 49.8462 - val_loss: 60.5939\n",
      "Epoch 226/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 49.4546 - val_loss: 61.0022\n",
      "Epoch 227/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 49.6306 - val_loss: 60.7513\n",
      "Epoch 228/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 49.3938 - val_loss: 60.2887\n",
      "Epoch 229/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 48.9225 - val_loss: 59.6781\n",
      "Epoch 230/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 49.0641 - val_loss: 59.9843\n",
      "Epoch 231/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 49.6435 - val_loss: 62.7616\n",
      "Epoch 232/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 50.4978 - val_loss: 59.8444\n",
      "Epoch 233/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 48.8012 - val_loss: 59.9394\n",
      "Epoch 234/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 48.4987 - val_loss: 59.2929\n",
      "Epoch 235/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 48.3430 - val_loss: 59.4339\n",
      "Epoch 236/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 48.3887 - val_loss: 58.4595\n",
      "Epoch 237/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 47.8495 - val_loss: 59.5293\n",
      "Epoch 238/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 48.3113 - val_loss: 58.2366\n",
      "Epoch 239/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 47.7054 - val_loss: 57.8633\n",
      "Epoch 240/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 47.1529 - val_loss: 57.6189\n",
      "Epoch 241/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 47.1307 - val_loss: 58.8044\n",
      "Epoch 242/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 47.8506 - val_loss: 58.0574\n",
      "Epoch 243/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 47.7202 - val_loss: 58.3522\n",
      "Epoch 244/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 47.6221 - val_loss: 58.3280\n",
      "Epoch 245/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 47.1804 - val_loss: 57.1083\n",
      "Epoch 246/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 46.6330 - val_loss: 56.3106\n",
      "Epoch 247/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 46.4945 - val_loss: 57.4498\n",
      "Epoch 248/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 47.2395 - val_loss: 56.7589\n",
      "Epoch 249/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.7443 - val_loss: 56.7462\n",
      "Epoch 250/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.5953 - val_loss: 56.1114\n",
      "Epoch 251/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.7513 - val_loss: 56.8726\n",
      "Epoch 252/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.4691 - val_loss: 57.1753\n",
      "Epoch 253/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 46.3654 - val_loss: 55.7542\n",
      "Epoch 254/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 45.4990 - val_loss: 55.7031\n",
      "Epoch 255/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 45.4201 - val_loss: 55.0202\n",
      "Epoch 256/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 45.0325 - val_loss: 56.6935\n",
      "Epoch 257/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 45.4872 - val_loss: 55.9565\n",
      "Epoch 258/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 45.0442 - val_loss: 54.8272\n",
      "Epoch 259/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 44.5179 - val_loss: 54.9273\n",
      "Epoch 260/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 44.3003 - val_loss: 54.5224\n",
      "Epoch 261/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 44.9511 - val_loss: 54.2064\n",
      "Epoch 262/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 44.2837 - val_loss: 53.7781\n",
      "Epoch 263/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 44.3281 - val_loss: 53.7951\n",
      "Epoch 264/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 44.1426 - val_loss: 54.2311\n",
      "Epoch 265/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 44.1423 - val_loss: 53.3170\n",
      "Epoch 266/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 44.2312 - val_loss: 53.2313\n",
      "Epoch 267/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 43.5866 - val_loss: 53.5511\n",
      "Epoch 268/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 43.8481 - val_loss: 52.8845\n",
      "Epoch 269/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 43.7385 - val_loss: 53.7164\n",
      "Epoch 270/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 44.0466 - val_loss: 53.3803\n",
      "Epoch 271/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 43.3035 - val_loss: 52.5085\n",
      "Epoch 272/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 43.2318 - val_loss: 53.9496\n",
      "Epoch 273/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 43.9957 - val_loss: 53.1949\n",
      "Epoch 274/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 43.6917 - val_loss: 52.0493\n",
      "Epoch 275/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 42.8211 - val_loss: 52.3787\n",
      "Epoch 276/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 42.7093 - val_loss: 52.0406\n",
      "Epoch 277/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 42.9014 - val_loss: 52.7005\n",
      "Epoch 278/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 42.5367 - val_loss: 51.5404\n",
      "Epoch 279/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 42.1586 - val_loss: 51.0505\n",
      "Epoch 280/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 42.1144 - val_loss: 52.0532\n",
      "Epoch 281/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 43.0369 - val_loss: 52.4882\n",
      "Epoch 282/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 43.7797 - val_loss: 51.2417\n",
      "Epoch 283/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 42.8401 - val_loss: 52.4446\n",
      "Epoch 284/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 42.4291 - val_loss: 50.4955\n",
      "Epoch 285/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 41.5340 - val_loss: 51.6714\n",
      "Epoch 286/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 41.4566 - val_loss: 49.9721\n",
      "Epoch 287/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 41.1560 - val_loss: 50.0311\n",
      "Epoch 288/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 40.8749 - val_loss: 50.0234\n",
      "Epoch 289/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 40.9177 - val_loss: 50.0745\n",
      "Epoch 290/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 41.5906 - val_loss: 49.5095\n",
      "Epoch 291/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 40.7043 - val_loss: 49.6100\n",
      "Epoch 292/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 40.9407 - val_loss: 49.2975\n",
      "Epoch 293/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 40.7992 - val_loss: 49.5009\n",
      "Epoch 294/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 40.6041 - val_loss: 49.0744\n",
      "Epoch 295/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 40.5003 - val_loss: 48.8552\n",
      "Epoch 296/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 40.1693 - val_loss: 49.5447\n",
      "Epoch 297/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 40.6632 - val_loss: 49.6023\n",
      "Epoch 298/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 40.7673 - val_loss: 49.1377\n",
      "Epoch 299/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 41.7341 - val_loss: 48.4738\n",
      "Epoch 300/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 40.4290 - val_loss: 48.8837\n",
      "Epoch 301/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 40.2426 - val_loss: 48.5170\n",
      "Epoch 302/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 40.2397 - val_loss: 48.5358\n",
      "Epoch 303/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 39.9438 - val_loss: 47.4863\n",
      "Epoch 304/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 40.0777 - val_loss: 47.8201\n",
      "Epoch 305/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 40.1784 - val_loss: 47.9927\n",
      "Epoch 306/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 40.1207 - val_loss: 49.6191\n",
      "Epoch 307/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 40.1912 - val_loss: 48.4438\n",
      "Epoch 308/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 39.7305 - val_loss: 47.2997\n",
      "Epoch 309/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 39.8399 - val_loss: 47.3425\n",
      "Epoch 310/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 39.4395 - val_loss: 47.1506\n",
      "Epoch 311/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 39.5018 - val_loss: 47.3904\n",
      "Epoch 312/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 39.1515 - val_loss: 47.0300\n",
      "Epoch 313/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 39.4138 - val_loss: 46.7233\n",
      "Epoch 314/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 39.3517 - val_loss: 46.7257\n",
      "Epoch 315/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 39.3863 - val_loss: 47.2942\n",
      "Epoch 316/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 39.0376 - val_loss: 46.7667\n",
      "Epoch 317/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 38.6133 - val_loss: 45.4989\n",
      "Epoch 318/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 38.3570 - val_loss: 45.5898\n",
      "Epoch 319/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 38.6890 - val_loss: 45.7415\n",
      "Epoch 320/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 38.2689 - val_loss: 46.6425\n",
      "Epoch 321/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 38.3920 - val_loss: 45.3526\n",
      "Epoch 322/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 38.3627 - val_loss: 45.6391\n",
      "Epoch 323/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 38.3753 - val_loss: 46.7654\n",
      "Epoch 324/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 38.4449 - val_loss: 44.8818\n",
      "Epoch 325/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 37.9698 - val_loss: 45.2328\n",
      "Epoch 326/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 37.7427 - val_loss: 45.3671\n",
      "Epoch 327/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 37.6717 - val_loss: 45.6487\n",
      "Epoch 328/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 38.3666 - val_loss: 46.3180\n",
      "Epoch 329/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 38.1269 - val_loss: 45.5544\n",
      "Epoch 330/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 37.7461 - val_loss: 45.5120\n",
      "Epoch 331/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 38.0875 - val_loss: 45.6426\n",
      "Epoch 332/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 37.9422 - val_loss: 44.7878\n",
      "Epoch 333/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 37.2300 - val_loss: 45.5595\n",
      "Epoch 334/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 37.6088 - val_loss: 45.9642\n",
      "Epoch 335/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 37.5347 - val_loss: 43.8753\n",
      "Epoch 336/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 36.9885 - val_loss: 43.9017\n",
      "Epoch 337/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 37.0324 - val_loss: 43.7077\n",
      "Epoch 338/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 36.9328 - val_loss: 45.9380\n",
      "Epoch 339/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 37.9090 - val_loss: 44.1900\n",
      "Epoch 340/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 36.7623 - val_loss: 44.5787\n",
      "Epoch 341/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 37.0719 - val_loss: 43.6126\n",
      "Epoch 342/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 36.6243 - val_loss: 43.4110\n",
      "Epoch 343/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 36.9131 - val_loss: 43.1062\n",
      "Epoch 344/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 36.3154 - val_loss: 43.1309\n",
      "Epoch 345/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 36.4541 - val_loss: 43.5190\n",
      "Epoch 346/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 36.8030 - val_loss: 43.4985\n",
      "Epoch 347/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 36.8423 - val_loss: 43.4771\n",
      "Epoch 348/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 36.7029 - val_loss: 43.8215\n",
      "Epoch 349/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 36.5356 - val_loss: 42.1852\n",
      "Epoch 350/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 36.2113 - val_loss: 42.5913\n",
      "Epoch 351/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 36.0291 - val_loss: 42.4961\n",
      "Epoch 352/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 35.7041 - val_loss: 43.7051\n",
      "Epoch 353/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 36.9386 - val_loss: 43.7313\n",
      "Epoch 354/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 36.9147 - val_loss: 44.2030\n",
      "Epoch 355/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 38.1227 - val_loss: 42.8834\n",
      "Epoch 356/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 37.2550 - val_loss: 42.5386\n",
      "Epoch 357/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 36.5613 - val_loss: 41.9307\n",
      "Epoch 358/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 36.1178 - val_loss: 42.4997\n",
      "Epoch 359/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 36.7376 - val_loss: 42.3330\n",
      "Epoch 360/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 35.8985 - val_loss: 41.9599\n",
      "Epoch 361/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 35.6354 - val_loss: 42.1045\n",
      "Epoch 362/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 35.4230 - val_loss: 42.0968\n",
      "Epoch 363/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 35.3495 - val_loss: 41.4373\n",
      "Epoch 364/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 35.1799 - val_loss: 41.9796\n",
      "Epoch 365/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 35.3342 - val_loss: 41.8752\n",
      "Epoch 366/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 35.1301 - val_loss: 41.5939\n",
      "Epoch 367/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 35.7180 - val_loss: 41.3775\n",
      "Epoch 368/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 35.1019 - val_loss: 41.0817\n",
      "Epoch 369/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 35.1980 - val_loss: 42.1242\n",
      "Epoch 370/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 35.2873 - val_loss: 41.8203\n",
      "Epoch 371/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 35.4632 - val_loss: 41.0672\n",
      "Epoch 372/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 34.8500 - val_loss: 40.7822\n",
      "Epoch 373/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 34.8712 - val_loss: 41.2890\n",
      "Epoch 374/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 35.1846 - val_loss: 41.7800\n",
      "Epoch 375/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 35.2912 - val_loss: 40.5785\n",
      "Epoch 376/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 34.4915 - val_loss: 40.4734\n",
      "Epoch 377/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 34.8804 - val_loss: 40.3918\n",
      "Epoch 378/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 34.3650 - val_loss: 40.1197\n",
      "Epoch 379/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 34.2924 - val_loss: 39.9119\n",
      "Epoch 380/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 34.4181 - val_loss: 40.7645\n",
      "Epoch 381/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 34.5882 - val_loss: 40.0170\n",
      "Epoch 382/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 34.5932 - val_loss: 40.2998\n",
      "Epoch 383/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 34.4132 - val_loss: 39.7296\n",
      "Epoch 384/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 34.1833 - val_loss: 39.8477\n",
      "Epoch 385/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 34.2350 - val_loss: 39.4254\n",
      "Epoch 386/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 34.2704 - val_loss: 39.7511\n",
      "Epoch 387/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 34.4462 - val_loss: 39.7975\n",
      "Epoch 388/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 34.2826 - val_loss: 39.1722\n",
      "Epoch 389/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 34.4357 - val_loss: 41.0001\n",
      "Epoch 390/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 34.7458 - val_loss: 40.2614\n",
      "Epoch 391/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 34.7162 - val_loss: 43.0230\n",
      "Epoch 392/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 35.2082 - val_loss: 40.6327\n",
      "Epoch 393/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 35.1430 - val_loss: 40.5063\n",
      "Epoch 394/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 34.8404 - val_loss: 38.9206\n",
      "Epoch 395/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 35.1756 - val_loss: 42.5448\n",
      "Epoch 396/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 34.9255 - val_loss: 39.4361\n",
      "Epoch 397/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 34.3220 - val_loss: 39.7189\n",
      "Epoch 398/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 33.9689 - val_loss: 39.0850\n",
      "Epoch 399/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 33.7137 - val_loss: 38.8396\n",
      "Epoch 400/2000\n",
      "6/6 [==============================] - ETA: 0s - loss: 33.5163\n",
      "Epoch 00400: saving model to saved_models/latent4/cp-0400.h5\n",
      "6/6 [==============================] - 1s 154ms/step - loss: 33.5163 - val_loss: 38.7729\n",
      "Epoch 401/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 33.4340 - val_loss: 38.7176\n",
      "Epoch 402/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 33.3198 - val_loss: 38.4267\n",
      "Epoch 403/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 33.2315 - val_loss: 38.1629\n",
      "Epoch 404/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 33.3942 - val_loss: 39.6225\n",
      "Epoch 405/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 33.7290 - val_loss: 38.9480\n",
      "Epoch 406/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 33.6275 - val_loss: 38.1611\n",
      "Epoch 407/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 33.3601 - val_loss: 39.7909\n",
      "Epoch 408/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 34.2409 - val_loss: 39.0273\n",
      "Epoch 409/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 33.8530 - val_loss: 38.5593\n",
      "Epoch 410/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 33.6295 - val_loss: 39.0529\n",
      "Epoch 411/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 33.4760 - val_loss: 38.2525\n",
      "Epoch 412/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 33.2572 - val_loss: 38.0510\n",
      "Epoch 413/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 33.4344 - val_loss: 39.8130\n",
      "Epoch 414/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 33.6174 - val_loss: 37.8105\n",
      "Epoch 415/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 33.3631 - val_loss: 38.1949\n",
      "Epoch 416/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 33.4663 - val_loss: 37.7211\n",
      "Epoch 417/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 33.2155 - val_loss: 38.7090\n",
      "Epoch 418/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 33.4293 - val_loss: 38.0561\n",
      "Epoch 419/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 32.9132 - val_loss: 37.1297\n",
      "Epoch 420/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 32.7501 - val_loss: 37.6192\n",
      "Epoch 421/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 33.0197 - val_loss: 38.2489\n",
      "Epoch 422/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 32.8665 - val_loss: 37.3790\n",
      "Epoch 423/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 32.9971 - val_loss: 36.7127\n",
      "Epoch 424/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 33.3349 - val_loss: 39.4828\n",
      "Epoch 425/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 33.3629 - val_loss: 37.2949\n",
      "Epoch 426/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 32.9565 - val_loss: 38.0706\n",
      "Epoch 427/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 33.2817 - val_loss: 37.5610\n",
      "Epoch 428/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 32.9773 - val_loss: 37.1529\n",
      "Epoch 429/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 32.5561 - val_loss: 37.7505\n",
      "Epoch 430/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 32.9604 - val_loss: 38.3586\n",
      "Epoch 431/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 32.9571 - val_loss: 37.5758\n",
      "Epoch 432/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 32.5734 - val_loss: 36.7997\n",
      "Epoch 433/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 32.4238 - val_loss: 37.4162\n",
      "Epoch 434/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 32.8490 - val_loss: 36.7686\n",
      "Epoch 435/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 32.6358 - val_loss: 37.2506\n",
      "Epoch 436/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 32.4412 - val_loss: 36.9403\n",
      "Epoch 437/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 32.7456 - val_loss: 36.7701\n",
      "Epoch 438/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 32.8095 - val_loss: 37.4911\n",
      "Epoch 439/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 33.4672 - val_loss: 37.6527\n",
      "Epoch 440/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 33.2409 - val_loss: 37.3434\n",
      "Epoch 441/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 32.8882 - val_loss: 37.5496\n",
      "Epoch 442/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 32.7298 - val_loss: 37.0050\n",
      "Epoch 443/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 33.0098 - val_loss: 36.5003\n",
      "Epoch 444/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 32.0786 - val_loss: 38.1166\n",
      "Epoch 445/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 33.2090 - val_loss: 36.0949\n",
      "Epoch 446/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 32.3690 - val_loss: 35.9564\n",
      "Epoch 447/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 32.2885 - val_loss: 36.6681\n",
      "Epoch 448/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 32.0944 - val_loss: 37.5388\n",
      "Epoch 449/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 32.2180 - val_loss: 36.3349\n",
      "Epoch 450/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 32.2221 - val_loss: 35.7541\n",
      "Epoch 451/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 32.0295 - val_loss: 35.9584\n",
      "Epoch 452/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 31.6978 - val_loss: 35.4268\n",
      "Epoch 453/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.8690 - val_loss: 36.2895\n",
      "Epoch 454/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 32.1101 - val_loss: 35.3397\n",
      "Epoch 455/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 31.6580 - val_loss: 36.3678\n",
      "Epoch 456/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 31.9385 - val_loss: 36.3444\n",
      "Epoch 457/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 31.8720 - val_loss: 35.8581\n",
      "Epoch 458/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 32.0231 - val_loss: 35.6626\n",
      "Epoch 459/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.8867 - val_loss: 36.0050\n",
      "Epoch 460/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 31.6454 - val_loss: 35.2465\n",
      "Epoch 461/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 31.6435 - val_loss: 35.9470\n",
      "Epoch 462/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.7456 - val_loss: 35.7268\n",
      "Epoch 463/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.9650 - val_loss: 35.2544\n",
      "Epoch 464/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.7660 - val_loss: 36.3154\n",
      "Epoch 465/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 32.0540 - val_loss: 36.1289\n",
      "Epoch 466/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.9831 - val_loss: 35.9249\n",
      "Epoch 467/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 32.2096 - val_loss: 36.3716\n",
      "Epoch 468/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 32.2412 - val_loss: 35.1797\n",
      "Epoch 469/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 31.5265 - val_loss: 35.2031\n",
      "Epoch 470/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.6706 - val_loss: 34.3285\n",
      "Epoch 471/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.6354 - val_loss: 37.0777\n",
      "Epoch 472/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 32.2709 - val_loss: 34.7264\n",
      "Epoch 473/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 32.2749 - val_loss: 36.2308\n",
      "Epoch 474/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 32.6085 - val_loss: 36.2071\n",
      "Epoch 475/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 32.4075 - val_loss: 36.2430\n",
      "Epoch 476/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 32.2804 - val_loss: 35.6776\n",
      "Epoch 477/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 32.3624 - val_loss: 36.1832\n",
      "Epoch 478/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 32.0397 - val_loss: 36.1628\n",
      "Epoch 479/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.8388 - val_loss: 35.3217\n",
      "Epoch 480/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.5672 - val_loss: 34.9635\n",
      "Epoch 481/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.6795 - val_loss: 35.2504\n",
      "Epoch 482/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.4758 - val_loss: 35.3486\n",
      "Epoch 483/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 31.2137 - val_loss: 34.7525\n",
      "Epoch 484/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.5390 - val_loss: 34.8996\n",
      "Epoch 485/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 31.4283 - val_loss: 34.4085\n",
      "Epoch 486/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.3408 - val_loss: 34.6050\n",
      "Epoch 487/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 31.1546 - val_loss: 34.3482\n",
      "Epoch 488/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.5313 - val_loss: 35.6873\n",
      "Epoch 489/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.3611 - val_loss: 34.1351\n",
      "Epoch 490/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.4335 - val_loss: 35.6262\n",
      "Epoch 491/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.5697 - val_loss: 35.3081\n",
      "Epoch 492/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.3097 - val_loss: 34.5989\n",
      "Epoch 493/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.2879 - val_loss: 35.6369\n",
      "Epoch 494/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.4050 - val_loss: 34.7713\n",
      "Epoch 495/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 31.5880 - val_loss: 34.5631\n",
      "Epoch 496/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.8228 - val_loss: 34.2607\n",
      "Epoch 497/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.8762 - val_loss: 34.3434\n",
      "Epoch 498/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.2392 - val_loss: 34.2999\n",
      "Epoch 499/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 30.9367 - val_loss: 35.0805\n",
      "Epoch 500/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 31.4551 - val_loss: 35.4258\n",
      "Epoch 501/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.8241 - val_loss: 34.5059\n",
      "Epoch 502/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.5489 - val_loss: 34.9859\n",
      "Epoch 503/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 31.4816 - val_loss: 34.0144\n",
      "Epoch 504/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 30.9168 - val_loss: 34.9550\n",
      "Epoch 505/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.4171 - val_loss: 34.8120\n",
      "Epoch 506/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.3610 - val_loss: 35.4266\n",
      "Epoch 507/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.7065 - val_loss: 34.7147\n",
      "Epoch 508/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.1873 - val_loss: 34.2952\n",
      "Epoch 509/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 30.8874 - val_loss: 34.0107\n",
      "Epoch 510/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.0890 - val_loss: 34.3994\n",
      "Epoch 511/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.9787 - val_loss: 34.9645\n",
      "Epoch 512/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.4124 - val_loss: 34.8930\n",
      "Epoch 513/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.6666 - val_loss: 34.4325\n",
      "Epoch 514/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 32.3786 - val_loss: 35.1072\n",
      "Epoch 515/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.5553 - val_loss: 34.5068\n",
      "Epoch 516/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 31.7450 - val_loss: 34.3954\n",
      "Epoch 517/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.2022 - val_loss: 33.9117\n",
      "Epoch 518/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.9980 - val_loss: 36.1372\n",
      "Epoch 519/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 32.4079 - val_loss: 34.9512\n",
      "Epoch 520/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.6363 - val_loss: 34.2692\n",
      "Epoch 521/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 31.7319 - val_loss: 35.1323\n",
      "Epoch 522/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.9294 - val_loss: 33.8035\n",
      "Epoch 523/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.0925 - val_loss: 34.0892\n",
      "Epoch 524/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.1505 - val_loss: 34.1396\n",
      "Epoch 525/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.2735 - val_loss: 34.8091\n",
      "Epoch 526/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.1576 - val_loss: 34.3721\n",
      "Epoch 527/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.1639 - val_loss: 33.8731\n",
      "Epoch 528/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.9536 - val_loss: 34.0497\n",
      "Epoch 529/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.6440 - val_loss: 36.2929\n",
      "Epoch 530/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 32.0882 - val_loss: 35.3795\n",
      "Epoch 531/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.4364 - val_loss: 34.8773\n",
      "Epoch 532/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.0858 - val_loss: 33.8180\n",
      "Epoch 533/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.9376 - val_loss: 34.2045\n",
      "Epoch 534/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 31.2257 - val_loss: 34.5216\n",
      "Epoch 535/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.6262 - val_loss: 34.2476\n",
      "Epoch 536/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.1580 - val_loss: 33.3640\n",
      "Epoch 537/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 30.7370 - val_loss: 33.5450\n",
      "Epoch 538/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.9465 - val_loss: 33.4415\n",
      "Epoch 539/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 30.5238 - val_loss: 33.8524\n",
      "Epoch 540/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.6969 - val_loss: 33.3285\n",
      "Epoch 541/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.7741 - val_loss: 33.3954\n",
      "Epoch 542/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.6211 - val_loss: 33.3279\n",
      "Epoch 543/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.5726 - val_loss: 32.9138\n",
      "Epoch 544/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 30.2848 - val_loss: 32.6037\n",
      "Epoch 545/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.3500 - val_loss: 33.3662\n",
      "Epoch 546/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.5060 - val_loss: 32.0134\n",
      "Epoch 547/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 29.9920 - val_loss: 31.6043\n",
      "Epoch 548/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 29.8604 - val_loss: 32.2439\n",
      "Epoch 549/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.2263 - val_loss: 32.6129\n",
      "Epoch 550/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.4808 - val_loss: 32.0616\n",
      "Epoch 551/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.2589 - val_loss: 32.1684\n",
      "Epoch 552/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.0176 - val_loss: 31.4544\n",
      "Epoch 553/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.0016 - val_loss: 32.2696\n",
      "Epoch 554/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.0024 - val_loss: 31.5736\n",
      "Epoch 555/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.9371 - val_loss: 33.0657\n",
      "Epoch 556/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.1497 - val_loss: 32.5298\n",
      "Epoch 557/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 29.3740 - val_loss: 31.0637\n",
      "Epoch 558/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.3869 - val_loss: 31.3597\n",
      "Epoch 559/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.5033 - val_loss: 32.0004\n",
      "Epoch 560/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 29.1917 - val_loss: 30.4534\n",
      "Epoch 561/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 29.0774 - val_loss: 30.9950\n",
      "Epoch 562/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.8027 - val_loss: 30.6025\n",
      "Epoch 563/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.8949 - val_loss: 28.8108\n",
      "Epoch 564/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.2661 - val_loss: 29.7464\n",
      "Epoch 565/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.1477 - val_loss: 29.0740\n",
      "Epoch 566/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.6656 - val_loss: 28.4190\n",
      "Epoch 567/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.2439 - val_loss: 28.5041\n",
      "Epoch 568/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.0990 - val_loss: 28.0728\n",
      "Epoch 569/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.1442 - val_loss: 27.2468\n",
      "Epoch 570/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.7276 - val_loss: 27.3965\n",
      "Epoch 571/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.0853 - val_loss: 27.7854\n",
      "Epoch 572/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.1195 - val_loss: 27.4188\n",
      "Epoch 573/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.2080 - val_loss: 27.4031\n",
      "Epoch 574/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.7868 - val_loss: 27.8043\n",
      "Epoch 575/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 26.9008 - val_loss: 27.1781\n",
      "Epoch 576/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.0488 - val_loss: 27.6519\n",
      "Epoch 577/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.7674 - val_loss: 26.9036\n",
      "Epoch 578/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.7998 - val_loss: 28.5192\n",
      "Epoch 579/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.6298 - val_loss: 26.8615\n",
      "Epoch 580/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.7376 - val_loss: 27.3611\n",
      "Epoch 581/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 26.6362 - val_loss: 27.3051\n",
      "Epoch 582/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.3139 - val_loss: 27.2687\n",
      "Epoch 583/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.2362 - val_loss: 27.0389\n",
      "Epoch 584/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.3818 - val_loss: 26.4290\n",
      "Epoch 585/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.1374 - val_loss: 26.9420\n",
      "Epoch 586/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 26.2707 - val_loss: 27.5300\n",
      "Epoch 587/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.3935 - val_loss: 27.0200\n",
      "Epoch 588/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.5201 - val_loss: 26.8452\n",
      "Epoch 589/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.2274 - val_loss: 26.9661\n",
      "Epoch 590/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 26.2958 - val_loss: 26.6678\n",
      "Epoch 591/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.0555 - val_loss: 26.3574\n",
      "Epoch 592/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.0356 - val_loss: 26.4153\n",
      "Epoch 593/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 26.1647 - val_loss: 28.3652\n",
      "Epoch 594/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.4754 - val_loss: 27.4972\n",
      "Epoch 595/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.9065 - val_loss: 26.9060\n",
      "Epoch 596/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.8122 - val_loss: 26.1032\n",
      "Epoch 597/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.5751 - val_loss: 26.7415\n",
      "Epoch 598/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.7715 - val_loss: 26.0640\n",
      "Epoch 599/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.5913 - val_loss: 26.0243\n",
      "Epoch 600/2000\n",
      "6/6 [==============================] - ETA: 0s - loss: 25.7033\n",
      "Epoch 00600: saving model to saved_models/latent4/cp-0600.h5\n",
      "6/6 [==============================] - 1s 165ms/step - loss: 25.7033 - val_loss: 26.7315\n",
      "Epoch 601/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.3549 - val_loss: 26.7823\n",
      "Epoch 602/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.5609 - val_loss: 25.9798\n",
      "Epoch 603/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 25.8351 - val_loss: 26.5616\n",
      "Epoch 604/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.9876 - val_loss: 27.2320\n",
      "Epoch 605/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.8510 - val_loss: 27.0946\n",
      "Epoch 606/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.0612 - val_loss: 27.1366\n",
      "Epoch 607/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.2897 - val_loss: 26.6845\n",
      "Epoch 608/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.0210 - val_loss: 26.6194\n",
      "Epoch 609/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.9131 - val_loss: 26.7054\n",
      "Epoch 610/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.8593 - val_loss: 25.7952\n",
      "Epoch 611/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 26.1449 - val_loss: 28.0316\n",
      "Epoch 612/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.1530 - val_loss: 28.1206\n",
      "Epoch 613/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.1270 - val_loss: 26.5696\n",
      "Epoch 614/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.5569 - val_loss: 26.5260\n",
      "Epoch 615/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.4917 - val_loss: 27.5467\n",
      "Epoch 616/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 25.6627 - val_loss: 26.1619\n",
      "Epoch 617/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.3112 - val_loss: 25.8212\n",
      "Epoch 618/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.1654 - val_loss: 26.2663\n",
      "Epoch 619/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.3386 - val_loss: 26.4578\n",
      "Epoch 620/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.2852 - val_loss: 25.7605\n",
      "Epoch 621/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.1465 - val_loss: 26.1420\n",
      "Epoch 622/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.0597 - val_loss: 26.3487\n",
      "Epoch 623/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.0013 - val_loss: 26.5266\n",
      "Epoch 624/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 25.4179 - val_loss: 25.9042\n",
      "Epoch 625/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.0789 - val_loss: 27.1768\n",
      "Epoch 626/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.8020 - val_loss: 26.9371\n",
      "Epoch 627/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.7888 - val_loss: 26.6416\n",
      "Epoch 628/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.3204 - val_loss: 26.5454\n",
      "Epoch 629/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 25.2610 - val_loss: 27.6312\n",
      "Epoch 630/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.4618 - val_loss: 27.3006\n",
      "Epoch 631/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 25.5516 - val_loss: 26.3696\n",
      "Epoch 632/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.2416 - val_loss: 26.8315\n",
      "Epoch 633/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.4266 - val_loss: 26.7714\n",
      "Epoch 634/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.5005 - val_loss: 26.6054\n",
      "Epoch 635/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.2779 - val_loss: 27.3991\n",
      "Epoch 636/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.2785 - val_loss: 26.6561\n",
      "Epoch 637/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.1451 - val_loss: 26.4179\n",
      "Epoch 638/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.9079 - val_loss: 27.5362\n",
      "Epoch 639/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.8386 - val_loss: 27.2139\n",
      "Epoch 640/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.4785 - val_loss: 26.4628\n",
      "Epoch 641/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 24.9120 - val_loss: 26.9502\n",
      "Epoch 642/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.8520 - val_loss: 26.3863\n",
      "Epoch 643/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.9932 - val_loss: 25.5860\n",
      "Epoch 644/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.4606 - val_loss: 26.2299\n",
      "Epoch 645/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.5777 - val_loss: 26.0467\n",
      "Epoch 646/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 24.5930 - val_loss: 25.8123\n",
      "Epoch 647/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 24.9397 - val_loss: 26.4160\n",
      "Epoch 648/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.4432 - val_loss: 26.4648\n",
      "Epoch 649/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.9226 - val_loss: 26.2452\n",
      "Epoch 650/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.9773 - val_loss: 27.1979\n",
      "Epoch 651/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 25.0975 - val_loss: 26.1853\n",
      "Epoch 652/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.0564 - val_loss: 25.9670\n",
      "Epoch 653/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.0537 - val_loss: 26.2415\n",
      "Epoch 654/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.6140 - val_loss: 25.8512\n",
      "Epoch 655/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.7143 - val_loss: 25.8404\n",
      "Epoch 656/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.5860 - val_loss: 25.5966\n",
      "Epoch 657/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 24.3075 - val_loss: 25.3949\n",
      "Epoch 658/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.6819 - val_loss: 27.3772\n",
      "Epoch 659/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 25.0419 - val_loss: 25.4341\n",
      "Epoch 660/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 24.6298 - val_loss: 25.7941\n",
      "Epoch 661/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.4075 - val_loss: 25.4793\n",
      "Epoch 662/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 24.2870 - val_loss: 25.6695\n",
      "Epoch 663/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.2886 - val_loss: 26.4735\n",
      "Epoch 664/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.6031 - val_loss: 26.1916\n",
      "Epoch 665/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.9321 - val_loss: 26.6638\n",
      "Epoch 666/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.2473 - val_loss: 28.2196\n",
      "Epoch 667/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.3538 - val_loss: 27.9717\n",
      "Epoch 668/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.5970 - val_loss: 27.7462\n",
      "Epoch 669/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.3104 - val_loss: 28.1495\n",
      "Epoch 670/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.1586 - val_loss: 26.5421\n",
      "Epoch 671/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.0712 - val_loss: 26.0476\n",
      "Epoch 672/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.7218 - val_loss: 25.9225\n",
      "Epoch 673/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.5905 - val_loss: 25.5083\n",
      "Epoch 674/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.4821 - val_loss: 26.0381\n",
      "Epoch 675/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.2391 - val_loss: 26.8886\n",
      "Epoch 676/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.7411 - val_loss: 26.3087\n",
      "Epoch 677/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.5672 - val_loss: 26.4876\n",
      "Epoch 678/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.4274 - val_loss: 26.2971\n",
      "Epoch 679/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 24.1925 - val_loss: 26.4278\n",
      "Epoch 680/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.2292 - val_loss: 25.6546\n",
      "Epoch 681/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.3965 - val_loss: 27.9128\n",
      "Epoch 682/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.8822 - val_loss: 25.7243\n",
      "Epoch 683/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.8080 - val_loss: 26.4112\n",
      "Epoch 684/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.0980 - val_loss: 25.2601\n",
      "Epoch 685/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.9869 - val_loss: 25.3479\n",
      "Epoch 686/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.1730 - val_loss: 26.0284\n",
      "Epoch 687/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.5842 - val_loss: 25.4674\n",
      "Epoch 688/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.5732 - val_loss: 26.9928\n",
      "Epoch 689/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.2283 - val_loss: 26.2153\n",
      "Epoch 690/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.4011 - val_loss: 25.7919\n",
      "Epoch 691/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.1933 - val_loss: 26.0853\n",
      "Epoch 692/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 24.1851 - val_loss: 26.6385\n",
      "Epoch 693/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.3074 - val_loss: 27.1244\n",
      "Epoch 694/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.2365 - val_loss: 25.4213\n",
      "Epoch 695/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.9167 - val_loss: 25.9204\n",
      "Epoch 696/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.0431 - val_loss: 25.5069\n",
      "Epoch 697/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.8331 - val_loss: 25.7698\n",
      "Epoch 698/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.9038 - val_loss: 25.4695\n",
      "Epoch 699/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.8962 - val_loss: 26.3922\n",
      "Epoch 700/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 24.1186 - val_loss: 25.8301\n",
      "Epoch 701/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.1912 - val_loss: 25.6086\n",
      "Epoch 702/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.8081 - val_loss: 26.1944\n",
      "Epoch 703/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.0797 - val_loss: 25.8520\n",
      "Epoch 704/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.1601 - val_loss: 25.4612\n",
      "Epoch 705/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.1678 - val_loss: 27.2247\n",
      "Epoch 706/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.6234 - val_loss: 25.8494\n",
      "Epoch 707/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.6513 - val_loss: 27.8223\n",
      "Epoch 708/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.8431 - val_loss: 27.4290\n",
      "Epoch 709/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.8365 - val_loss: 26.4523\n",
      "Epoch 710/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.2486 - val_loss: 26.1171\n",
      "Epoch 711/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.7276 - val_loss: 26.0162\n",
      "Epoch 712/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.4590 - val_loss: 27.6716\n",
      "Epoch 713/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 24.7572 - val_loss: 27.7966\n",
      "Epoch 714/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.9071 - val_loss: 27.2737\n",
      "Epoch 715/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 25.5342 - val_loss: 27.0480\n",
      "Epoch 716/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.0388 - val_loss: 27.6732\n",
      "Epoch 717/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.3655 - val_loss: 29.7179\n",
      "Epoch 718/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.4390 - val_loss: 28.7271\n",
      "Epoch 719/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.2099 - val_loss: 26.5679\n",
      "Epoch 720/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.4309 - val_loss: 26.5383\n",
      "Epoch 721/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.6498 - val_loss: 26.1457\n",
      "Epoch 722/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.0814 - val_loss: 26.6364\n",
      "Epoch 723/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.9005 - val_loss: 25.4209\n",
      "Epoch 724/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.6912 - val_loss: 25.6903\n",
      "Epoch 725/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.6542 - val_loss: 25.5404\n",
      "Epoch 726/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 23.8217 - val_loss: 25.1375\n",
      "Epoch 727/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.7091 - val_loss: 25.7544\n",
      "Epoch 728/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.9705 - val_loss: 26.2827\n",
      "Epoch 729/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.9951 - val_loss: 25.8878\n",
      "Epoch 730/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 23.7561 - val_loss: 26.3212\n",
      "Epoch 731/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.8292 - val_loss: 25.8647\n",
      "Epoch 732/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 24.1174 - val_loss: 26.7608\n",
      "Epoch 733/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.3753 - val_loss: 25.9029\n",
      "Epoch 734/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 24.0569 - val_loss: 25.7897\n",
      "Epoch 735/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 23.7013 - val_loss: 26.4385\n",
      "Epoch 736/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.7130 - val_loss: 25.8184\n",
      "Epoch 737/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.5881 - val_loss: 26.5447\n",
      "Epoch 738/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.7272 - val_loss: 26.8965\n",
      "Epoch 739/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.9183 - val_loss: 25.9872\n",
      "Epoch 740/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.5817 - val_loss: 25.8030\n",
      "Epoch 741/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.5927 - val_loss: 25.8375\n",
      "Epoch 742/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.5839 - val_loss: 26.0097\n",
      "Epoch 743/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.7030 - val_loss: 25.6298\n",
      "Epoch 744/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.6702 - val_loss: 26.3817\n",
      "Epoch 745/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.9872 - val_loss: 26.2898\n",
      "Epoch 746/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.9903 - val_loss: 26.4294\n",
      "Epoch 747/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.2834 - val_loss: 26.7632\n",
      "Epoch 748/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.2179 - val_loss: 25.5760\n",
      "Epoch 749/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.8976 - val_loss: 26.9004\n",
      "Epoch 750/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.8839 - val_loss: 25.5686\n",
      "Epoch 751/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.7478 - val_loss: 25.2536\n",
      "Epoch 752/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.6722 - val_loss: 25.8922\n",
      "Epoch 753/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.4526 - val_loss: 25.5223\n",
      "Epoch 754/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.4815 - val_loss: 25.3384\n",
      "Epoch 755/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.5483 - val_loss: 25.8201\n",
      "Epoch 756/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.1193 - val_loss: 25.7419\n",
      "Epoch 757/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.7999 - val_loss: 26.2084\n",
      "Epoch 758/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 24.2297 - val_loss: 26.1903\n",
      "Epoch 759/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.8904 - val_loss: 26.2311\n",
      "Epoch 760/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.6563 - val_loss: 25.8186\n",
      "Epoch 761/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.5786 - val_loss: 26.9108\n",
      "Epoch 762/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.5393 - val_loss: 25.7343\n",
      "Epoch 763/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.5522 - val_loss: 25.7847\n",
      "Epoch 764/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.4328 - val_loss: 25.5688\n",
      "Epoch 765/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.3183 - val_loss: 25.6615\n",
      "Epoch 766/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 23.4624 - val_loss: 26.4083\n",
      "Epoch 767/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.6610 - val_loss: 25.7824\n",
      "Epoch 768/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.3676 - val_loss: 25.7703\n",
      "Epoch 769/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.3926 - val_loss: 26.4411\n",
      "Epoch 770/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.7075 - val_loss: 26.5849\n",
      "Epoch 771/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 24.1728 - val_loss: 26.0538\n",
      "Epoch 772/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 24.0991 - val_loss: 26.5628\n",
      "Epoch 773/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.2623 - val_loss: 26.5682\n",
      "Epoch 774/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.0868 - val_loss: 26.8866\n",
      "Epoch 775/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.9317 - val_loss: 26.4653\n",
      "Epoch 776/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.7596 - val_loss: 25.6498\n",
      "Epoch 777/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.9562 - val_loss: 27.3228\n",
      "Epoch 778/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 23.9552 - val_loss: 26.3468\n",
      "Epoch 779/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 23.8690 - val_loss: 26.6834\n",
      "Epoch 780/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.9394 - val_loss: 26.5206\n",
      "Epoch 781/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.9559 - val_loss: 27.8193\n",
      "Epoch 782/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.7166 - val_loss: 27.1128\n",
      "Epoch 783/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.3278 - val_loss: 26.6891\n",
      "Epoch 784/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.7464 - val_loss: 26.1673\n",
      "Epoch 785/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.4679 - val_loss: 26.2253\n",
      "Epoch 786/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.5054 - val_loss: 26.2172\n",
      "Epoch 787/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.3729 - val_loss: 25.6596\n",
      "Epoch 788/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 23.3634 - val_loss: 25.2957\n",
      "Epoch 789/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 23.3965 - val_loss: 25.4911\n",
      "Epoch 790/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.2956 - val_loss: 25.8026\n",
      "Epoch 791/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.2152 - val_loss: 25.7820\n",
      "Epoch 792/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.1520 - val_loss: 25.2190\n",
      "Epoch 793/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 23.4917 - val_loss: 25.2770\n",
      "Epoch 794/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.5443 - val_loss: 25.7071\n",
      "Epoch 795/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.2444 - val_loss: 25.5841\n",
      "Epoch 796/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.1594 - val_loss: 25.9837\n",
      "Epoch 797/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.0631 - val_loss: 25.6414\n",
      "Epoch 798/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.0055 - val_loss: 26.0398\n",
      "Epoch 799/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.0774 - val_loss: 25.4552\n",
      "Epoch 800/2000\n",
      "6/6 [==============================] - ETA: 0s - loss: 22.9731\n",
      "Epoch 00800: saving model to saved_models/latent4/cp-0800.h5\n",
      "6/6 [==============================] - 1s 154ms/step - loss: 22.9731 - val_loss: 25.6569\n",
      "Epoch 801/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 23.1469 - val_loss: 26.3356\n",
      "Epoch 802/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.6212 - val_loss: 26.3616\n",
      "Epoch 803/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.0766 - val_loss: 27.1635\n",
      "Epoch 804/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.9279 - val_loss: 26.8774\n",
      "Epoch 805/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.9938 - val_loss: 26.8181\n",
      "Epoch 806/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 23.7283 - val_loss: 26.4855\n",
      "Epoch 807/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 23.8504 - val_loss: 25.7486\n",
      "Epoch 808/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 23.5837 - val_loss: 25.4978\n",
      "Epoch 809/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 23.9866 - val_loss: 28.1331\n",
      "Epoch 810/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.8133 - val_loss: 26.7611\n",
      "Epoch 811/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.5218 - val_loss: 27.1644\n",
      "Epoch 812/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.6194 - val_loss: 25.6067\n",
      "Epoch 813/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.4506 - val_loss: 26.4592\n",
      "Epoch 814/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 23.5408 - val_loss: 26.6297\n",
      "Epoch 815/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.8235 - val_loss: 25.8876\n",
      "Epoch 816/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.0436 - val_loss: 26.4136\n",
      "Epoch 817/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.1649 - val_loss: 26.0075\n",
      "Epoch 818/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.3532 - val_loss: 26.0089\n",
      "Epoch 819/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.4271 - val_loss: 25.9557\n",
      "Epoch 820/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.5658 - val_loss: 25.0176\n",
      "Epoch 821/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.4034 - val_loss: 25.4478\n",
      "Epoch 822/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.2282 - val_loss: 25.7879\n",
      "Epoch 823/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 23.1297 - val_loss: 25.5435\n",
      "Epoch 824/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 22.8879 - val_loss: 25.8677\n",
      "Epoch 825/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.9722 - val_loss: 25.1758\n",
      "Epoch 826/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 22.7974 - val_loss: 25.3304\n",
      "Epoch 827/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.9443 - val_loss: 26.1160\n",
      "Epoch 828/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.3559 - val_loss: 25.8332\n",
      "Epoch 829/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.5214 - val_loss: 26.6839\n",
      "Epoch 830/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.2851 - val_loss: 27.5061\n",
      "Epoch 831/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.3669 - val_loss: 26.5391\n",
      "Epoch 832/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 23.3688 - val_loss: 25.5652\n",
      "Epoch 833/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.6233 - val_loss: 25.8302\n",
      "Epoch 834/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.3877 - val_loss: 26.1344\n",
      "Epoch 835/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.5408 - val_loss: 26.9000\n",
      "Epoch 836/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 23.5074 - val_loss: 25.5154\n",
      "Epoch 837/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.9668 - val_loss: 26.1709\n",
      "Epoch 838/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.8392 - val_loss: 25.7862\n",
      "Epoch 839/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 23.2740 - val_loss: 25.4538\n",
      "Epoch 840/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.2773 - val_loss: 26.0995\n",
      "Epoch 841/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.4947 - val_loss: 25.8917\n",
      "Epoch 842/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.9508 - val_loss: 26.1525\n",
      "Epoch 843/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.2112 - val_loss: 25.7963\n",
      "Epoch 844/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.0453 - val_loss: 27.6196\n",
      "Epoch 845/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.4405 - val_loss: 26.3669\n",
      "Epoch 846/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.5315 - val_loss: 25.4055\n",
      "Epoch 847/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.4404 - val_loss: 25.2122\n",
      "Epoch 848/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.9502 - val_loss: 26.0672\n",
      "Epoch 849/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 23.0754 - val_loss: 25.0885\n",
      "Epoch 850/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.1393 - val_loss: 26.7433\n",
      "Epoch 851/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.5294 - val_loss: 26.9538\n",
      "Epoch 852/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 23.8874 - val_loss: 26.5009\n",
      "Epoch 853/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.7355 - val_loss: 25.8001\n",
      "Epoch 854/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.1442 - val_loss: 25.6067\n",
      "Epoch 855/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.0569 - val_loss: 26.2249\n",
      "Epoch 856/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 23.1133 - val_loss: 25.7658\n",
      "Epoch 857/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 23.2513 - val_loss: 25.6600\n",
      "Epoch 858/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.2193 - val_loss: 25.7600\n",
      "Epoch 859/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.1088 - val_loss: 25.8389\n",
      "Epoch 860/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.4254 - val_loss: 25.4026\n",
      "Epoch 861/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.1914 - val_loss: 25.6143\n",
      "Epoch 862/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.1228 - val_loss: 26.2876\n",
      "Epoch 863/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.1299 - val_loss: 25.9869\n",
      "Epoch 864/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.4773 - val_loss: 25.6192\n",
      "Epoch 865/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.6877 - val_loss: 25.6625\n",
      "Epoch 866/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.3948 - val_loss: 25.6473\n",
      "Epoch 867/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.2550 - val_loss: 25.9021\n",
      "Epoch 868/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.2884 - val_loss: 26.1628\n",
      "Epoch 869/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.1543 - val_loss: 25.7829\n",
      "Epoch 870/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.0254 - val_loss: 25.9541\n",
      "Epoch 871/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 23.0751 - val_loss: 26.2687\n",
      "Epoch 872/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.3878 - val_loss: 26.1643\n",
      "Epoch 873/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.1735 - val_loss: 25.7755\n",
      "Epoch 874/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.0690 - val_loss: 26.1409\n",
      "Epoch 875/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.8259 - val_loss: 25.4930\n",
      "Epoch 876/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.8073 - val_loss: 25.7169\n",
      "Epoch 877/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.3718 - val_loss: 26.7228\n",
      "Epoch 878/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.2271 - val_loss: 26.1640\n",
      "Epoch 879/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.1839 - val_loss: 25.6296\n",
      "Epoch 880/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.8208 - val_loss: 25.1585\n",
      "Epoch 881/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.9235 - val_loss: 25.6020\n",
      "Epoch 882/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.9143 - val_loss: 25.7435\n",
      "Epoch 883/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.9036 - val_loss: 26.0265\n",
      "Epoch 884/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.8903 - val_loss: 25.0622\n",
      "Epoch 885/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 22.7854 - val_loss: 25.5992\n",
      "Epoch 886/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.0614 - val_loss: 26.4687\n",
      "Epoch 887/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 23.4585 - val_loss: 26.1019\n",
      "Epoch 888/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 23.1722 - val_loss: 26.0791\n",
      "Epoch 889/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.9277 - val_loss: 25.9220\n",
      "Epoch 890/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.2884 - val_loss: 25.5039\n",
      "Epoch 891/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.7967 - val_loss: 25.1941\n",
      "Epoch 892/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.9184 - val_loss: 26.9031\n",
      "Epoch 893/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.1229 - val_loss: 27.1506\n",
      "Epoch 894/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.6115 - val_loss: 26.0591\n",
      "Epoch 895/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 23.5569 - val_loss: 25.5371\n",
      "Epoch 896/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.8474 - val_loss: 25.4216\n",
      "Epoch 897/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 22.9555 - val_loss: 26.5192\n",
      "Epoch 898/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.0577 - val_loss: 27.4037\n",
      "Epoch 899/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.9501 - val_loss: 26.5373\n",
      "Epoch 900/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.2714 - val_loss: 26.7141\n",
      "Epoch 901/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 23.2772 - val_loss: 26.1805\n",
      "Epoch 902/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.8618 - val_loss: 25.5318\n",
      "Epoch 903/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.7909 - val_loss: 25.4658\n",
      "Epoch 904/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 22.6390 - val_loss: 25.4520\n",
      "Epoch 905/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.7148 - val_loss: 26.1596\n",
      "Epoch 906/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 22.9775 - val_loss: 25.6867\n",
      "Epoch 907/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 23.1616 - val_loss: 26.0867\n",
      "Epoch 908/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.7509 - val_loss: 25.5723\n",
      "Epoch 909/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.8812 - val_loss: 25.5274\n",
      "Epoch 910/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.1226 - val_loss: 28.1820\n",
      "Epoch 911/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.6214 - val_loss: 26.2263\n",
      "Epoch 912/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 23.1504 - val_loss: 25.1956\n",
      "Epoch 913/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.8609 - val_loss: 25.8074\n",
      "Epoch 914/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.7565 - val_loss: 25.8865\n",
      "Epoch 915/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.8318 - val_loss: 25.9527\n",
      "Epoch 916/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 22.8951 - val_loss: 25.6186\n",
      "Epoch 917/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.7568 - val_loss: 25.2979\n",
      "Epoch 918/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.8281 - val_loss: 26.1739\n",
      "Epoch 919/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.0636 - val_loss: 25.3615\n",
      "Epoch 920/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.1441 - val_loss: 25.3024\n",
      "Epoch 921/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.6926 - val_loss: 25.4755\n",
      "Epoch 922/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.8979 - val_loss: 25.2446\n",
      "Epoch 923/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.7009 - val_loss: 25.8920\n",
      "Epoch 924/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.7130 - val_loss: 25.3593\n",
      "Epoch 925/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.8425 - val_loss: 25.2521\n",
      "Epoch 926/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 23.0795 - val_loss: 26.0607\n",
      "Epoch 927/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.0118 - val_loss: 25.3154\n",
      "Epoch 928/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.8661 - val_loss: 25.5501\n",
      "Epoch 929/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.7835 - val_loss: 25.9214\n",
      "Epoch 930/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.1123 - val_loss: 27.7303\n",
      "Epoch 931/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.6043 - val_loss: 25.5684\n",
      "Epoch 932/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.3298 - val_loss: 25.2969\n",
      "Epoch 933/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.1488 - val_loss: 25.9175\n",
      "Epoch 934/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.9985 - val_loss: 25.3254\n",
      "Epoch 935/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.8578 - val_loss: 25.7035\n",
      "Epoch 936/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 22.7779 - val_loss: 25.3001\n",
      "Epoch 937/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.7189 - val_loss: 26.1866\n",
      "Epoch 938/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 22.9015 - val_loss: 25.9384\n",
      "Epoch 939/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.1373 - val_loss: 26.0802\n",
      "Epoch 940/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 22.9464 - val_loss: 25.7998\n",
      "Epoch 941/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.0184 - val_loss: 25.1974\n",
      "Epoch 942/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 23.0628 - val_loss: 25.7767\n",
      "Epoch 943/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 23.2478 - val_loss: 25.6231\n",
      "Epoch 944/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 23.1437 - val_loss: 25.4932\n",
      "Epoch 945/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 23.4422 - val_loss: 26.2096\n",
      "Epoch 946/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.1001 - val_loss: 26.1272\n",
      "Epoch 947/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.0335 - val_loss: 26.0084\n",
      "Epoch 948/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.0716 - val_loss: 25.1843\n",
      "Epoch 949/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.3311 - val_loss: 27.9524\n",
      "Epoch 950/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.3916 - val_loss: 26.7206\n",
      "Epoch 951/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.1650 - val_loss: 26.2834\n",
      "Epoch 952/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.7959 - val_loss: 25.6200\n",
      "Epoch 953/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 22.5695 - val_loss: 25.7245\n",
      "Epoch 954/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.6971 - val_loss: 25.2485\n",
      "Epoch 955/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.8688 - val_loss: 25.1926\n",
      "Epoch 956/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.0382 - val_loss: 25.3830\n",
      "Epoch 957/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 22.8144 - val_loss: 26.1335\n",
      "Epoch 958/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.8916 - val_loss: 25.3747\n",
      "Epoch 959/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.9419 - val_loss: 26.1062\n",
      "Epoch 960/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.0464 - val_loss: 26.2020\n",
      "Epoch 961/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.9481 - val_loss: 25.5580\n",
      "Epoch 962/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.9414 - val_loss: 25.9587\n",
      "Epoch 963/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 22.7717 - val_loss: 26.3778\n",
      "Epoch 964/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.2486 - val_loss: 25.6159\n",
      "Epoch 965/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.9355 - val_loss: 25.5526\n",
      "Epoch 966/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.0150 - val_loss: 25.4594\n",
      "Epoch 967/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.1741 - val_loss: 25.9393\n",
      "Epoch 968/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.7277 - val_loss: 26.3268\n",
      "Epoch 969/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.7906 - val_loss: 25.2363\n",
      "Epoch 970/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.6204 - val_loss: 25.0132\n",
      "Epoch 971/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 22.7332 - val_loss: 25.9036\n",
      "Epoch 972/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 22.7105 - val_loss: 25.9486\n",
      "Epoch 973/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.6914 - val_loss: 26.6322\n",
      "Epoch 974/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.7871 - val_loss: 25.2527\n",
      "Epoch 975/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 22.8977 - val_loss: 24.6630\n",
      "Epoch 976/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.6691 - val_loss: 25.3225\n",
      "Epoch 977/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 22.4761 - val_loss: 25.8565\n",
      "Epoch 978/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.1238 - val_loss: 26.3263\n",
      "Epoch 979/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.2981 - val_loss: 26.6468\n",
      "Epoch 980/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.4990 - val_loss: 26.9654\n",
      "Epoch 981/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 23.5566 - val_loss: 25.8432\n",
      "Epoch 982/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 23.2154 - val_loss: 25.6559\n",
      "Epoch 983/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.1498 - val_loss: 26.4679\n",
      "Epoch 984/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.5828 - val_loss: 26.3350\n",
      "Epoch 985/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.1505 - val_loss: 26.4024\n",
      "Epoch 986/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.4504 - val_loss: 26.2161\n",
      "Epoch 987/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.1728 - val_loss: 26.5844\n",
      "Epoch 988/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.0189 - val_loss: 24.9058\n",
      "Epoch 989/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.6185 - val_loss: 24.8340\n",
      "Epoch 990/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.7325 - val_loss: 25.8094\n",
      "Epoch 991/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.0001 - val_loss: 25.8877\n",
      "Epoch 992/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.2291 - val_loss: 25.4830\n",
      "Epoch 993/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.8784 - val_loss: 25.9830\n",
      "Epoch 994/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.0753 - val_loss: 25.5127\n",
      "Epoch 995/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.7812 - val_loss: 25.8166\n",
      "Epoch 996/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.7213 - val_loss: 26.3392\n",
      "Epoch 997/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.5290 - val_loss: 25.7684\n",
      "Epoch 998/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 22.4304 - val_loss: 25.1201\n",
      "Epoch 999/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.6628 - val_loss: 25.5279\n",
      "Epoch 1000/2000\n",
      "6/6 [==============================] - ETA: 0s - loss: 22.6079\n",
      "Epoch 01000: saving model to saved_models/latent4/cp-1000.h5\n",
      "6/6 [==============================] - 1s 157ms/step - loss: 22.6079 - val_loss: 25.4553\n",
      "Epoch 1001/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.5730 - val_loss: 25.3406\n",
      "Epoch 1002/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.7274 - val_loss: 25.9431\n",
      "Epoch 1003/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.8641 - val_loss: 25.8099\n",
      "Epoch 1004/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 22.4381 - val_loss: 25.5752\n",
      "Epoch 1005/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.4734 - val_loss: 24.7780\n",
      "Epoch 1006/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 22.4127 - val_loss: 24.9454\n",
      "Epoch 1007/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.4459 - val_loss: 25.0426\n",
      "Epoch 1008/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.5257 - val_loss: 25.0734\n",
      "Epoch 1009/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.8156 - val_loss: 25.2532\n",
      "Epoch 1010/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.5546 - val_loss: 24.8661\n",
      "Epoch 1011/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.7675 - val_loss: 25.8431\n",
      "Epoch 1012/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.6295 - val_loss: 25.2569\n",
      "Epoch 1013/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 22.8002 - val_loss: 25.2075\n",
      "Epoch 1014/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.0568 - val_loss: 27.1875\n",
      "Epoch 1015/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 23.6959 - val_loss: 25.9936\n",
      "Epoch 1016/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.8766 - val_loss: 26.0892\n",
      "Epoch 1017/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 22.9660 - val_loss: 25.6053\n",
      "Epoch 1018/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.6306 - val_loss: 25.6241\n",
      "Epoch 1019/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.5414 - val_loss: 25.3523\n",
      "Epoch 1020/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.8444 - val_loss: 26.5638\n",
      "Epoch 1021/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.0461 - val_loss: 26.2564\n",
      "Epoch 1022/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.2703 - val_loss: 25.9246\n",
      "Epoch 1023/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.4435 - val_loss: 25.5634\n",
      "Epoch 1024/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.9050 - val_loss: 26.7666\n",
      "Epoch 1025/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 22.9306 - val_loss: 25.5784\n",
      "Epoch 1026/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.7796 - val_loss: 25.6067\n",
      "Epoch 1027/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.0279 - val_loss: 27.1607\n",
      "Epoch 1028/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.1772 - val_loss: 26.1296\n",
      "Epoch 1029/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 23.1690 - val_loss: 25.8029\n",
      "Epoch 1030/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.1295 - val_loss: 25.8926\n",
      "Epoch 1031/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.1699 - val_loss: 25.5025\n",
      "Epoch 1032/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.9509 - val_loss: 25.4733\n",
      "Epoch 1033/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.8653 - val_loss: 27.0317\n",
      "Epoch 1034/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.1822 - val_loss: 26.9518\n",
      "Epoch 1035/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 23.1153 - val_loss: 25.7667\n",
      "Epoch 1036/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 22.6475 - val_loss: 25.1099\n",
      "Epoch 1037/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.5120 - val_loss: 26.0095\n",
      "Epoch 1038/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 22.3800 - val_loss: 25.3063\n",
      "Epoch 1039/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.6151 - val_loss: 25.3172\n",
      "Epoch 1040/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 22.4837 - val_loss: 25.6125\n",
      "Epoch 1041/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 22.2721 - val_loss: 24.8867\n",
      "Epoch 1042/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 22.3410 - val_loss: 25.7720\n",
      "Epoch 1043/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 22.7672 - val_loss: 26.4634\n",
      "Epoch 1044/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.3191 - val_loss: 26.3369\n",
      "Epoch 1045/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.0569 - val_loss: 26.5070\n",
      "Epoch 1046/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.8846 - val_loss: 25.1475\n",
      "Epoch 1047/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.9524 - val_loss: 25.0035\n",
      "Epoch 1048/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.7793 - val_loss: 26.1479\n",
      "Epoch 1049/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 22.6677 - val_loss: 25.6630\n",
      "Epoch 1050/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 22.7954 - val_loss: 26.2362\n",
      "Epoch 1051/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.6010 - val_loss: 25.6559\n",
      "Epoch 1052/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 22.7276 - val_loss: 25.4060\n",
      "Epoch 1053/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.5012 - val_loss: 25.0749\n",
      "Epoch 1054/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.6459 - val_loss: 26.2964\n",
      "Epoch 1055/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.1138 - val_loss: 25.9594\n",
      "Epoch 1056/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.9319 - val_loss: 25.7216\n",
      "Epoch 1057/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.9305 - val_loss: 25.0011\n",
      "Epoch 1058/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 22.5844 - val_loss: 25.0101\n",
      "Epoch 1059/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.6265 - val_loss: 25.2194\n",
      "Epoch 1060/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 22.7268 - val_loss: 25.4546\n",
      "Epoch 1061/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.7473 - val_loss: 25.3043\n",
      "Epoch 1062/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.4772 - val_loss: 25.9450\n",
      "Epoch 1063/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.4617 - val_loss: 25.1301\n",
      "Epoch 1064/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.5573 - val_loss: 25.6234\n",
      "Epoch 1065/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.2767 - val_loss: 25.4791\n",
      "Epoch 1066/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.5610 - val_loss: 25.0577\n",
      "Epoch 1067/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.8463 - val_loss: 25.3577\n",
      "Epoch 1068/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.7983 - val_loss: 25.3741\n",
      "Epoch 1069/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 22.8737 - val_loss: 26.0701\n",
      "Epoch 1070/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 22.9252 - val_loss: 25.6984\n",
      "Epoch 1071/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.0441 - val_loss: 25.6690\n",
      "Epoch 1072/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.7505 - val_loss: 26.0436\n",
      "Epoch 1073/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 22.7079 - val_loss: 26.3903\n",
      "Epoch 1074/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.9371 - val_loss: 25.9153\n",
      "Epoch 1075/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.7661 - val_loss: 25.8733\n",
      "Epoch 1076/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.7136 - val_loss: 25.0058\n",
      "Epoch 1077/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.4338 - val_loss: 24.8551\n",
      "Epoch 1078/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.3070 - val_loss: 25.2991\n",
      "Epoch 1079/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.4632 - val_loss: 24.8679\n",
      "Epoch 1080/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.4172 - val_loss: 25.3034\n",
      "Epoch 1081/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 22.7436 - val_loss: 25.1369\n",
      "Epoch 1082/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.6032 - val_loss: 25.1301\n",
      "Epoch 1083/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.4638 - val_loss: 25.4356\n",
      "Epoch 1084/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 22.2249 - val_loss: 24.5681\n",
      "Epoch 1085/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.3413 - val_loss: 25.5698\n",
      "Epoch 1086/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 22.5318 - val_loss: 25.4722\n",
      "Epoch 1087/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.4326 - val_loss: 25.6550\n",
      "Epoch 1088/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 22.5822 - val_loss: 25.8313\n",
      "Epoch 1089/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 22.8722 - val_loss: 26.5264\n",
      "Epoch 1090/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.6558 - val_loss: 24.6433\n",
      "Epoch 1091/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.4982 - val_loss: 25.0948\n",
      "Epoch 1092/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.4974 - val_loss: 24.9438\n",
      "Epoch 1093/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.4885 - val_loss: 25.5011\n",
      "Epoch 1094/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.4279 - val_loss: 25.2951\n",
      "Epoch 1095/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.5845 - val_loss: 25.2340\n",
      "Epoch 1096/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.4833 - val_loss: 25.1113\n",
      "Epoch 1097/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.3934 - val_loss: 25.4371\n",
      "Epoch 1098/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 22.4622 - val_loss: 25.9242\n",
      "Epoch 1099/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 22.8493 - val_loss: 25.5597\n",
      "Epoch 1100/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.4888 - val_loss: 25.5905\n",
      "Epoch 1101/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.4852 - val_loss: 26.1353\n",
      "Epoch 1102/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.7774 - val_loss: 24.8990\n",
      "Epoch 1103/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.5729 - val_loss: 25.6320\n",
      "Epoch 1104/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.8940 - val_loss: 25.4841\n",
      "Epoch 1105/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.6267 - val_loss: 25.1546\n",
      "Epoch 1106/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 22.6831 - val_loss: 25.9250\n",
      "Epoch 1107/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 22.6621 - val_loss: 25.3587\n",
      "Epoch 1108/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.3109 - val_loss: 25.1631\n",
      "Epoch 1109/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.3151 - val_loss: 25.7790\n",
      "Epoch 1110/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.4973 - val_loss: 25.4261\n",
      "Epoch 1111/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.4973 - val_loss: 25.4223\n",
      "Epoch 1112/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.7913 - val_loss: 26.4677\n",
      "Epoch 1113/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.8183 - val_loss: 25.8969\n",
      "Epoch 1114/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.9407 - val_loss: 25.1406\n",
      "Epoch 1115/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.8951 - val_loss: 25.4907\n",
      "Epoch 1116/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 22.4483 - val_loss: 26.1095\n",
      "Epoch 1117/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.4088 - val_loss: 24.6209\n",
      "Epoch 1118/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.3938 - val_loss: 25.3985\n",
      "Epoch 1119/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.3043 - val_loss: 24.9999\n",
      "Epoch 1120/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 22.1476 - val_loss: 24.6844\n",
      "Epoch 1121/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.4253 - val_loss: 25.6209\n",
      "Epoch 1122/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.6070 - val_loss: 25.8075\n",
      "Epoch 1123/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.5795 - val_loss: 25.6103\n",
      "Epoch 1124/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.4009 - val_loss: 25.0767\n",
      "Epoch 1125/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 22.5119 - val_loss: 26.3064\n",
      "Epoch 1126/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 22.7358 - val_loss: 25.4842\n",
      "Epoch 1127/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 22.4126 - val_loss: 24.8928\n",
      "Epoch 1128/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.3074 - val_loss: 24.6825\n",
      "Epoch 1129/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.4133 - val_loss: 25.7830\n",
      "Epoch 1130/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.1921 - val_loss: 26.0922\n",
      "Epoch 1131/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.8289 - val_loss: 25.4045\n",
      "Epoch 1132/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 22.5274 - val_loss: 25.2530\n",
      "Epoch 1133/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.8268 - val_loss: 25.8385\n",
      "Epoch 1134/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.0711 - val_loss: 25.1060\n",
      "Epoch 1135/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 22.6452 - val_loss: 24.9808\n",
      "Epoch 1136/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.5674 - val_loss: 25.2395\n",
      "Epoch 1137/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.5563 - val_loss: 24.8429\n",
      "Epoch 1138/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.3580 - val_loss: 25.4351\n",
      "Epoch 1139/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 22.3498 - val_loss: 25.7507\n",
      "Epoch 1140/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.4644 - val_loss: 25.5532\n",
      "Epoch 1141/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.3831 - val_loss: 24.6811\n",
      "Epoch 1142/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.3095 - val_loss: 25.0596\n",
      "Epoch 1143/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.1986 - val_loss: 25.2073\n",
      "Epoch 1144/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.2110 - val_loss: 25.0535\n",
      "Epoch 1145/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 22.4814 - val_loss: 24.9739\n",
      "Epoch 1146/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 22.3537 - val_loss: 25.4001\n",
      "Epoch 1147/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.9487 - val_loss: 25.3935\n",
      "Epoch 1148/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 22.4244 - val_loss: 24.5034\n",
      "Epoch 1149/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.1593 - val_loss: 24.8668\n",
      "Epoch 1150/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.5110 - val_loss: 24.9746\n",
      "Epoch 1151/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.3862 - val_loss: 25.2050\n",
      "Epoch 1152/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.2536 - val_loss: 24.9410\n",
      "Epoch 1153/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.2113 - val_loss: 24.7342\n",
      "Epoch 1154/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.4183 - val_loss: 25.2333\n",
      "Epoch 1155/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 22.4135 - val_loss: 24.8819\n",
      "Epoch 1156/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.3598 - val_loss: 24.9342\n",
      "Epoch 1157/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 22.3999 - val_loss: 24.9919\n",
      "Epoch 1158/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 22.2722 - val_loss: 25.4276\n",
      "Epoch 1159/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 22.4465 - val_loss: 25.2400\n",
      "Epoch 1160/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.3407 - val_loss: 25.9764\n",
      "Epoch 1161/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.5588 - val_loss: 25.7533\n",
      "Epoch 1162/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.7529 - val_loss: 26.6370\n",
      "Epoch 1163/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.4340 - val_loss: 25.6230\n",
      "Epoch 1164/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 22.7489 - val_loss: 25.4861\n",
      "Epoch 1165/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.7399 - val_loss: 25.0049\n",
      "Epoch 1166/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.3900 - val_loss: 25.5336\n",
      "Epoch 1167/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.3641 - val_loss: 24.9979\n",
      "Epoch 1168/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.3396 - val_loss: 25.2570\n",
      "Epoch 1169/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 22.0225 - val_loss: 24.6931\n",
      "Epoch 1170/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.1491 - val_loss: 24.7411\n",
      "Epoch 1171/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.1588 - val_loss: 24.8097\n",
      "Epoch 1172/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 22.4352 - val_loss: 25.3808\n",
      "Epoch 1173/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.6152 - val_loss: 26.2416\n",
      "Epoch 1174/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.9428 - val_loss: 26.4956\n",
      "Epoch 1175/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.9013 - val_loss: 24.9715\n",
      "Epoch 1176/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 23.0059 - val_loss: 26.0187\n",
      "Epoch 1177/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.8292 - val_loss: 25.4908\n",
      "Epoch 1178/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.4119 - val_loss: 25.6721\n",
      "Epoch 1179/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.4063 - val_loss: 26.3712\n",
      "Epoch 1180/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.9406 - val_loss: 25.9327\n",
      "Epoch 1181/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.8565 - val_loss: 25.1018\n",
      "Epoch 1182/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.7493 - val_loss: 26.1631\n",
      "Epoch 1183/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 22.7500 - val_loss: 26.6441\n",
      "Epoch 1184/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.5718 - val_loss: 25.5770\n",
      "Epoch 1185/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.1520 - val_loss: 24.6844\n",
      "Epoch 1186/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 21.9067 - val_loss: 25.3148\n",
      "Epoch 1187/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.1292 - val_loss: 25.1440\n",
      "Epoch 1188/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.3148 - val_loss: 25.3350\n",
      "Epoch 1189/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.3265 - val_loss: 25.2389\n",
      "Epoch 1190/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.2522 - val_loss: 24.6695\n",
      "Epoch 1191/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.1794 - val_loss: 24.9056\n",
      "Epoch 1192/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.4399 - val_loss: 26.0170\n",
      "Epoch 1193/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.4268 - val_loss: 24.9828\n",
      "Epoch 1194/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 22.2570 - val_loss: 24.8715\n",
      "Epoch 1195/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 22.4850 - val_loss: 25.2219\n",
      "Epoch 1196/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.2702 - val_loss: 25.1957\n",
      "Epoch 1197/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.4436 - val_loss: 24.8655\n",
      "Epoch 1198/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.3252 - val_loss: 25.0906\n",
      "Epoch 1199/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.6390 - val_loss: 26.5547\n",
      "Epoch 1200/2000\n",
      "6/6 [==============================] - ETA: 0s - loss: 23.1111\n",
      "Epoch 01200: saving model to saved_models/latent4/cp-1200.h5\n",
      "6/6 [==============================] - 1s 166ms/step - loss: 23.1111 - val_loss: 27.2193\n",
      "Epoch 1201/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 23.5705 - val_loss: 25.1530\n",
      "Epoch 1202/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.8315 - val_loss: 25.1242\n",
      "Epoch 1203/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.3884 - val_loss: 24.6754\n",
      "Epoch 1204/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.2640 - val_loss: 25.6229\n",
      "Epoch 1205/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.3297 - val_loss: 25.2499\n",
      "Epoch 1206/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.6320 - val_loss: 25.0074\n",
      "Epoch 1207/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.2807 - val_loss: 25.0681\n",
      "Epoch 1208/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.2229 - val_loss: 25.1707\n",
      "Epoch 1209/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.4907 - val_loss: 25.2096\n",
      "Epoch 1210/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 22.3087 - val_loss: 24.6657\n",
      "Epoch 1211/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.1293 - val_loss: 24.6252\n",
      "Epoch 1212/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.1166 - val_loss: 24.8881\n",
      "Epoch 1213/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 22.1261 - val_loss: 24.9808\n",
      "Epoch 1214/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.0608 - val_loss: 24.6920\n",
      "Epoch 1215/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 21.9882 - val_loss: 25.0903\n",
      "Epoch 1216/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 22.2802 - val_loss: 25.0105\n",
      "Epoch 1217/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.0855 - val_loss: 25.0868\n",
      "Epoch 1218/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 22.2057 - val_loss: 25.5343\n",
      "Epoch 1219/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 22.2013 - val_loss: 25.9396\n",
      "Epoch 1220/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 22.2302 - val_loss: 25.5858\n",
      "Epoch 1221/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 22.3006 - val_loss: 25.8762\n",
      "Epoch 1222/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.5061 - val_loss: 25.1134\n",
      "Epoch 1223/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.2076 - val_loss: 25.0363\n",
      "Epoch 1224/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.5430 - val_loss: 25.9889\n",
      "Epoch 1225/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 22.0588 - val_loss: 25.0933\n",
      "Epoch 1226/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.0367 - val_loss: 24.6227\n",
      "Epoch 1227/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.3757 - val_loss: 25.3401\n",
      "Epoch 1228/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.2782 - val_loss: 25.3787\n",
      "Epoch 1229/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.3521 - val_loss: 25.3853\n",
      "Epoch 1230/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 22.1982 - val_loss: 24.7777\n",
      "Epoch 1231/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.2491 - val_loss: 26.2572\n",
      "Epoch 1232/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 22.4212 - val_loss: 25.1234\n",
      "Epoch 1233/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 22.6810 - val_loss: 27.0398\n",
      "Epoch 1234/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 22.9216 - val_loss: 26.1232\n",
      "Epoch 1235/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 22.5725 - val_loss: 24.6900\n",
      "Epoch 1236/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.6982 - val_loss: 26.3159\n",
      "Epoch 1237/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.7319 - val_loss: 24.9479\n",
      "Epoch 1238/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.9880 - val_loss: 26.6686\n",
      "Epoch 1239/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.7495 - val_loss: 25.5672\n",
      "Epoch 1240/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.4144 - val_loss: 25.5194\n",
      "Epoch 1241/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.2130 - val_loss: 25.3439\n",
      "Epoch 1242/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.1545 - val_loss: 25.2586\n",
      "Epoch 1243/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 22.2043 - val_loss: 26.1782\n",
      "Epoch 1244/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.4138 - val_loss: 25.4029\n",
      "Epoch 1245/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.2283 - val_loss: 25.2944\n",
      "Epoch 1246/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.2648 - val_loss: 25.4917\n",
      "Epoch 1247/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.5176 - val_loss: 25.2701\n",
      "Epoch 1248/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.4407 - val_loss: 24.7796\n",
      "Epoch 1249/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 22.4055 - val_loss: 25.3978\n",
      "Epoch 1250/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 22.3673 - val_loss: 25.6106\n",
      "Epoch 1251/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.5559 - val_loss: 25.6333\n",
      "Epoch 1252/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.2609 - val_loss: 25.2394\n",
      "Epoch 1253/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.1232 - val_loss: 25.1021\n",
      "Epoch 1254/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.1130 - val_loss: 25.2256\n",
      "Epoch 1255/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.3013 - val_loss: 25.5293\n",
      "Epoch 1256/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 22.3009 - val_loss: 24.6691\n",
      "Epoch 1257/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 22.4354 - val_loss: 24.2650\n",
      "Epoch 1258/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 22.0681 - val_loss: 24.3845\n",
      "Epoch 1259/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 21.9181 - val_loss: 24.7912\n",
      "Epoch 1260/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.0626 - val_loss: 25.2248\n",
      "Epoch 1261/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 21.9684 - val_loss: 25.8000\n",
      "Epoch 1262/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.2928 - val_loss: 25.0246\n",
      "Epoch 1263/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.1428 - val_loss: 24.8591\n",
      "Epoch 1264/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.2558 - val_loss: 25.8695\n",
      "Epoch 1265/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 22.5582 - val_loss: 25.4517\n",
      "Epoch 1266/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.5026 - val_loss: 25.3502\n",
      "Epoch 1267/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.2938 - val_loss: 25.2021\n",
      "Epoch 1268/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.4200 - val_loss: 25.6237\n",
      "Epoch 1269/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.5488 - val_loss: 24.8174\n",
      "Epoch 1270/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.1131 - val_loss: 24.9754\n",
      "Epoch 1271/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.2036 - val_loss: 25.3645\n",
      "Epoch 1272/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.1601 - val_loss: 24.7877\n",
      "Epoch 1273/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.0083 - val_loss: 24.9277\n",
      "Epoch 1274/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.1842 - val_loss: 25.0087\n",
      "Epoch 1275/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.1928 - val_loss: 25.1909\n",
      "Epoch 1276/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.3510 - val_loss: 25.7792\n",
      "Epoch 1277/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.3003 - val_loss: 25.2914\n",
      "Epoch 1278/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.6899 - val_loss: 25.1362\n",
      "Epoch 1279/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.3109 - val_loss: 24.8734\n",
      "Epoch 1280/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.0825 - val_loss: 24.8008\n",
      "Epoch 1281/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 22.1732 - val_loss: 25.0076\n",
      "Epoch 1282/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 22.0681 - val_loss: 25.3667\n",
      "Epoch 1283/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 21.9650 - val_loss: 24.9331\n",
      "Epoch 1284/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 22.1899 - val_loss: 25.3171\n",
      "Epoch 1285/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 21.9519 - val_loss: 24.6977\n",
      "Epoch 1286/2000\n",
      "6/6 [==============================] - ETA: 0s - loss: 22.0575Restoring model weights from the end of the best epoch.\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 22.0575 - val_loss: 24.9947\n",
      "Epoch 01286: early stopping\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Train VAE model with callbacks \"\"\"\n",
    "history = vae.fit(trainX, trainX, \n",
    "                  batch_size = batch_size, \n",
    "                  epochs = epochs, \n",
    "                  callbacks=[initial_checkpoint, checkpoint, EarlyStoppingAtMinLoss()],\n",
    "                  #callbacks=[initial_checkpoint, checkpoint],\n",
    "                  shuffle=True,\n",
    "                  validation_data=(valX, valX)\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step size: 6.0\n"
     ]
    }
   ],
   "source": [
    "# In this GPU implementation, it shows the number of batches/iterations for one epoch (and not the training samples), which is:\n",
    "print (\"Step size:\", np.ceil(trainX.shape[0]/batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Save the entire model \"\"\"\n",
    "# Save notes about hyperparameters used:\n",
    "with open(f'{SAVE_FOLDER}/notes.txt', 'w') as f:\n",
    "    f.write(f'Epochs: {epochs} \\n')\n",
    "    f.write(f'Batch size: {batch_size} \\n')\n",
    "    f.write(f'Latent dimension: {latent_dim} \\n')\n",
    "    f.write(f'Filter sizes: {filters} \\n')\n",
    "    f.write(f'img_width: {img_width} \\n')\n",
    "    f.write(f'img_height: {img_height} \\n')\n",
    "    f.write(f'num_channels: {num_channels}')\n",
    "    f.close()\n",
    "\n",
    "# Save weights for encoder model:\n",
    "encoder.save_weights(f'{SAVE_FOLDER}/ENCODER_WEIGHTS.h5')\n",
    "\n",
    "# Save weights for decoder model:\n",
    "decoder.save_weights(f'{SAVE_FOLDER}/DECODER_WEIGHTS.h5')\n",
    "\n",
    "# Save weights for total VAE:\n",
    "vae.save_weights(f'{SAVE_FOLDER}/VAE_WEIGHTS.h5')\n",
    "\n",
    "# Save the history at each epochs (cost of train and validation sets):\n",
    "np.save(f'{SAVE_FOLDER}/HISTORY.npy', history.history)\n",
    "\n",
    "# Save exact training, validation and testing data set (as numpy arrays):\n",
    "np.save(f'{SAVE_FOLDER}/trainX.npy', trainX)\n",
    "np.save(f'{SAVE_FOLDER}/valX.npy', valX)\n",
    "np.save(f'{SAVE_FOLDER}/testAllX.npy', testAllX)\n",
    "np.save(f'{SAVE_FOLDER}/testAllY.npy', testAllY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate training process\n",
    "\n",
    "Now that the training process is complete, lets evaluate the average loss of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAyEAAAGQCAYAAAC5528KAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/d3fzzAAAACXBIWXMAAAsTAAALEwEAmpwYAABnh0lEQVR4nO3deVxUVeMG8OfOwIgsMmziQimSQykiiIYKvApulVKauZV7qbyv1psZSJmae+CSb2plbmWp1U/RcqlMy1zIXMLMJTSXFJfYQbYZmLm/P6a5MiyuMHMZn+/nw0fn3jP3njtnYO4z55x7BVEURRAREREREVmIwtoVICIiIiKiBwtDCBERERERWRRDCBERERERWRRDCBERERERWRRDCBERERERWRRDCBERERERWRRDCBER0W0kJSXB398fv/zyi7WrYjHx8fHw9/e/5+enpaXB398fS5YsqcFaEZGtsLN2BYiI7lVeXh4iIiKg1WqRkJCAvn37WrtKsvfLL79g+PDhiIuLw4svvmjt6tyRtLQ0dOvWTXosCAKcnJzg6emJVq1aoWfPnujRowfs7Gz3I23JkiVYunTpHZXt168f3nnnnVquERHR/bHdv9hEZPO2bt0KnU4HHx8fbNq0iSHExoWFheGZZ54BABQVFeHy5cvYs2cPduzYgdatW2Pp0qVo0qRJrez7mWeeQe/evWFvb18r27+dHj164OGHHzZbNm/ePADAG2+8Yba8Yrl7NWvWLMyYMeOen9+0aVMcP34cSqWyRupDRLaFIYSI6qyNGzciNDQU3bp1w9y5c3H58mU89NBDVqmLKIooKiqCk5OTVfb/IGjevLkUQkzi4uLw8ccfY968eRg3bhw2b95coz0iBQUFcHZ2hlKptOrJ9KOPPopHH33UbNn//vc/AKj0mlSk1+uh0+lQv379u9rn/QYuQRBQr169+9oGEdkuzgkhojrp5MmTOH36NPr164c+ffrAzs4OGzdulNbr9XqEh4ejX79+VT7/888/h7+/P3bt2iUt0+l0+PDDD9G7d2+0adMG7du3R0xMDE6dOmX23F9++QX+/v5ISkrCunXr8NRTT6FNmzZYvXo1AOD48eOIj49Hr1690LZtWwQHB2Pw4MH4/vvvq6zLoUOHMGjQIAQGBiIsLAyzZ8/G2bNnqxxPL4oi1q9fj2effVba9rBhw3Dw4MF7eh1v5fDhwxg1ahRCQkIQGBiIfv364f/+7/8qlTt79ixeeeUVREREICAgAGFhYRg2bBj27NkjldFqtViyZIn0mrRv3x7R0dFISEi473qOHDkS0dHROHPmDLZv3y4tX7JkCfz9/ZGWllbpOVFRURg2bJjZMn9/f8THx+Pnn3/GkCFDEBwcjH//+98Aqp4TYlr2888/Y9WqVejevTsCAgLQq1cvbN68udI+9Xo9li1bhsjISLRp0wbR0dHYsWPHLet5t0x1Sk5OxrJly9C9e3cEBgbim2++AQDs378fr776Krp164bAwEC0b98eo0ePxqFDhyptq6o5IaZlN27cwPTp09GpUye0adMGgwcPxm+//WZWtqo5IeWX/fjjj+jfvz/atGmD8PBwJCQkoKysrFI9vvvuOzz99NNo06YNunbtiqVLlyI5OVn6HSSiuok9IURUJ23cuBGOjo7o2bMnHB0d0bVrV2zZsgX//e9/oVAooFQq8fTTT2PVqlU4e/YsWrZsafb8LVu2wM3NDV26dAEAlJaW4sUXX0RKSgqeeeYZvPDCCygoKMCXX36JIUOG4LPPPkObNm3MtvHJJ58gNzcXAwYMgJeXFxo1agQA+P7773H+/Hk88cQTaNq0KXJzc7F582ZMmDABCxYsQHR0tLSNI0eOYPTo0XB1dcXYsWPh4uKCb775Br/++muVxx0bG4vt27ejV69eePbZZ6HT6bB161aMHj0aS5YsMZs7cT9++OEHTJgwAZ6enhg1ahScnZ2xfft2vPXWW0hLS8PEiRMBADk5ORgxYgQAYPDgwWjSpAlycnJw4sQJ/Pbbb+jatSsAYMaMGdKQueDgYOj1ely8eLHGJnoPGDAAW7duxU8//XTbnoFbOXHiBL777jsMHDiw2gBb0bvvvouSkhIMGjQIKpUKGzZsQHx8PB5++GGEhIRI5WbOnInPP/8coaGhGD16NLKzszFjxgw0bdr0nutbHdMJ/cCBA+Hk5ARfX18AwObNm5GXl4e+ffuiUaNG+Pvvv/F///d/GDlyJNauXYv27dvf0fZffPFFuLu7Y/z48cjNzcWaNWswduxY7N69G87Ozrd9/k8//YT169dj8ODB6N+/P3bv3o3Vq1fD1dUVMTExUrkdO3bgtddew8MPP4wJEyZAqVRiy5Yt+OGHH+7thSEi+RCJiOqYkpISsX379uLkyZOlZd9//72o0WjEPXv2SMvOnDkjajQaMSEhwez5f/31l6jRaMRZs2ZJy9asWSNqNBpx7969ZmVv3LghdunSRRw6dKi07ODBg6JGoxE7dOggZmZmVqpfYWFhpWVFRUViz549xSeffNJsef/+/cWAgADx0qVL0jKdTicOGjRI1Gg04nvvvSct37lzp6jRaMTPP//cbBulpaViv379xMjISNFgMFTad3mmuq9cubLaMmVlZWLXrl3FkJAQ8fr169JyrVYrDho0SHz00UfFCxcuiKIoirt27RI1Go24ffv2W+63Q4cO4ksvvXTLMtW5fPmyqNFoxBkzZlRbJicnR9RoNGK/fv2kZe+9956o0WjEy5cvVyofGRlp1qaiKIoajUbUaDTigQMHKpXftGmTqNFoxIMHD1Za9swzz4harVZafv36dbF169bixIkTpWWm9+Lo0aNFvV4vLf/jjz/ERx99tNp63kpkZKQYGRlZZT179uwpFhUVVXpOVe/NjIwM8fHHH6/UPpMnTxY1Gk2Vy6ZPn262fMeOHaJGoxE3bNggLTO1W/n3sGlZ27ZtzY7XYDCIvXv3FsPCwqRlpaWlYnh4uNipUycxNzdXWl5QUCBGRUWJGo1G3LRpU1UvDRHVARyORUR1zs6dO5Gfn282Eb1Lly5wd3fHpk2bpGUtW7ZE69atsXXrVhgMBmn5li1bAMDs+V9//TVatGiB1q1bIzs7W/rR6XTo3Lkzjh49ipKSErN6PPPMM/Dw8KhUP0dHR+n/xcXFyMnJQXFxMTp27Ihz586hoKAAAJCZmYnff/8d3bp1M5vLYm9vj+HDh1fa7tdffw0nJyd0797drI75+fmIiorClStXcPHixTt6DW/l5MmTuHr1Kvr37w9vb29puUqlwksvvQSDwYDdu3cDAFxcXAAA+/btk46rKs7Ozvjzzz9x5syZ+65fddsHcMs63IlHH30UnTt3vqvnPP/881CpVNJjb29v+Pr6mrXFjz/+CAAYPnw4FIqbH73+/v4IDw+/rzpXZciQIVXOASn/3iwsLEROTg4UCgXatm2L48eP3/H2R44cafa4Y8eOAIC//vrrjp7frVs3+Pj4SI8FQUBoaCgyMjJQWFgIwPg+TE9PR79+/eDq6iqVdXJywuDBg++4rkQkTxyORUR1zsaNG+Hu7o5GjRqZnfSEhYXh22+/RXZ2Ntzd3QEYL1c6e/ZsJCcnIzw8HKIo4uuvv0bLli0REBAgPffcuXMoKSlBp06dqt1vTk4OGjduLD1u3rx5leWysrKwePFi7N69G1lZWZXW5+fnw9nZWZoDYBoqU16LFi0qLTt37hwKCwtveZKclZVV5fbuhqlejzzySKV1pmFtly9fBgA8/vjj6Nu3L5KSkrB161YEBASgc+fOeOqpp8ye/+abbyIuLg7R0dF46KGHEBoaisjISERFRZmdlN8rU/i4k6FAt1Jdm95KVRdDUKvVuHLlivTY9JpW1a6+vr7Yu3fvXe/3Vqp7D1y6dAnvvvsu9u/fj/z8fLN1giDc8fYrHrObmxsAIDc3956eDxhfM9M2nJycbvn7cb/vcSKyPoYQIqpTLl++jF9++QWiKKJXr15Vlvn666+lb2p79+6NhIQEbNmyBeHh4Th69CguX76M119/3ew5oihCo9FUutxpeaZgY1LVN82iKGL06NE4d+4chg8fjoCAALi4uECpVGLTpk3Ytm2bWa/M3RBFEe7u7li4cGG1ZSrOfbGEhIQEvPjii9i7dy+OHDmCNWvW4MMPP8Sbb76JoUOHAgC6d++OH374AT/99BMOHz6M5ORkbNy4Ee3bt8eaNWvMehLuRWpqKgDzk9NbnVRXNQEaqLpNb6cmQlRNc3BwqLSssLAQL7zwAoqLizFixAhoNBo4OTlBoVBg+fLld3Vxg+quFCaK4n09/262QUR1G0MIEdUpSUlJEEURs2fPloYClbd48WJs2rRJCiHu7u7417/+hV27dqGwsBBbtmyBQqHA008/bfa8Zs2aIScnBx07dryvk8rU1FT88ccfGD9+PF555RWzdRWvLGWakHzhwoVK2zl//nylZc2aNcPFixfRtm3bWr0UsGmYzJ9//llpnWlZxW+yNRoNNBoNXnrpJeTn52PAgAFYuHAhXnjhBSkMqNVqPPPMM3jmmWcgiiIWLFiAlStXYvfu3XjyySfvq86m19Z0oQEA0hCevLw8s6E/Wq0WGRkZaNas2X3t826Y9n/+/PlKr11V7V8bfv75Z6Snp2Pu3Lno37+/2brFixdbpA5341a/H5Z6zYio9sjv6xsiomoYDAZs3rwZGo0GAwYMwBNPPFHpp0+fPjhz5ozZ+PZ+/fqhuLgYX3/9Nb799lt07tzZbK4DYJwfkpGRgTVr1lS578zMzDuqoynAVPw298yZM5Uu0evl5YWAgADs3r1bGt4EGK/UtXbt2krb7tu3LwwGAxYtWnRfdbyd1q1bo0mTJkhKSkJGRoZZvVatWgVBEKSrcOXm5lbq2WnQoAF8fHxQXFwMrVYLvV5f5dCfVq1aATCGhPvxySefYOvWrfD398dTTz0lLTcNrUpOTjYr//HHH99zb9S9ioyMBACsXbvWbN+pqanYv3+/Repg6n2o+N7cv39/pcvrykFAQAC8vLykK3qZFBYW4vPPP7dizYioJrAnhIjqjP379+PatWt47rnnqi3Ts2dPLFmyBBs3bkRgYCAA47fjarUaCxYsQEFBQZWXXh0+fDiSk5ORmJiIgwcPomPHjnB2dsbVq1dx8OBBqFQqfPrpp7eto5+fH1q2bImVK1eipKQEvr6+uHDhAr744gtoNBqcPHnSrPzkyZMxevRoDB48GEOGDJEu0VtaWgrAfEjRE088gWeffRafffYZTp48icjISLi5ueH69es4duwY/vrrL2nC+O38/PPP0Gq1lZa7ublhyJAhmDp1KiZMmIDnnntOuszrN998g2PHjiEmJkY6wd+yZQs++eQTdO/eHc2aNYOdnR0OHz6M/fv348knn4SDgwPy8/MRHh6OqKgotGrVCu7u7khLS8OGDRvg6uoqnaDfzsWLF/HVV18BAEpKSnDp0iXs2bMHf/75J1q3bo3333/f7EaFnTt3hq+vL9577z3k5ubCx8cHR48exW+//SbNYbCUli1bYtCgQfjiiy8wcuRI9OjRA9nZ2Vi/fj0ee+wxnDx58q7mZNyLkJAQeHl5ISEhAVeuXEGjRo1w+vRpfPXVV9BoNLV20YB7ZWdnh8mTJ+P111/HgAED8Nxzz0GpVGLz5s1Qq9VIS0ur9deMiGoPQwgR1RmmmxH26NGj2jIajQbNmzfHjh078Oabb8LBwQEqlQp9+vTBZ599BmdnZ3Tv3r3S8+zt7bF8+XKsX78eX331lXSDtYYNG6JNmzZ3fM8IpVKJ5cuXIyEhAZs3b0ZxcTFatmyJhIQE/PHHH5VCyOOPP44VK1bg3XffxfLly9GgQQM8+eSTiI6OxsCBAyvdcXrevHkIDQ3Fl19+ieXLl6O0tBReXl5o1aoVJk2adEd1BIxXs9q3b1+l5b6+vhgyZAiioqLw8ccf44MPPsCqVatQWloKPz8/zJ49GwMGDJDKh4aG4vTp09izZw8yMjKgUCjg4+ODyZMnS/NBHBwcMGLECPz888/4+eefUVhYiIYNGyIqKgrjxo2r1CtVnQMHDuDAgQMQBAGOjo7ScU+YMAE9evSodKd0pVKJDz74ALNnz8Znn30Ge3t7hIWF4bPPPsOQIUPu+LWqKdOnT0fDhg2xceNGJCQkwNfXF9OnT8fvv/+OkydPVjmPoyY1aNAAK1euxPz58/HZZ5+hrKwMAQEBWLFiBTZu3Ci7EAIA0dHRsLOzw/vvv4/33nsPnp6eeO655+Dv748JEybwjuxEdZggcgYYEZHsfPfdd3jllVewaNEi9O7d29rVoVoUExODgwcP4ujRo7ecsE03rV69GgkJCfjiiy8QFBRk7eoQ0T3gnBAiIisSRbHSsKjS0lKsWbMGdnZ2ePzxx61UM6ppFe8zAwB//PEH9u7di44dOzKAVEGn00Gv15stKywsxLp166BWq6V5RURU93A4FhGRFel0OkRGRiI6Ohq+vr7Izc3Fjh07kJqaijFjxsDLy8vaVaQasnnzZnz11VfSjTXPnz+PL7/8Evb29pWupEZGly9fxpgxY9C7d2/4+PggIyMDmzdvRlpaGt5+++37vrQzEVkPQwgRkRXZ2dmhS5cu2L17NzIyMiCKInx9fTFt2jS88MIL1q4e1aDWrVtj165d+PTTT5GXlwcnJyeEhoZiwoQJ/Ea/Gu7u7ggKCsLWrVuRlZUFOzs7aDQaTJo0yexKaERU93BOCBERERERWRTnhBARERERkUVxOFYFBoMBer11O4eUSsHqdaCqsW3ki20jT2wX+WLbyBfbRp7YLnfP3r76C24whFSg14vIzS2yah3Uaker14GqxraRL7aNPLFd5IttI19sG3liu9w9Ly+XatdxOBYREREREVkUQwgREREREVkUQwgREREREVkUQwgREREREVkUQwgREREREVkUQwgREREREVkUL9FLRERERFUqLi5EQUEu9Poya1fF6v7+W4Ao8j4hAKBU2sHZWY369Z3ueRsMIURERERUSXFxIW7cyIFa7QV7exUEQbB2laxKqVRArzdYuxpWJ4oiSkt1yM3NAIB7DiIcjkVERERElRQU5EKt9oJKVe+BDyB0kyAIUKnqQa32QkFB7j1vhyGEiIiIiCrR68tgb6+ydjVIpuztVfc1TI8hhIiIiIiqxB4Qqs79vjcsGkLWrVuH6OhotGvXDu3atcOgQYOwZ88eaX18fDz8/f3NfgYOHGi2DZ1Oh1mzZiE0NBRBQUGIiYnB9evXzcpcvXoVMTExCAoKQmhoKGbPng2dTmeJQ7xvBgPwxx/WrgURERERUe2x6MR0b29vvP7662jevDkMBgO2bNmC8ePHY9OmTXj00UcBAJ07d0ZiYqL0HHt7e7NtzJkzB7t378aiRYugVqvxzjvvYNy4cUhKSoJSqYRer8e4ceOgVquxbt065ObmYvLkyRBFEVOnTrXk4d6TffuUGDhQgcOHBTz8MK/AQERERES2x6I9Id27d0eXLl3QrFkz+Pr6YuLEiXBycsKxY8ekMiqVCl5eXtKPWq2W1t24cQObNm1CXFwcwsLC0Lp1ayQmJiI1NRXJyckAgP379+Ps2bNITExE69atERYWhtjYWHz55ZcoKCiw5OHek6IiAaIoIDeX3Z9ERERENWnv3j34/PPPany7c+a8jeeei67x7doyq80J0ev12L59O4qKihAcHCwtP3r0KDp16oRevXrhrbfeQlZWlrTuxIkTKC0tRXh4uLSscePG8PPzQ0pKCgDg2LFj8PPzQ+PGjaUyERER0Ol0OHHihAWO7P6oVMbej9JSK1eEiIiIyMbs27cHX3yxvsa3O3LkS5g7d36Nb9eWWfw+IampqRg8eDC0Wi0cHR2xdOlS+Pv7AzCGhR49esDHxwdXrlzB4sWLMWLECCQlJUGlUiEzMxNKpRJubm5m2/Tw8EBmZiYAIDMzEx4eHmbr3dzcoFQqpTK3olQKUKsda+ho757p0OrVc0C5TiCSCaVSYdX3B1WPbSNPbBf5YtvIl1za5u+/BSiVtnUNI9Nk6tsdl06ng0pV+cpg1T3v4Ycfvv/K1UGCcO/nzRYPIb6+vtiyZQtu3LiB7777DpMnT8ann34KjUaD3r17S+X8/f3RunVrREVFYc+ePejZs6dF6qfXi8jNLbLIvqqi0ykBOCI7W4vcXL3V6kFVU6sdrfr+oOqxbeSJ7SJfbBv5kkvbiKJoUzfnmzPnbXzzzTYAQKdO7QAAjRo1xptvTscrr8RgzpxEHDyYjH379qCsrAzffrsHaWmXsWbNRzh+/DdkZWXBw8MToaEdMXbseDRo0MBs2ykpR7Fx41YAwLVrVzFgwNN4/fU3kJmZga1bN0Or1SIwMBivvx6Phg29LX34tUIUb33e7OXlUu06i4cQlUqFZs2aAQACAgLw+++/4+OPP8bcuXMrlfX29oa3tzcuXrwIAPD09IRer0dOTg7c3d2lcllZWWjfvr1U5tdffzXbTk5ODvR6PTw9PWvpqGqOnR2HYxEREZE8ffGFHTZssL99wVo0ZEgpBg26+/tTjBz5EnJzc3D69Cm8884iAIBKZS/NGX733fno2LEz3nprpnRV1czMDDRs2AivvNINrq6uSEtLw9q1a3D27H+xfPma2+7zs88+RkBAIOLjpyE3NwdLl76LmTOnYunSj+66/rbG4iGkIoPBUO3lc7Ozs5Geno6GDRsCMIYWe3t7HDhwANHRxsk/169fx7lz56R5JUFBQfjggw9w/fp1NGrUCABw4MABqFQqBAQEWOCI7o+p50+n48R0IiIioprStKkP1Go32NvbIyCgjbT811+PAAAee6w14uPNr6QaFNQOQUHGXhOlUoFWrdqgadOHMH78Szhz5g9oNI/ecp+NGjXG22/PkR7n5OTg/ff/h8zMDHh6etXUodVJFg0hCxYsQNeuXdGoUSMUFhZi27ZtOHToEJYvX47CwkIsXboUPXv2hJeXF65cuYJFixbB3d0d3bt3BwC4uLigf//+mD9/Pjw8PKBWqzFv3jz4+/ujc+fOAIDw8HC0bNkScXFxiI+PR25uLhITEzFw4EA4Oztb8nDvSV6eMXwUFlq5IkREREQVDBpUdk+9EHXBv/7VtdKy0tJSbNjwKb79djuuX78OnU4rrbt06a/bhpBOncLMHvv5PQLA+CU6Q4gFZWZmIjY2FhkZGXBxcYG/vz9WrFiBiIgIlJSU4MyZM9J8ES8vL4SGhmLx4sVm4WHKlCmws7PDxIkTUVJSgk6dOiExMRFKpRIAoFQqsXz5csyYMQNDhgyBg4MDoqOjERcXZ8lDvQ/G4VhF1h8KSkRERPTAqGrY/ocfLsWmTV9g5MiX0LZtEBwc6iM9PR1TpsTe0Y2wGzRwNXtsuv9d+TDzoLJoCHnnnXeqXefg4IBVq1bddhsqlQpTp0695Y0HmzRpguXLl99THa3NdG/GOnKDdyIiIiIbUXko/O7dO/HEE70xcuRLUCoV0OsNKC4utkLdbI9tXXfNBtwMIZwTQkRERFST7O3todXeeS9ESUkJ7OzMv7Pfvv3rmq7WA8nqE9PJnCmE8OpYRERERDWrefMWyM/fjM2bN+LRRx+DSlXvluVDQzvhm2+2oUWLR/Dwww/jxx9348SJ4xaqrW1jCJGZevV4iV4iIiKi2hAd3RcnT/6O5cuXoaDghnSfkOpMnBgHQMRHH70PQQA6dgzD22/PwZgxIyxXaRsliKIoWrsSclJaqrfqDYLOnxfQsaMzXn1Vizff5MQQuZHLDaSoMraNPLFd5IttI19yaZvr1/9Co0bNrF0N2TDNCaGbbvceudXNCjknRGbq/dMryJ4QIiIiIrJVDCEyY5z7JDKEEBEREZHNYgiRITs7XqKXiIiIiGwXQ4jMCAKgVAKlpbxELxERERHZJoYQmREEY08Ih2MRERERka1iCJEZUwgpK7N2TYiIiIiIagdDiMyYhmPxjulEREREZKsYQmRGEMR/5oRYuyZERERERLWDIURmbk5Mt3ZNiIiIiIhqB0OIzJhCCOeEEBEREZGtYgiRGUEw/hgMnBNCREREJEfXrl1FeHh77NixVVo2Z87beO656Ns+d8eOrQgPb49r167e1T5v3LiBVauWIzX1j0rrJkwYiwkTxt7V9qzNztoVIHOKf2KhKFq3HkRERER050aOfAkDBgyute0XFNzAmjUr0LChN/z9HzVbN2lSfK3tt7YwhMiQQsEQQkRERFSXNG3qY7V9+/q2sNq+7xWHY8mM8M8oLIPBuvUgIiIisiU//LAL4eHt8eefZyute/31VzBixBAAwKZNX2DcuFF48skoPPFEV4wdOxLJyftvu/2qhmNduZKG2Nj/olu3MPTp0x2LFy+ATqer9Nxdu77DK6/EoE+f7ujRIwKjRj2Pb77ZJq2/du0qBgx4GgCQkDAb4eHtzYaDVTUc69Kli3jjjdfxxBNdERUVhrFjR+LgwWSzMqtWLUd4eHtcvnwJsbH/RY8eEejfvw/WrFkBQy2fjLInRGZMc0LYE0JERERyc+6cgD//tO532I88YoCf392fKIWFRcDZ2Rk7d+7AI4/8V1qenZ2Fw4d/QUzMywCAa9euITr6GTRq1AR6vR4HDuxFXNyrWLRoCR5/vNMd76+0tBQTJ46HVqvFa69NhpubO776ahP27v2xUtmrV6+ga9duGDp0JARBwG+/peCdd2ZBqy1B377PwcPDE3PmzMeUKbEYNmwUwsL+BaD63pfMzAz85z8voX59J0ycGAcnJ2ckJf0f4uJeRULCu+jUKcys/Jtvvo6nnnoaAwc+jwMH9mHVquVo2NAbvXs/fcfHe7cYQmTm5sR0a9eEiIiIyHbUq1cPkZHd8f333yEm5mUo/pmIu2vXdwCAHj2eAABMmPCq9ByDwYCQkA64fPkSNm/eeFch5JtvtuHq1Sv48MM1CAhoAwDo2LEzhg+vPG9k+PDRZvsMDg5BVlYmNm/ehL59n4NKpYJG4w8AaNKkqbS96nz++TrcuHEDH364Bj4+DwEAOnUKw9ChA7BixfuVQsjgwUOlwNGhQyh+/fUwdu36jiHkQcKeECIiIpIrPz8Rfn56a1fjnj3xRG9s3boFR48eRocOoQCAb7/dgZCQDvD09AQA/PHHaaxevRynT59Cbm4OxH9Oyh5+uPld7evEieNo2NDbLDAoFApERXXH6tUfmZW9fPkSVq78EL/9loLs7CxpKJRKpbqn4/ztt1/RqlWAFEAAQKlUonv3Xvj445UoLCyAk5OztK5z53Cz5/v6+uHs2dR72vedYgiRGYYQIiIiotoRGBiExo2b4LvvdqBDh1BcvHgBZ878gWnTZgEA/v77Ol599d9o3rwFXn01Ft7ejWBnp8SKFR/ir78u3NW+srKy4O7uUWm5u7u72eOioiJMnDgeDg4OiImZgKZNfWBvb4/Nmzdi+/av7+k48/Pz0bKlf6XlHh4eEEURN27cMAshLi4NzMqpVKoq567UJIYQmTFNTGcIISIiIqpZgiCgZ88n8eWXG/D662/gu+92oH59R/zrX5EAgF9++RkFBQWYOXMeGjb0lp6n1Zbc9b48PDxw4cK5Ssuzs7PNHp88eRzXr1/DsmUr0bZtkLRcr7/3HqcGDRogOzur0vKsrCwIggAXF5d73nZN4dWxZEYQeIleIiIiotrSq9dTKC4uwk8//YCdO79Bly6RcHBwAACUlBjDhp3dze/pL136C7///ttd7ycgIBDp6X/jxInfpWUGgwE//LDLrFxV+8zPz8f+/T+ZlbO3Nw7NupNAFBQUgpMnfze7IaJer8cPP3yPli39zXpBrIU9ITLDS/QSERER1Z6HH26GVq0C8OGHS5GRkY4nnugtrWvf/nEolUrMnj0dgwcPRVZW5j9XimoEUby7k7Mnn+yDzz77GFOmxGLcuPFwc3PDli2bUFRUaFYuIKAtnJycsGhRAl58cRyKi4uxdu0quLqqUVBQIJVzd3eHq6srdu/eCT+/lqhfvz4aN24CV1d1pX0PGvQ8vvlmKyZOHI/Ro8fByckJmzf/Hy5fvoTExMV3dRy1hT0hMsM5IURERES1q1evp5CRkQ4vr4Zo1669tLxFCz9MmzYb169fQ3z8a1i3bi1iYiYgKCj4rvdhb2+Pd99dhpYtNVi48B3MmfM2GjduanYlLABwc3PD3LkLYDDo8dZbk7F8+VL06dMXPXs+aVZOoVBg8uSpuHHjBl599T946aXhOHBgX5X79vT0wvvvr4SvbwssXDgPU6dORn5+PhITF6Njx853fSy1QRBFnu6WV1qqR25ukdX2r9cDXbo4Q6XS44cfiq1WD6qaWu1o1fcHVY9tI09sF/li28iXXNrm+vW/0KhRM2tXQzaUSgX0eg5VKe927xEvr+rnnrAnRGbYE0JEREREto4hRGYUClMIEaxdFSIiIiKiWsEQIlOcmE5EREREtoohRIYUbBUiIiIismEWPd1dt24doqOj0a5dO7Rr1w6DBg3Cnj17pPWiKGLJkiUIDw9HYGAghg0bhrNnz5ptIy8vD7GxsQgJCUFISAhiY2ORn59vViY1NRVDhw5FYGAgIiIisHTpUtSl+feCwJ4QIiIisr66dP5ElnW/7w2LhhBvb2+8/vrr2Lx5MzZt2oSOHTti/Pjx+OOPPwAAK1aswOrVqzF16lRs3LgR7u7uGDVqlNk1kidNmoRTp05h5cqVWLlyJU6dOoW4uDhpfUFBAUaPHg0PDw9s3LgRU6ZMwapVq7BmzRpLHup94cR0IiIisjal0g6lpTprV4NkqrRUB6Xy3m85aNEQ0r17d3Tp0gXNmjWDr68vJk6cCCcnJxw7dgyiKGLt2rUYO3YsevXqBY1Gg4SEBBQWFmLbtm0AgHPnzmHfvn2YOXMmgoODERwcjBkzZuDHH3/E+fPnAQBff/01iouLkZCQAI1GgyeeeAJjxozBmjVr6kyaZwghIiIia3N2ViM3NwM6nbbOnENR7RNFETqdFrm5GXB2Vt/zdqx2x3S9Xo9vv/0WRUVFCA4ORlpaGjIyMhAWFiaVcXBwQIcOHZCSkoLBgwcjJSUFjo6OaNeunVQmJCQEjo6OSElJQYsWLXDs2DG0b98eDg4OUpnw8HD873//Q1paGh566CGLHue94HAsIiIisrb69Z0AAHl5mdDry6xcG+sTBIFh7B9KpR1cXNyk98i9sHgISU1NxeDBg6HVauHo6IilS5fC398fv/76KwDA09PTrLyHhwfS09MBAJmZmXB3d4cg3Lx8rSAIcHd3R2ZmplTG29vbbBumbWZmZt42hCiVAtRqx/s7yPukUBjvimntelBlSiXbRa7YNvLEdpEvto18yaltjPXwsnY1ZIE3K6xZFg8hvr6+2LJlC27cuIHvvvsOkydPxqeffmrpalRLrxdlcJdSZ5SVGWRQD6pILnexpcrYNvLEdpEvto18sW3kie1y92R1x3SVSoVmzZohICAAkyZNwmOPPYaPP/4YXl7GlG3q0TDJysqSejI8PT2RnZ1t1hUmiiKys7PNymRlZZltw7TNir0scqVQcE4IEREREdkuq9+RwmAwQKfTwcfHB15eXkhOTpbWabVaHDlyBMHBwQCA4OBgFBUVISUlRSqTkpIizSsBgKCgIBw5cgRarVYqk5ycjIYNG8LHx8dCR3V/ODGdiIiIiGyZRUPIggULcOTIEaSlpSE1NRULFy7EoUOHEB0dDUEQMHz4cKxYsQI7d+7EmTNnEB8fD0dHR/Tp0wcA4Ofnh4iICEyfPh0pKSlISUnB9OnTERkZiRYtWgAAoqOjUb9+fcTHx+PMmTPYuXMnPvroI4waNcpsLomcMYQQERERkS2z6JyQzMxMxMbGIiMjAy4uLvD398eKFSsQEREBABgzZgy0Wi1mzpyJvLw8tG3bFqtXr4azs7O0jYULF2LWrFl48cUXAQBRUVGYNm2atN7FxQWrV6/GzJkz0b9/f7i6umL06NEYNWqUJQ/1vgiCyBBCRERERDZLEHmtMTOlpXqrTzrq08cJaWkijh3j5Ce54aQ0+WLbyBPbRb7YNvLFtpEntsvdk9XEdLo948T0ujF0jIiIiIjobjGEyBDnhBARERGRLWMIkSGGECIiIiKyZQwhMsT7hBARERGRLWMIkSH2hBARERGRLWMIkSGGECIiIiKyZQwhMsQQQkRERES2jCFEhjgnhIiIiIhsGUOITDGEEBEREZGtYgiRIQ7HIiIiIiJbxhAiQxyORURERES2jCFEphhCiIiIiMhWMYTIkIKtQkREREQ2jKe7MiQIgMFg7VoQEREREdUOhhAZEgQAEKxdDSIiIiKiWsEQIkO8OhYRERER2TKGEBlSKDgci4iIiIhsF0OIDAkciUVERERENowhRIY4MZ2IiIiIbBlDiAxxTggRERER2TKGEBnicCwiIiIismUMITLEnhAiIiIismUMITLEEEJEREREtowhRIYYQoiIiIjIljGEyJBCwRBCRERERLaLIUSmGEKIiIiIyFYxhMgQh2MRERERkS1jCJEhhQIAeJ1eIiIiIrJNDCEyZLpPCHtDiIiIiMgWMYTIEEMIEREREdkyhhAZUvzTKgaDdetBRERERFQbLBpCli9fjv79+6Ndu3bo2LEjYmJicObMGbMy8fHx8Pf3N/sZOHCgWRmdTodZs2YhNDQUQUFBiImJwfXr183KXL16FTExMQgKCkJoaChmz54NnU5X68dYE9gTQkRERES2zM6SOzt06BCef/55tGnTBqIo4r333sOoUaOwfft2qNVqqVznzp2RmJgoPba3tzfbzpw5c7B7924sWrQIarUa77zzDsaNG4ekpCQolUro9XqMGzcOarUa69atQ25uLiZPngxRFDF16lRLHe49M4UQ9oQQERERkS2yaAhZtWqV2ePExES0b98ev/76K6KioqTlKpUKXl5eVW7jxo0b2LRpE+bOnYuwsDBpO5GRkUhOTkZERAT279+Ps2fP4scff0Tjxo0BALGxsXjrrbcwceJEODs719IR1gz2hBARERGRLbPqnJDCwkIYDAY0aNDAbPnRo0fRqVMn9OrVC2+99RaysrKkdSdOnEBpaSnCw8OlZY0bN4afnx9SUlIAAMeOHYOfn58UQAAgIiICOp0OJ06cqOWjun+mOSEMIURERERkiyzaE1LRnDlz8NhjjyE4OFhaFhERgR49esDHxwdXrlzB4sWLMWLECCQlJUGlUiEzMxNKpRJubm5m2/Lw8EBmZiYAIDMzEx4eHmbr3dzcoFQqpTLVUSoFqNWONXSE90ahMHaFNGjgCCcnq1aFKlAqFVZ/f1DV2DbyxHaRL7aNfLFt5IntUrOsFkLmzZuHo0ePYsOGDVAqldLy3r17S//39/dH69atERUVhT179qBnz561Xi+9XkRublGt7+fWnAAIyMkpQmmplatCZtRqRxm8P6gqbBt5YrvIF9tGvtg28sR2uXteXi7VrrPKcKy5c+di+/bt+OSTT/DQQw/dsqy3tze8vb1x8eJFAICnpyf0ej1ycnLMymVlZcHT01MqU34IFwDk5ORAr9dLZeSMc0KIiIiIyJZZPITMnj1bCiB+fn63LZ+dnY309HQ0bNgQABAQEAB7e3scOHBAKnP9+nWcO3dOGtYVFBSEc+fOmV2298CBA1CpVAgICKjhI6p5DCFEREREZMssOhxrxowZ+Oqrr7Bs2TI0aNAAGRkZAABHR0c4OTmhsLAQS5cuRc+ePeHl5YUrV65g0aJFcHd3R/fu3QEALi4u6N+/P+bPnw8PDw+o1WrMmzcP/v7+6Ny5MwAgPDwcLVu2RFxcHOLj45Gbm4vExEQMHDhQ9lfGAjgxnYiIiIhsm0VDyPr16wEAI0eONFs+YcIEvPzyy1AqlThz5gy2bNmCGzduwMvLC6GhoVi8eLFZeJgyZQrs7OwwceJElJSUoFOnTkhMTJTmliiVSixfvhwzZszAkCFD4ODggOjoaMTFxVnsWO8H7xNCRERERLZMEEV+315eaane6pOOpk1zwocfKvDHHzfg7m7VqlAFnJQmX2wbeWK7yBfbRr7YNvLEdrl7spuYTrdmGo5lMAjWrQgRERERUS1gCJEhTkwnIiIiIlvGECJDnBNCRERERLaMIUSGBI7CIiIiIiIbxhAiQ7xELxERERHZMoYQGeJwLCIiIiKyZQwhMsSeECIiIiKyZQwhMsSeECIiIiKyZQwhMsRL9BIRERGRLWMIkTGGECIiIiKyRQwhMnTzjunWrQcRERERUW1gCJEhDsciIiIiIlvGECJDvDoWEREREdkyhhAZYk8IEREREdkyhhAZunmJXsG6FSEiIiIiqgUMITLEnhAiIiIismUMITKkUBjTB0MIEREREdkihhAZ4h3TiYiIiMiWMYTIEK+ORURERES2jCFEhtgTQkRERES2jCFEhgReFIuIiIiIbBhDiAxxOBYRERER2TKGEBnicCwiIiIismUMITLEnhAiIiIismUMITLEnhAiIiIismUMITLEnhAiIiIismUMITLEnhAiIiIismUMITJkCiGiyGv1EhEREZHtYQiRoZshxLr1ICIiIiKqDQwhMsQ5IURERERkyxhCZIg9IURERERkyxhCZMjUE8KJ6URERERkiywaQpYvX47+/fujXbt26NixI2JiYnDmzBmzMqIoYsmSJQgPD0dgYCCGDRuGs2fPmpXJy8tDbGwsQkJCEBISgtjYWOTn55uVSU1NxdChQxEYGIiIiAgsXboUYh3pWmBPCBERERHZMouGkEOHDuH555/H559/jk8++QRKpRKjRo1Cbm6uVGbFihVYvXo1pk6dio0bN8Ld3R2jRo1CQUGBVGbSpEk4deoUVq5ciZUrV+LUqVOIi4uT1hcUFGD06NHw8PDAxo0bMWXKFKxatQpr1qyx5OHeM84JISIiIiJbZmfJna1atcrscWJiItq3b49ff/0VUVFREEURa9euxdixY9GrVy8AQEJCAjp16oRt27Zh8ODBOHfuHPbt24f169cjODgYADBjxgy88MILOH/+PFq0aIGvv/4axcXFSEhIgIODAzQaDc6fP481a9Zg1KhREAR5X/qW9wkhIiIiIltm1TkhhYWFMBgMaNCgAQAgLS0NGRkZCAsLk8o4ODigQ4cOSElJAQCkpKTA0dER7dq1k8qEhITA0dFRKnPs2DG0b98eDg4OUpnw8HCkp6cjLS3NEod2X9gTQkRERES2zKI9IRXNmTMHjz32mNSjkZGRAQDw9PQ0K+fh4YH09HQAQGZmJtzd3c16MwRBgLu7OzIzM6Uy3t7eZtswbTMzMxMPPfRQtXVSKgWo1Y73eWT3R6k0phBHx3pQq61aFapAqVRY/f1BVWPbyBPbRb7YNvLFtpEntkvNsloImTdvHo4ePYoNGzZAqVRaqxqV6PUicnOLrFoHUXQEoERBgRa5uXqr1oXMqdWOVn9/UNXYNvLEdpEvto18sW3kie1y97y8XKpdZ5XhWHPnzsX27dvxySefmPVKeHl5AYDUo2GSlZUl9WR4enoiOzvb7EpXoigiOzvbrExWVpbZNkzbrNjLIke8RC8RERER2TKLh5DZs2dLAcTPz89snY+PD7y8vJCcnCwt02q1OHLkiDRkKzg4GEVFRdL8D8A4T6SoqEgqExQUhCNHjkCr1UplkpOT0bBhQ/j4+NTm4dUITkwnIiIiIlt2xyHksccew/Hjx6tcd+LECTz22GO33caMGTOQlJSEBQsWoEGDBsjIyEBGRgYKCwsBGOd2DB8+HCtWrMDOnTtx5swZxMfHw9HREX369AEA+Pn5ISIiAtOnT0dKSgpSUlIwffp0REZGokWLFgCA6Oho1K9fH/Hx8Thz5gx27tyJjz76qE5cGQvgxHQiIiIism13PCfkVjf6MxgMd3Ryv379egDAyJEjzZZPmDABL7/8MgBgzJgx0Gq1mDlzJvLy8tC2bVusXr0azs7OUvmFCxdi1qxZePHFFwEAUVFRmDZtmrTexcUFq1evxsyZM9G/f3+4urpi9OjRGDVq1J0erlWZXkq9Xv6BiYiIiIjobt02hBgMBimAGAwGGCqMESopKcHevXvh5uZ2252lpqbetowgCHj55ZelUFIVV1dXLFiw4Jbb8ff3x7p16267PzniHdOJiIiIyJbdMoQsXboUy5YtA2AMB0OGDKm27PPPP1+zNXuAcWI6EREREdmyW4aQxx9/HIBxKNayZcvw3HPPoVGjRmZlVCoV/Pz8EBkZWXu1fECxJ4SIiIiIbNFtQ4gpiAiCgAEDBlS6CSDVPBndNoWIiIiIqMbd8cT0CRMmVFr2559/4ty5cwgKCmI4qUE3J6Zbtx5ERERERLXhjkPIzJkzUVZWhpkzZwIAdu7ciYkTJ0Kv18PZ2RmrV69GYGBgrVX0QcJL9BIRERGRLbvj+4Ts3bsX7dq1kx4vWbIEXbt2xVdffYXAwEBpAjvdP96skIiIiIhs2R2HkIyMDDRt2hQAcP36dZw9exbjxo2Dv78/hg0bht9//73WKvmg4SV6iYiIiMiW3XEIcXBwQFFREQDg0KFDcHZ2RkBAAADA0dFRuus53T/TcCzOCSEiIiIiW3THc0Jat26NdevWoXHjxli/fj06d+4MxT9ny2lpafDy8qq1Sj5o2BNCRERERLbsjntCXn31Vfz222945plncOHCBfznP/+R1u3atYuT0msQJ6YTERERkS27456QwMBA/Pjjjzh//jyaN28OZ2dnad2gQYPQrFmzWqngg4gT04mIiIjIlt1xCAGMcz9M80DK69q1a03Vh8CbFRIRERGRbburEJKamoply5bh0KFDyM/PR4MGDRAaGorx48dDo9HUVh0fODdvVihYtyJERERERLXgjkPI8ePHMWzYMDg4OCAqKgqenp7IzMzEDz/8gJ9++gmfffZZlb0kdO84J4SIiIiIbNEdh5BFixahZcuW+Pjjj83mgxQUFGDUqFFYtGgRVq9eXSuVfNCYhmMxhBARERGRLbrjq2P99ttvGDdunFkAAQBnZ2eMGTMGKSkpNV65BxUnphMRERGRLbvjEHI7gsD5CzWFl+glIiIiIlt2xyGkbdu2+PDDD1FQUGC2vKioCCtWrEBQUFBN1+2BdXNiunXrQURERERUG+54Tshrr72GYcOGISoqCl27doWXlxcyMzPx008/obi4GJ9++mlt1vOBwp4QIiIiIrJld3Wzwi+++ALvv/8+9u/fj7y8PLi6uiI0NBT/+c9/4O/vX5v1fKAwhBARERGRLbtlCDEYDNizZw98fHyg0Wjw6KOP4r333jMrk5qaiitXrjCE1CDTcCyGECIiIiKyRbecE/L1119j0qRJqF+/frVlnJycMGnSJGzbtq3GK/egMvWE8OpYRERERGSLbhtCnn32WTz00EPVlvHx8UH//v2xefPmGq/cg4qX6CUiIiIiW3bLEHLy5EmEhYXddiOdO3fGiRMnaqxSDzrOCSEiIiIiW3bLEFJYWIgGDRrcdiMNGjRAYWFhjVXqQceeECIiIiKyZbcMIW5ubrh69eptN3Lt2jW4ubnVWKUedOwJISIiIiJbdssQEhISgi1bttx2I5s3b0ZISEhN1emBdzOE8C70RERERGR7bhlCRowYgZ9//hlz586FTqertL60tBRz5szBwYMHMXLkyNqq4wOHl+glIiIiIlt2y/uEBAcHY/LkyUhISMDWrVsRFhaGpk2bAgCuXLmC5ORk5ObmYvLkyQgKCrJEfR8IHI5FRERERLbstndMHzlyJFq3bo0VK1Zg165dKCkpAQA4ODjg8ccfx9ixY9G+fftar+iDxNQTotdbtx5ERERERLXhtiEEADp06IAOHTrAYDAgJycHAKBWq6FUKmu1cg8q9oQQERERkS275ZyQSoUVCnh4eMDDw+OeA8jhw4cRExODiIgI+Pv7IykpyWx9fHw8/P39zX4GDhxoVkan02HWrFkIDQ1FUFAQYmJicP36dbMyV69eRUxMDIKCghAaGorZs2dXOa9FjnjHdCIiIiKyZXfUE1KTioqKoNFo0LdvX0yePLnKMp07d0ZiYqL02N7e3mz9nDlzsHv3bixatAhqtRrvvPMOxo0bh6SkJCiVSuj1eowbNw5qtRrr1q2T5q2IooipU6fW6vHVBE5MJyIiIiJbdlc9ITWhS5cueO211/DEE09Aoah69yqVCl5eXtKPWq2W1t24cQObNm1CXFwcwsLC0Lp1ayQmJiI1NRXJyckAgP379+Ps2bNITExE69atERYWhtjYWHz55ZcoKCiwxGHeF4YQIiIiIrJlFg8hd+Lo0aPo1KkTevXqhbfeegtZWVnSuhMnTqC0tBTh4eHSssaNG8PPzw8pKSkAgGPHjsHPzw+NGzeWykRERECn0+HEiROWO5B7xOFYRERERGTLLD4c63YiIiLQo0cP+Pj44MqVK1i8eDFGjBiBpKQkqFQqZGZmQqlUVrpDu4eHBzIzMwEAmZmZ8PDwMFvv5uYGpVIplamOUilArXas2YO6S3l5CgiCCJXKHmq17JrogaZUKqz+/qCqsW3kie0iX2wb+WLbyBPbpWbJ7gy3d+/e0v/9/f3RunVrREVFYc+ePejZs2et71+vF5GbW1Tr+7l1HRwBKFBSUorc3Loxmf5BoVY7Wv39QVVj28gT20W+2DbyxbaRJ7bL3fPycql2nSyHY5Xn7e0Nb29vXLx4EQDg6ekJvV4vXSrYJCsrC56enlKZ8kO4ACAnJwd6vV4qI2eCYPzhnBAiIiIiskWyDyHZ2dlIT09Hw4YNAQABAQGwt7fHgQMHpDLXr1/HuXPnEBwcDAAICgrCuXPnzC7be+DAAahUKgQEBFj2AO6RIHBOCBERERHZJosPxyosLMSlS5cAAAaDAVevXsXp06fh6uoKV1dXLF26FD179oSXlxeuXLmCRYsWwd3dHd27dwcAuLi4oH///pg/fz48PDygVqsxb948+Pv7o3PnzgCA8PBwtGzZEnFxcYiPj0dubi4SExMxcOBAODs7W/qQ75qpJ4QhhIiIiIhskcVDyIkTJzB8+HDp8ZIlS7BkyRL069cPb7/9Ns6cOYMtW7bgxo0b8PLyQmhoKBYvXmwWHqZMmQI7OztMnDgRJSUl6NSpExITE6UbKCqVSixfvhwzZszAkCFD4ODggOjoaMTFxVn6cO+ZcTiWYO1qEBERERHVOEEUOfOgvNJSvdUnHeXmOiIgQIHnny9FYqLWqnUhc5yUJl9sG3liu8gX20a+2DbyxHa5e3V6YvqDiDcrJCIiIiJbxhAiQ7w6FhERERHZMoYQGeId04mIiIjIljGEyJAgGIMIe0KIiIiIyBYxhMiQaU6IXm/dehARERER1QaGEBlSKABBENkTQkREREQ2iSFEhnizQiIiIiKyZQwhMsQQQkRERES2jCFEhkxXx+JwLCIiIiKyRQwhMsT7hBARERGRLWMIkSEOxyIiIiIiW8YQIkPGq2MxhBARERGRbWIIkSHTfUJEUbBuRYiIiIiIagFDiAxxOBYRERER2TKGEBnicCwiIiIismUMITLEq2MRERERkS1jCJEh05wQ9oQQERERkS1iCJEh03As9oQQERERkS1iCJEhTkwnIiIiIlvGECJDnBNCRERERLaMIUSGOByLiIiIiGwZQ4gMcWI6EREREdkyhhAZUiiMP+wJISIiIiJbxBAiQ6aeEL1esG5FiIiIiIhqAUOIjLEnhIiIiIhsEUOITHE4FhERERHZKoYQmeLVsYiIiIjIVjGEyBRDCBERERHZKoYQmeId04mIiIjIVjGEyJRCAej11q4FEREREVHNYwiRKTs7EaWl1q4FEREREVHNs3gIOXz4MGJiYhAREQF/f38kJSWZrRdFEUuWLEF4eDgCAwMxbNgwnD171qxMXl4eYmNjERISgpCQEMTGxiI/P9+sTGpqKoYOHYrAwEBERERg6dKlEOvQJAs7O6C0lPcJISIiIiLbY/EQUlRUBI1GgylTpsDBwaHS+hUrVmD16tWYOnUqNm7cCHd3d4waNQoFBQVSmUmTJuHUqVNYuXIlVq5ciVOnTiEuLk5aX1BQgNGjR8PDwwMbN27ElClTsGrVKqxZs8Yix1gT7O3BnhAiIiIiskkWDyFdunTBa6+9hieeeAIKhfnuRVHE2rVrMXbsWPTq1QsajQYJCQkoLCzEtm3bAADnzp3Dvn37MHPmTAQHByM4OBgzZszAjz/+iPPnzwMAvv76axQXFyMhIQEajQZPPPEExowZgzVr1tSZ3hAOxyIiIiIiWyWrOSFpaWnIyMhAWFiYtMzBwQEdOnRASkoKACAlJQWOjo5o166dVCYkJASOjo5SmWPHjqF9+/ZmPS3h4eFIT09HWlqahY7m/hh7Qjgci4iIiIhsj521K1BeRkYGAMDT09NsuYeHB9LT0wEAmZmZcHd3hyDcPEEXBAHu7u7IzMyUynh7e5ttw7TNzMxMPPTQQ9XWQakUoFY73v/B3AelUgFHRxFlZdavC5lTKhVsE5li28gT20W+2DbyxbaRJ7ZLzZJVCJEDvV5Ebm6RVetgfIMboNMprV4XMqdWO7JNZIptI09sF/li28gX20ae2C53z8vLpdp1shqO5eXlBQBSj4ZJVlaW1JPh6emJ7Oxss7kdoigiOzvbrExWVpbZNkzbrNjLIlccjkVEREREtkpWIcTHxwdeXl5ITk6Wlmm1Whw5cgTBwcEAgODgYBQVFUnzPwDjPJGioiKpTFBQEI4cOQKtViuVSU5ORsOGDeHj42Oho7k/9vZAWZm1a0FEREREVPMsHkIKCwtx+vRpnD59GgaDAVevXsXp06dx9epVCIKA4cOHY8WKFdi5cyfOnDmD+Ph4ODo6ok+fPgAAPz8/REREYPr06UhJSUFKSgqmT5+OyMhItGjRAgAQHR2N+vXrIz4+HmfOnMHOnTvx0UcfYdSoUWZzSeRMpTL2hNSRi3kREREREd0xQbTwNWt/+eUXDB8+vNLyfv364Z133oEoili6dCm++OIL5OXloW3btpg2bRo0Go1UNi8vD7NmzcIPP/wAAIiKisK0adPQoEEDqUxqaipmzpyJ48ePw9XVFYMHD8b48eNvG0JKS/VWH++nVjtixAg9NmxQIS3tBlQqq1aHyuF4UPli28gT20W+2DbyxbaRJ7bL3bvVnBCLhxC5k0sIGT++DCtW1MOff95AuWxFVsY/QPLFtpEntot8sW3ki20jT2yXu1dnJqbTTc7Oxn9zc+vG8DEiIiIiojvFECJTrq7GDqqrVxlCiIiIiMi2MITIlLu7MYRcuMAmIiIiIiLbwjNcmXJwMP7L4VhEREREZGsYQmTKdEWs/HyGECIiIiKyLQwhMuXgYByOVVxs/CEiIiIishUMITLVooUBAJCerkBBAXtDiIiIiMh2MITIVLNmIpydRVy5IrAnhIiIiIhsCkOITAkC0LatHhcuKKDVsieEiIiIiGwHQ4iMRUWV4fp1BS5dYgghIiIiItvBECJjnTrpAQCnT7OZiIiIiMh28OxWxho3Nl4h6++/2UxEREREZDt4ditjDRsaQ0hGBodjEREREZHtYAiRMXt7oEEDA7KyrF0TIiIiIqKawxAic+7uIrKz2UxEREREZDt4ditzHh4icnM5HIuIiIiIbAdDiMy5u4soLAR0OmvXhIiIiIioZjCEyJwxhAjQaq1dEyIiIiKimsEQInMeHiK0WgF5eRySRURERES2gSFE5kyX6b1+nSGEiIiIiGwDQ4jMNWliDCGXL7OpiIiIiMg28MxW5kw9IVevsieEiIiIiGwDQ4jMNW5sAAD89RebioiIiIhsA89sZa5pUxGAiKtXBVy5wt4QIiIiIqr7GEJkTqUCvL1F5OcLOHGCzUVEREREdR/PauuAhx4SodUCf/+twO+/s8mIiIiIqG7jGW0dEBioxx9/KNGkiQEpKUp8/70Sly4JEEVr14yIiIiI6O4xhNQBTz5ZhuJiAbm5Atq31yM3V8CePXbYvt0O2dnWrh0RERER0d1hCKkDwsP1eOwxPebPr4eWLQ3o378MYWFlKCoCdu+2g15v7RoSEREREd05hpA6QKkEpk7V4sIFBRYsUEGhAPz8RISF6VFcLODyZV41i4iIiIjqDjtrV4DuTLduegwdqsO779ZDWRkwdmwpmjQRoVAA2dkCmjfnBBEiIiIiqhsYQuoIQQDmzdOipETAkiX18MEHKvTuXYbgYD2KitgTQkRERER1h+yGYy1ZsgT+/v5mP2FhYdJ6URSxZMkShIeHIzAwEMOGDcPZs2fNtpGXl4fY2FiEhIQgJCQEsbGxyM/Pt/Sh1Lh69YD33y/BwYMFeOmlUvz4ox0SE+vh/HmGECIiIiKqO2QXQgDA19cX+/fvl362bt0qrVuxYgVWr16NqVOnYuPGjXB3d8eoUaNQUFAglZk0aRJOnTqFlStXYuXKlTh16hTi4uKscSi1okULETNnarF7dyFKS4Fvv7W3dpWIiIiIiO6YLEOInZ0dvLy8pB93d3cAxl6QtWvXYuzYsejVqxc0Gg0SEhJQWFiIbdu2AQDOnTuHffv2YebMmQgODkZwcDBmzJiBH3/8EefPn7fmYdW4Zs1E+PvrkZoqy2YkIiIiIqqSLM9eL1++jPDwcERFRWHixIm4fPkyACAtLQ0ZGRlmw7McHBzQoUMHpKSkAABSUlLg6OiIdu3aSWVCQkLg6OgolbElbdoYkJYmoKTE2jUhIiIiIrozspuYHhgYiHnz5qFFixbIzs7GBx98gMGDB2Pbtm3IyMgAAHh6epo9x8PDA+np6QCAzMxMuLu7QxBuzpMQBAHu7u7IzMy87f6VSgFqtWMNHtHdUyoVd1wHjUaAKArIznZEq1a1XDG6q7Yhy2LbyBPbRb7YNvLFtpEntkvNkl0I6dKli9njtm3bonv37tiyZQvatm1b6/vX60Xk5hbV+n5uRa12vOM6eHnZAaiP48e1aNKEdy2sbXfTNmRZbBt5YrvIF9tGvtg28sR2uXteXi7VrpPlcKzynJyc8Mgjj+DixYvw8vICgEo9GllZWVLviKenJ7KzsyGKN++bIYoisrOzK/Wg2ILmzQ0AgEuXZN+UREREREQA6kAI0Wq1uHDhAry8vODj4wMvLy8kJyebrT9y5AiCg4MBAMHBwSgqKjKb/5GSkoKioiKpjC3x9TWGkLQ02TclEREREREAGQ7HSkhIQGRkJBo3bozs7Gy8//77KCoqQr9+/SAIAoYPH47ly5ejRYsWaN68OT744AM4OjqiT58+AAA/Pz9ERERg+vTpmDlzJgBg+vTpiIyMRIsWLax5aLXC3R1QqUSkp/NeIURERERUN8guhFy/fh2vvfYacnNz4ebmhqCgIHz55Zdo2rQpAGDMmDHQarWYOXMm8vLy0LZtW6xevRrOzs7SNhYuXIhZs2bhxRdfBABERUVh2rRpVjme2qZUAi4uIrKyGEKIiIiIqG4QxPKTJwilpXqrTzq624lPoaFOcHERsWsXJ0vVNk5Kky+2jTyxXeSLbSNfbBt5YrvcvTo9MZ1uT60WkZtr7VoQEREREd0ZhhAb4OZmQF4eh2MRERERUd3AEGID3N1F3LghgAPriIiIiKguYAixAd7eIgwGAX//zd4QIiIiIpI/hhAb8NBDxi6Qc+cYQoiIiIhI/hhCbEDz5noAwIULbE4iIiIikj+etdqARx4x9oRcusTmJCIiIiL541mrDWjSRIQgiDh/nsOxiIiIiEj+GEJsgJ0d8PDDBvz5pxJ6vbVrQ0RERER0awwhNqJ1awPS0gRkZLA3hIiIiIjkjSHERnTsqEd+vgJHjrBJiYiIiEjeeMZqI3r2LAMA7Nhhh7IyK1eGiIiIiOgWGEJsRIsWIh55RI9Dh+yQmspmJSIiIiL54tmqDRkxohSXLimQlGQHnc7atSEiIiIiqhpDiA0ZPrwUjRsbsHGjPY4eZdMSERERkTzxTNWG1K8PzJ6txbVrCrz/vgrXr/NKWUREREQkPwwhNiY6ugx9+5Zi5047fP65HQwGa9eIiIiIiMgcQ4gNmj+/BA0bili1SoVff2UTExEREZG88AzVBrm6AgsXluDvvxV48816OHGCzUxERERE8sGzUxvVs6ce//mPDseO2WHmTBXOn+f8ECIiIiKSB4YQG/b221q88IIOe/bYYe1ae5w7xyBCRERERNZnZ+0KUO2aNUuL5GQ7rFmjgkJh7CH56y8BkZF6NGwoWrt6RERERPQAYk+IjXN2BpKSitCokYiVK1XYtk0JrVbADz8o8fffAoqLrV1DIiIiInrQsCfkAdC0qYgtW4rw7LOO+OijemjY0ABfXwMKCgSoVICHh4imTQ2wswOcnEQ4OQEGA9CwoQhBAASO4iIiIiKiGsQQ8oBo3FjE998X4tNP7bF0qQq//GKHX36xQ6dOZWja1AAvLwUaNxbh6HjzOYIAKJUinJ0BlUqEQgG4u98sk58v4NFH9XB0BFQq6xwXEREREdU9DCEPEGdn4N//LsW//12KH35QYutWO/z4ox1+/vnm26BJEwPc3ESo1SKcnY0BxNvbAAcHQKsFXFwANzcRSqWx/JkzxhF9CoUxsLi6Gpc7OIjw8hJRrx5Qr56IkhIBpaWAj4+BoYWIiIjoAccQ8oCKitIjKkoPQIvMTAG//67A778rcfKkAtnZAjIyBJw6pUROTlVjsYy9IUqlcfiWn58eWq2A3FwBjz1mQOPGBgACHB1FqFTGQOLkBOTnA87OSqhUxsBSvz5Qrx6g1xu3pVaLuHEDaNRIhMFgXF6vnnEfeXkCGjcW4e5u3ObffwtwcjKGJCIiIiKqWxhCCJ6eIiIj9YiM1FdaV1gIXLsm4OpVBQQBuHRJgStXBNy4IUAUgfR0AadPK5CeroCbm4gdO+yg11c/icTOTkSzZga4uIhwcADs7YHSUuNwr0aNRNjZGXtOHByMAUShAOztjY/r1wfs7IzbKCsz7sPZ2VTWOFzMwcEYeuzsjNu1twdcXERpP/b2xu06OxvLa7XGYWfFxQLc3IzPIyIiIqLaxVMuuiUnJ+CRR0Q88ogpoFQOKuWVlBhDS16egCtXFNBqgdxcAZmZAho2FHH5soDTp5XIzRWQlQVotcYwkZNjDDa3IggivL1FuLqKKCkxLnNxAXQ6Yzhp2tSABg1ECIKA+vVFqFTGSxCrVJCGhRn/NS4TBEAUgbIy40R8U4CpX9+47eJiY/DJzwfUauN8GDc3oLjYGMjs7Y3lDQZAFAUIgnE4mlIpSuvt7U1za4yBrkGDe20JIiIiItvBEEI1ysEB8PUVAYgICjLc8fPKyoCsLAEFBcCNGwIKCoyhpKzM2FuRl2ccIvbXXwpkZBh7YeztjZPjXV1FFBYK2LfPDiUld3YpL0Ew9qCUlQGlpcbnqNUGuLoae02UypsBwt4eAIy9JGq1AFE0TmhRqYy9LMbeGWMPzc3/G59nZydKYcQ4b8YYhpRKY8DR6437cHAAFArjcoXi5mtpmmujVBq3adqGiV5vCjvGnh07O+MxGXuUjPsyXeFMoTD+3HzOzSufiSKvgkZERESWwxBCsmBnB3h7i/D2BoB7u4liWZmxt6GoSEB2tgCt1nhyXVws/BNsgIICY8ApLDT+v149UeqduHhRQFaWgOJiY/gx/hgn1JeWGnt3jh8XUFJiXwNHXD7sGEOG6cfOzvRYMAsmxmXGoGMqq1Dc7MWxtzf2yhQW3pwvYwospgCiUIhSIBEE4+suijeHvZm2bQpSpnKAsYeo/CWbTQHNyUmEVgtpWJxSeXPbpvqKovE1dHcX/9lX+ToZe7NMIcp0/ACksAkYj9NgMIZUR0cROp2ABg1EaY6RwQBkZAhS3VQqEaWlgvT6lj/eoiJjb5ko3px7pFQa14micT+mwFav3s1gp+CdlYiIiGqEzYeQdevWYdWqVcjIyEDLli3x5ptvon379tauFtUCOzvA1RVwdRXRuHHt3A1erXZETk4RAKCgwHjSq9MJ0OmMPTY6nTH8aLXGf0tKjCFIqzWeyOr1wj+9L8YfnU6o8v/lH+t0xhN4nc64rKDg5nLTyfGNG8YbTyoUxgn++fmCNG9GjurVE2FnJ5oFGtMJvinolF9X/kevN/6YgoKp18rOToBC4QilUpTmJSkUohR0yoevmz+iWW8QABgMxjYy9WTpdAIcHESz0GQayle+98g4LO9m8KtYf2N9UGl/Fdebjsu0TWNgE6V6mS7aYP5a3gx6piBnGgZ4M8SavxamUFZ+aKGj4835WC4uxuGFjo4iSkvNj83eXpTqa9qeKcRptQLUagOcnIyP69cHsrONvyem11urNd6jqOJ7oPzrYeqhNAZn43MNBuGfsCpKobCszDzQmuZ6qVSm4ZbGOWQGg3G7zs7GupeVQQqnph97e+PvlelxWZkx0Jva2tTupt9PU3jXagXo9UBRkbFtPD2N4dze/ub7whR47ezM3ysKxc1hoaae0fr1zQOxab0pvAPGsmVllX8/qnrfmbZV/n1n+jc7+2adtFrje9203vT+EYSb78uyMqCkxHjs9erdLFtxX/b2xjqWlkL6ckMs92e5/P8r/j6Y2qd88DdtV6EwblOvN7axqfdbpRKl9zMA6Ua8jo7m7xHTtk1fMhl7ho0XWyk/RLcinc7497t+fWO7VmyfivU0/b9+fdNnw83fN70e0hxG0++z6Xfa9J4oKTH+lG9P05dMpi97dDpjedP7oPyXNsbfnZt1M/3tMP2uGgw3291Un/J/n8q3S3VtVZ7BAGkos6GaQRCm37mqnn+77d/JuqrqVP5LpoqvdfnjNb02Dg7G9VWNDigsNK4vLa1+nxU/E0yvs+l3u7gY0t8F03u44t9l0/ZNX9iZ2r783xLT51lVv1Pl6yD3ea4yr9792bFjB+bOnYvp06cjJCQE69evx5gxY7B9+3Y0adLE2tWjOsr0C+7iYjxRu9eem5pW/g+QXm+cy6LXC9IfVNMfX9MJvMEgmD02/79QadnNx0IVy24+r6p1gPGPt2koXW6uIJ1ImP74V9yv6QPe9HzTH1/TH22l0nhyafqANxjsUFJi+OfDXjQLLMYPYEH6I35zH4LZH3XTH3bThQzy8xVwcBCRmamQPsRN2yj/h970QW/6UKjqQ6qqE8GKgaTyh71g9hoZT8rMh+1ptcbXzVQn035Mx2n6AawZSh+8y9jZ2YmVgrXJ3ZxI3anqhlPeapil8f1SddtU/7zqK1/dc0TR2KtrCssVT2yr+oKg4sn9vajqda5uWcW/AXeqqnatuI1bha/q6iYIAkTRqVKZ0tKbX76Yft+B6k/878bthuTW5pDd+9n2rZ5rCqB3eluA2wdlAYLgZNa+VX0JUP5vevn/m/9NrvwFSsXPl+r+bpR/r5Z/H5QnCMB//qPFiy9WkaplwqZDyJo1a9CvXz8MHDgQADB16lTs27cPGzZswKRJk6xcO6KaVf6PlVIJuLkBtw5I8ghPNUWtdkRubrG1qyFbpg9A0/9N3/iahhuWlhq/eS0oMH67nJ9vnItVVCT8cyEHUfrm2NjTJEIUBWmbpg9We3vxn545AUol4OJiD51OV+EbdVEKhRXraPowNX243gyhxt4Q43GYhzPT/k3HaPpQLj8M0fQBr9MJ0rqKr4npcuHl61FWJlTZe2XaZvlv+k29Z/n5wj+9kebHVr4HzfRamJQ/mTD15JXvqatYvvyJblUB1/SamMqWZ9pPvXp2KC0tM+uZKb+Pm4GhfE+UKL1Gpm93K+7XuG/jCtOFOoy9wYI0JLTiCZvp/XDzfWRcV7Gdyx+7wXCzd658W5rKVOyxNG2nYs9o+W+Yy78fKp4Amr7Rruo1rtgbYtqXqZfB1OtQ/vU3rasqpNrbK1FWVvlCMA4Oxp5eU4+d8TWu+pvxir8L5du0/IlrxS8vyiv/Ot5K+fdDxfdb+RPuuw3g9xPYTV/clJXd7Bkr/96reFJvqmt1IdTYy6xESYle6n0qX0fT36XKoeTmMGhTb5gpSJq+JKvYU1X+SzRj8BHNXtuKvzPVhaCHHpL357zNhhCdToeTJ09i9OjRZsvDwsKQkpJipVoREVmH6cOtPHt70xCKih9UNffBpVbbITf3FuMXyGrUaiVyc7XWrgZVwfilSom1q0EVsF1qls2GkJycHOj1enh6epot9/DwQHJycrXPUyoFqNWOtV29W1IqFVavA1WNbSNfbBt5YrvIF9tGvtg28sR2qVk2G0LulV4vIje3yKp1MCZt69aBqsa2kS+2jTyxXeSLbSNfbBt5YrvcPS8vl2rX2ewFJ93c3KBUKpGZmWm2PCsrC15eXlaqFRERERER2WwIUalUaN26daWhV8nJyQgODrZSrYiIiIiIyKaHY40aNQpxcXEIDAxEu3btsGHDBqSnp2Pw4MHWrhoRERER0QPLpkPIU089hZycHHzwwQdIT0+HRqPBRx99hKZNm1q7akREREREDyybDiEA8MILL+CFF16wdjWIiIiIiOgfNjsnhIiIiIiI5IkhhIiIiIiILIohhIiIiIiILIohhIiIiIiILIohhIiIiIiILIohhIiIiIiILEoQRVG0diWIiIiIiOjBwZ4QIiIiIiKyKIYQIiIiIiKyKIYQIiIiIiKyKIYQIiIiIiKyKIYQIiIiIiKyKIYQIiIiIiKyKIYQIiIiIiKyKIYQmVm3bh2ioqLQpk0bPPvsszhy5Ii1q2TTli9fjv79+6Ndu3bo2LEjYmJicObMGbMyoihiyZIlCA8PR2BgIIYNG4azZ8+alcnLy0NsbCxCQkIQEhKC2NhY5OfnW/JQbNry5cvh7++PmTNnSsvYLtaTnp6OyZMno2PHjmjTpg2eeuopHDp0SFrPtrEOvV6PxYsXS58hUVFRePfdd1FWViaVYdtYxuHDhxETE4OIiAj4+/sjKSnJbH1NtUNqaiqGDh2KwMBAREREYOnSpeDt36p3q3YpLS3F/PnzER0djaCgIISHh2PSpEm4evWq2TZ0Oh1mzZqF0NBQBAUFISYmBtevXzcrc/XqVcTExCAoKAihoaGYPXs2dDqdRY6xLmEIkZEdO3Zg7ty5iImJwZYtWxAcHIwxY8ZU+gWgmnPo0CE8//zz+Pzzz/HJJ59AqVRi1KhRyM3NlcqsWLECq1evxtSpU7Fx40a4u7tj1KhRKCgokMpMmjQJp06dwsqVK7Fy5UqcOnUKcXFxVjgi23Ps2DF88cUX8Pf3N1vOdrGO/Px8DBkyBKIo4qOPPsKOHTswdepUeHh4SGXYNtaxYsUKrF+/Hm+99Ra++eYbTJkyBevXr8fy5cvNyrBtal9RURE0Gg2mTJkCBweHSutroh0KCgowevRoeHh4YOPGjZgyZQpWrVqFNWvWWOQY66JbtUtJSQlOnTqFf//730hKSsL777+Pa9eu4aWXXjIL8nPmzMF3332HRYsWYd26dSgsLMS4ceOg1+sBGL8MGDduHAoLC7Fu3TosWrQI3377LRISEix6rHWCSLLx3HPPiVOmTDFb1qNHD3HBggVWqtGDp6CgQHz00UfF3bt3i6IoigaDQQwLCxPff/99qUxxcbEYFBQkbtiwQRRFUfzzzz9FjUYjHjlyRCpz+PBhUaPRiOfOnbPsAdiY/Px8sVu3buLPP/8sDh06VJwxY4YoimwXa1q4cKE4aNCgatezbaxn7NixYlxcnNmyuLg4cezYsaIosm2sJSgoSNy0aZP0uKbaYd26dWJwcLBYXFwslVm2bJkYHh4uGgyG2j6sOq9iu1Tl7NmzokajEf/44w9RFI2fSa1btxa/+uorqczVq1dFf39/ce/evaIoiuKePXtEf39/8erVq1KZLVu2iAEBAeKNGzdq4UjqLvaEyIROp8PJkycRFhZmtjwsLAwpKSlWqtWDp7CwEAaDAQ0aNAAApKWlISMjw6xdHBwc0KFDB6ldUlJS4OjoiHbt2kllQkJC4OjoyLa7T1OnTkWvXr3QsWNHs+VsF+vZtWsX2rZti1dffRWdOnXCM888g88++0waAsK2sZ6QkBD88ssvOHfuHADgzz//xMGDB/Gvf/0LANtGLmqqHY4dO4b27dubfaMfHh6O9PR0pKWlWehobJupZ8rV1RUAcOLECZSWliI8PFwq07hxY/j5+Zm1i5+fHxo3biyViYiIgE6nw4kTJyxYe/mzs3YFyCgnJwd6vR6enp5myz08PJCcnGylWj145syZg8ceewzBwcEAgIyMDACosl3S09MBAJmZmXB3d4cgCNJ6QRDg7u6OzMxMC9Xc9nz55Ze4dOkS5s+fX2kd28V6Ll++jPXr12PkyJEYO3YsTp8+jdmzZwMAhg4dyraxojFjxqCwsBC9e/eGUqlEWVkZYmJi8MILLwDg741c1FQ7ZGZmwtvb22wbpm1mZmbioYceqrVjeBDodDq88847iIyMRKNGjQAYX1elUgk3Nzezsh4eHmbtUn54KgC4ublBqVTyd6gChhCif8ybNw9Hjx7Fhg0boFQqrV2dB9r58+exaNEirF+/Hvb29tauDpUjiiICAgIwadIkAECrVq3w119/Yd26dRg6dKiVa/dg27FjB7Zs2YKFCxfikUcewenTpzF37lz4+PhgwIAB1q4eUZ1RVlaG2NhY3LhxAx988IG1q2OzOBxLJqpLyVlZWfDy8rJSrR4cc+fOxfbt2/HJJ5+YfXtkeu2rahfTN06enp7Izs42uyKJKIrIzs6u9E0X3Zljx44hJycHffr0QatWrdCqVSscOnQI69evR6tWraBWqwGwXazBy8sLfn5+ZstatGiBa9euSesBto01JCYmYvTo0ejduzf8/f3Rt29fjBw5Eh999BEAto1c1FQ7eHp6Iisry2wbpm2yre5dWVkZXnvtNaSmpuLjjz826/Xw9PSEXq9HTk6O2XMqtl3FdqlutMuDjiFEJlQqFVq3bl1p6FVycrI0NIhqx+zZs6UAUvHkysfHB15eXmbtotVqceTIEaldgoODUVRUZDZeOiUlBUVFRWy7e9S9e3ds3boVW7ZskX4CAgLQu3dvbNmyBb6+vmwXK2nXrh0uXLhgtuzixYto0qQJAP7OWFNJSUmlXlylUgmDwQCAbSMXNdUOQUFBOHLkCLRarVQmOTkZDRs2hI+Pj4WOxraUlpZi4sSJSE1Nxdq1ayt9CRwQEAB7e3scOHBAWnb9+nWcO3fOrF3OnTtndtneAwcOQKVSISAgwDIHUkdwOJaMjBo1CnFxcQgMDES7du2wYcMGpKenY/Dgwdaums2aMWMGvvrqKyxbtgwNGjSQxuo6OjrCyckJgiBg+PDhWL58OVq0aIHmzZvjgw8+gKOjI/r06QMA8PPzQ0REBKZPny7dx2L69OmIjIxEixYtrHZsdVmDBg2kiwOYODo6wtXVFRqNBgDYLlYyYsQIDBkyBB988AGeeuopnDp1Cp9++ilee+01AODvjBVFRkbio48+go+PjzQca82aNejbty8Ato0lFRYW4tKlSwAAg8GAq1ev4vTp03B1dUWTJk1qpB2io6OxbNkyxMfH49///jcuXryIjz76CBMmTDCbS0I33apdGjZsiP/+97/4/fff8eGHH0IQBOmcwMXFBQ4ODnBxcUH//v0xf/58eHh4QK1WY968efD390fnzp0BGC8O0LJlS8TFxSE+Ph65ublITEzEwIED4ezsbLVjlyNBFHlXGzlZt24dVq1ahfT0dGg0Grzxxhvo0KGDtatlsyree8JkwoQJePnllwEYu8CXLl2KL774Anl5eWjbti2mTZsmnQwDxptKzZo1Cz/88AMAICoqCtOmTat0Ik33btiwYWjZsiWmTZsGgO1iTXv27MGiRYtw4cIFNGnSBC+88AKGDRsmnfiwbayjoKAA//vf/7Br1y5pKG/v3r0xfvx41KtXDwDbxlJ++eUXDB8+vNLyfv364Z133qmxdkhNTcXMmTNx/PhxuLq6YvDgwRg/fjxDSDVu1S4TJkxAt27dqnzevHnz8OyzzwIwTlhPSEjAtm3bUFJSgk6dOmH69OlmV8O6evUqZsyYgYMHD8LBwQHR0dGIi4uDSqWqnQOroxhCiIiIiIjIojgnhIiIiIiILIohhIiIiIiILIohhIiIiIiILIohhIiIiIiILIohhIiIiIiILIohhIiIiIiILIo3KyQiolqRlJSEN954o8p1Li4uOHLkiIVrZBQfH4/k5GTs3bvXKvsnIiKGECIiqmX/+9//0KhRI7NlSqXSSrUhIiI5YAghIqJa9dhjj6FZs2bWrgYREckI54QQEZHVJCUlwd/fH4cPH8Z//vMfBAcHIzQ0FDNmzEBJSYlZ2fT0dMTFxSE0NBQBAQGIjo7GV199VWmbly9fRmxsLMLCwhAQEIBu3bph9uzZlcqdOnUKzz//PNq2bYuePXtiw4YNtXacRERkjj0hRERUq/R6PcrKysyWKRQKKBQ3vweLjY3Fk08+ieeffx7Hjx/H+++/j+LiYrzzzjsAgKKiIgwbNgx5eXl47bXX0KhRI3z99deIi4tDSUkJBg0aBMAYQAYMGID69evjlVdeQbNmzXDt2jXs37/fbP8FBQWYNGkSRowYgfHjxyMpKQlvv/02fH190bFjx1p+RYiIiCGEiIhq1ZNPPllpWdeuXbF8+XLp8b/+9S9MnjwZABAeHg5BEPDee+9h3Lhx8PX1RVJSEi5evIi1a9ciNDQUANClSxdkZWVh8eLFeO6556BUKrFkyRJotVp89dVX8Pb2lrbfr18/s/0XFhZi+vTpUuDo0KED9u/fj+3btzOEEBFZAEMIERHVqmXLlpkFAgBo0KCB2eOKQaV3795YvHgxjh8/Dl9fXxw+fBje3t5SADF5+umn8cYbb+DPP/+Ev78/Dhw4gK5du1baX0X169c3CxsqlQrNmzfH1atX7+UQiYjoLjGEEBFRrWrZsuVtJ6Z7enqaPfbw8AAA/P333wCAvLw8eHl5Vfu8vLw8AEBubm6lK3FVpWIIAoxBRKfT3fa5RER0/zgxnYiIrC4zM9PscVZWFgBIPRqurq6VypR/nqurKwDAzc1NCi5ERCRfDCFERGR133zzjdnj7du3Q6FQoG3btgCAxx9/HNevX8fRo0fNym3btg0eHh545JFHAABhYWH48ccfkZ6ebpmKExHRPeFwLCIiqlWnT59GTk5OpeUBAQHS//fu3YuEhASEh4fj+PHjWLZsGfr27YvmzZsDME4sX7t2LV5++WVMnDgR3t7e2Lp1Kw4cOICZM2dKNz98+eWX8dNPP2Hw4MGIiYnBww8/jL///hv79u3DggULLHK8RER0ewwhRERUq/773/9Wufznn3+W/j9//nysXr0an3/+Oezt7TFgwADpalkA4OjoiE8//RTz58/HggULUFhYCF9fXyQmJuKZZ56Ryvn4+ODLL7/E4sWLsXDhQhQVFcHb2xvdunWrvQMkIqK7JoiiKFq7EkRE9GBKSkrCG2+8gZ07d/Ku6kREDxDOCSEiIiIiIotiCCEiIiIiIovicCwiIiIiIrIo9oQQEREREZFFMYQQEREREZFFMYQQEREREZFFMYQQEREREZFFMYQQEREREZFFMYQQEREREZFF/T+WlNc5XSOL/AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 936x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set_style(\"darkgrid\")\n",
    "plt.figure(figsize=(13, 6))\n",
    "\n",
    "# summarize history for loss:\n",
    "plt.plot(history.history['loss'], color='blue', label='train')\n",
    "plt.plot(history.history['val_loss'], color='blue', alpha=0.4, label='validation')\n",
    "plt.xlabel('Epoch', fontsize=16)\n",
    "plt.ylabel('Cost', fontsize=16)\n",
    "plt.tick_params(axis='both', which='major', labelsize=14)\n",
    "plt.title('Average Loss During Training', fontsize=18)\n",
    "#plt.xticks(range(0,epochs))\n",
    "plt.legend(loc='upper right', fontsize=16)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
