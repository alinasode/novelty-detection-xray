{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Autoencoder: Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import axis, colorbar, imshow, show, figure, subplot\n",
    "from matplotlib import cm\n",
    "import matplotlib as mpl\n",
    "mpl.rc('axes', labelsize=18)\n",
    "mpl.rc('xtick', labelsize=16)\n",
    "mpl.rc('ytick', labelsize=16)\n",
    "%matplotlib inline\n",
    "\n",
    "import glob\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "import cv2\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "## ----- GPU ------------------------------------\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'    # use of GPU 0 (ERDA) (use before importing torch or tensorflow/keeas)\n",
    "## ----------------------------------------------\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.layers import Conv2D, Conv2DTranspose, Input, Flatten, Dense, Lambda, Reshape\n",
    "from keras.layers import BatchNormalization, ReLU, LeakyReLU\n",
    "from keras.models import Model\n",
    "from keras.losses import binary_crossentropy, mse\n",
    "from keras.activations import relu\n",
    "from keras.callbacks import Callback, ModelCheckpoint\n",
    "from keras import backend as K                         #contains calls for tensor manipulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n",
      "2.4.3\n"
     ]
    }
   ],
   "source": [
    "print (tf.__version__)\n",
    "print (keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NORMAL_TRAIN_AUG:       /home/jovyan/work/Speciale/FoodSorting/generated_dataset/KitKatChunky//DataAugmentation/NormalTrainAugmentations\n",
      "NORMAL_VALIDATION_AUG:  /home/jovyan/work/Speciale/FoodSorting/generated_dataset/KitKatChunky//DataAugmentation/NormalValidationAugmentations\n",
      "NORMAL_TEST_AUG:        /home/jovyan/work/Speciale/FoodSorting/generated_dataset/KitKatChunky//DataAugmentation/NormalTestAugmentations\n",
      "ANOMALY_AUG:            /home/jovyan/work/Speciale/FoodSorting/generated_dataset/KitKatChunky//DataAugmentation/AnomalyAugmentations\n"
     ]
    }
   ],
   "source": [
    "from utils_vae_kitkat_paths import *\n",
    "\n",
    "# Get work directions for augmentated data set:\n",
    "print (\"NORMAL_TRAIN_AUG:      \", NORMAL_TRAIN_AUG)\n",
    "print (\"NORMAL_VALIDATION_AUG: \", NORMAL_VAL_AUG)\n",
    "print (\"NORMAL_TEST_AUG:       \", NORMAL_TEST_AUG)\n",
    "print (\"ANOMALY_AUG:           \", ANOMALY_AUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Which Folder The Models Are Saved In: saved_models/latent128\n"
     ]
    }
   ],
   "source": [
    "# What latent dimension and filter size are we using?:\n",
    "latent_dim = 128\n",
    "filters    = 32\n",
    "\n",
    "# Get work direction for saving files:\n",
    "SAVE_FOLDER = f'saved_models/latent{latent_dim}'\n",
    "print (\"Which Folder The Models Are Saved In:\", SAVE_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] deleting existing files in folder...\n",
      "         done in 0.051 minutes\n"
     ]
    }
   ],
   "source": [
    "# Delete all existing files in folder where models are saved:\n",
    "print(\"[INFO] deleting existing files in folder...\")\n",
    "t0 = time()\n",
    "\n",
    "files = glob.glob(f'{SAVE_FOLDER}/*')\n",
    "for f in files:\n",
    "    os.remove(f)\n",
    "\n",
    "print (\"         done in %0.3f minutes\" % ((time() - t0)/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_VAE_AE import get_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Investigation of Ground Truths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of \"normal\" training images is:     1400\n",
      "Number of \"normal\" validation images is:   134\n",
      "Number of \"normal\" test images is:         266\n",
      "Number of \"anomaly\" images is:             600\n"
     ]
    }
   ],
   "source": [
    "# Glob the directories and get the lists of good and bad images\n",
    "good_train_fns = [f for f in os.listdir(NORMAL_TRAIN_AUG) if not f.startswith('.')]\n",
    "good_val_fns = [f for f in os.listdir(NORMAL_VAL_AUG) if not f.startswith('.')]\n",
    "good_test_fns = [f for f in os.listdir(NORMAL_TEST_AUG) if not f.startswith('.')]\n",
    "bad_fns = [f for f in os.listdir(ANOMALY_AUG) if not f.startswith('.')]\n",
    "\n",
    "print('Number of \"normal\" training images is:     {}'.format(len(good_train_fns)))\n",
    "print('Number of \"normal\" validation images is:   {}'.format(len(good_val_fns)))\n",
    "print('Number of \"normal\" test images is:         {}'.format(len(good_test_fns)))\n",
    "print('Number of \"anomaly\" images is:             {}'.format(len(bad_fns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Procentage of normal samples (ground truth):   75.00 %\n",
      "> Procentage of anomaly samples (ground truth):  25.00 %\n"
     ]
    }
   ],
   "source": [
    "# compute procentage of bad images vs good images:\n",
    "normal_fns = len(good_train_fns) + len(good_val_fns) + len(good_test_fns)\n",
    "anomaly_fns = len(bad_fns)\n",
    "print(f\"> Procentage of normal samples (ground truth):   {(normal_fns / (normal_fns + anomaly_fns)) * 100:.2f} %\")\n",
    "print(f\"> Procentage of anomaly samples (ground truth):  {(anomaly_fns / (normal_fns + anomaly_fns)) * 100:.2f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Normal Data\n",
    "\n",
    "Load normal samples in pre-splitted data sets: train, valdiation and test.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading normal training images...\n",
      "          done\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] loading normal training images...\")\n",
    "trainX = [cv2.imread(file, 0) for file in glob.glob(f\"{NORMAL_TRAIN_AUG}/*.png\")]  # read as grayscale\n",
    "trainX = np.array(trainX)\n",
    "print(\"          done\")\n",
    "\n",
    "# create a corresponding list of labels:\n",
    "trainY = get_labels(trainX, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading normal validation images...\n",
      "          done\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] loading normal validation images...\")\n",
    "x_normal_val = [cv2.imread(file, 0) for file in glob.glob(f\"{NORMAL_VAL_AUG}/*.png\")]  # read as grayscale\n",
    "x_normal_val = np.array(x_normal_val)\n",
    "print(\"          done\")\n",
    "\n",
    "# create a corresponding list of labels:\n",
    "y_normal_val = get_labels(x_normal_val, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading normal testing images...\n",
      "          done\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] loading normal testing images...\")\n",
    "x_normal_test = [cv2.imread(file, 0) for file in glob.glob(f\"{NORMAL_TEST_AUG}/*.png\")]  # read as grayscale\n",
    "x_normal_test = np.array(x_normal_test)\n",
    "print(\"          done\")\n",
    "\n",
    "# create a corresponding list of labels:\n",
    "y_normal_test = get_labels(x_normal_test, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Anomaly Data\n",
    "\n",
    "\n",
    "Load anomaly samples in its singular training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading anomaly images...\n",
      "          done\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] loading anomaly images...\")\n",
    "x_anomaly = [cv2.imread(file, 0) for file in glob.glob(f\"{ANOMALY_AUG}/*.png\")]  # read as grayscale\n",
    "x_anomaly = np.array(x_anomaly)\n",
    "print(\"          done\")\n",
    "\n",
    "# create a corresponding list of labels:\n",
    "y_anomaly = get_labels(x_anomaly, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine normal samples and anomaly samples and split them into training, validation and test sets\n",
    "\n",
    "`label 0` == Normal Samples\n",
    "\n",
    "`label 1` == Anomaly Samples\n",
    "\n",
    "Train and Validation sets only consists of `Normal Data`, aka. `label 0`.\n",
    "\n",
    "Testing sets consists of both  `Normal Data` (aka. `label 0`) and `Anomaly Data` (aka. `label 1`)\n",
    "\n",
    "________________\n",
    "\n",
    "Due to the very sparse original (preprossed) KitKat Chunky data, the validation set will be a mix of normal testing and validation data. The normal testing set will consists of all validation and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine pre-loaded validation and testing set into a new testing set:\n",
    "testvalX = np.vstack((x_normal_val, x_normal_test))\n",
    "testvalY = get_labels(testvalX, 0)\n",
    "\n",
    "# randomly split in order to make new validation set:\n",
    "NoUsageX, valX, NoUsageY, valY = train_test_split(testvalX, testvalY, test_size=0.25, random_state=4345672)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine pre-loaded validation and testing set into a new testing set:\n",
    "testNormalX = np.vstack((x_normal_val, x_normal_test))\n",
    "testNormalY = get_labels(testNormalX, 0)\n",
    "\n",
    "# anomaly class (only used for testing):\n",
    "testAnomalyX, testAnomalyY = x_anomaly, y_anomaly\n",
    "\n",
    "# Combine all testing data in one array:\n",
    "testAllX = np.vstack((testNormalX, testAnomalyX))\n",
    "testAllY = np.hstack((testNormalY, testAnomalyY))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Data Dimensions and Distributions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect data shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:\n",
      " > images: (1400, 128, 128)\n",
      " > labels: (1400,)\n",
      "\n",
      "Validation set:\n",
      " > images: (100, 128, 128)\n",
      " > labels: (100,)\n",
      "\n",
      "Test set:\n",
      " > images: (1000, 128, 128)\n",
      " > labels: (1000,)\n",
      "\t'Normal' test set:\n",
      "\t  > images: (400, 128, 128)\n",
      "\t  > labels: (400,)\n",
      "\t'Anomaly' test set:\n",
      "\t  > images: (600, 128, 128)\n",
      "\t  > labels: (600,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set:\")\n",
    "print(\" > images:\", trainX.shape)\n",
    "print(\" > labels:\", trainY.shape)\n",
    "print (\"\")\n",
    "\n",
    "print(\"Validation set:\")\n",
    "print(\" > images:\", valX.shape)\n",
    "print(\" > labels:\", valY.shape)\n",
    "print (\"\")\n",
    "\n",
    "print(\"Test set:\")\n",
    "print(\" > images:\", testAllX.shape)\n",
    "print(\" > labels:\", testAllY.shape)\n",
    "print(\"\\t'Normal' test set:\")\n",
    "print(\"\\t  > images:\", testNormalX.shape)\n",
    "print(\"\\t  > labels:\", testNormalY.shape)\n",
    "print(\"\\t'Anomaly' test set:\")\n",
    "print(\"\\t  > images:\", testAnomalyX.shape)\n",
    "print(\"\\t  > labels:\", testAnomalyY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procentage of training samples: 73.68 %\n",
      "Procentage of validation samples: 5.26 %\n",
      "Procentage of (only normal) testing samples: 21.05 %\n"
     ]
    }
   ],
   "source": [
    "# compute procentage between training, validation and test data sets (only normal data):\n",
    "print(f\"Procentage of training samples: {(len(trainX) / (len(trainX) + len(valX) + len(testNormalX))) * 100:.2f} %\")\n",
    "print(f\"Procentage of validation samples: {(len(valX) / (len(trainX) + len(valX) + len(testNormalX))) * 100:.2f} %\")\n",
    "print(f\"Procentage of (only normal) testing samples: {(len(testNormalX) / (len(trainX) + len(valX) + len(testNormalX))) * 100:.2f} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procentage of training samples: 56.00 %\n",
      "Procentage of validation samples: 4.00 %\n",
      "Procentage of (total) testing samples: 40.00 %\n"
     ]
    }
   ],
   "source": [
    "# compute procentage between training, validation and test data sets:\n",
    "print(f\"Procentage of training samples: {(len(trainX) / (len(trainX) + len(valX) + len(testAllX))) * 100:.2f} %\")\n",
    "print(f\"Procentage of validation samples: {(len(valX) / (len(trainX) + len(valX) + len(testAllX))) * 100:.2f} %\")\n",
    "print(f\"Procentage of (total) testing samples: {(len(testAllX) / (len(trainX) + len(valX) + len(testAllX))) * 100:.2f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for (un)balanced data\n",
    "\n",
    "In an ideal world the data should be balanced. However in the real world we expect there being around ~$99 \\%$ good samples and only ~$1 \\%$ bad samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels = ['0: normal', '1: anomaly']\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(16,4))\n",
    "ax[0].hist(trainY, align=\"left\"); ax[0].set_title('train', fontsize=18); ax[0].set_xticks([0,1]); ax[0].set_xticklabels(labels); ax[0].tick_params(axis='both', which='major', labelsize=16); ax[0].set_xlim(-0.5, 1.5);\n",
    "ax[1].hist(valY, align=\"left\"); ax[1].set_title('validation', fontsize=18); ax[1].set_xticks([0,1]); ax[1].set_xticklabels(labels); ax[1].tick_params(axis='both', which='major', labelsize=16); ax[1].set_xlim(-0.5, 1.5);\n",
    "ax[2].hist(testAllY, align=\"left\"); ax[2].set_title('test', fontsize=18); ax[2].set_xticks([0,1]); ax[2].set_xticklabels(labels); ax[2].tick_params(axis='both', which='major', labelsize=16); ax[2].set_xlim(-0.5, 1.5);\n",
    "\n",
    "ax[0].set_ylabel(\"Number of images\", fontsize=18)\n",
    "ax[1].set_xlabel(\"Class Labels\", fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procentage of normal test samples: 40.00 %\n",
      "Procentage of total anomaly test samples: 60.00 %\n"
     ]
    }
   ],
   "source": [
    "# compute procentage of normal test images vs total anomaly test images:\n",
    "print(f\"Procentage of normal test samples: {(len(testNormalX) / (len(testNormalX) + len(testAnomalyX))) * 100:.2f} %\")\n",
    "print(f\"Procentage of total anomaly test samples: {(len(testAnomalyX) / (len(testNormalX) + len(testAnomalyX))) * 100:.2f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling\n",
    "\n",
    "Only normalization has been used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data configuration\n",
    "img_width, img_height = trainX.shape[1], trainX.shape[2]      # input image dimensions\n",
    "num_channels = 1                                              # gray-channel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization\n",
    "\n",
    "Neural Networks learns faster with normalized data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize to be in  the [0, 1] range:\n",
    "trainX = trainX.astype('float32') / 255.\n",
    "valX = valX.astype('float32') / 255.\n",
    "testAllX = testAllX.astype('float32') / 255.\n",
    "\n",
    "# reshape data to keras inputs --> (n_samples, img_rows, img_cols, n_channels):\n",
    "trainX = trainX.reshape(trainX.shape[0], img_height, img_width, num_channels)\n",
    "valX = valX.reshape(valX.shape[0], img_height, img_width, num_channels)\n",
    "testAllX = testAllX.reshape(testAllX.shape[0], img_height, img_width, num_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture and Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import architecture of VAE model and its hyperparameters:\n",
    "from J128_VAE_model import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Encoder Part*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_input (InputLayer)      [(None, 128, 128, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 64, 64, 32)   320         encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)         (None, 64, 64, 32)   0           conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 32, 32, 32)   9248        leaky_re_lu[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)       (None, 32, 32, 32)   0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 16, 16, 32)   9248        leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)       (None, 16, 16, 32)   0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 8, 8, 32)     9248        leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)       (None, 8, 8, 32)     0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 4, 4, 32)     9248        leaky_re_lu_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)       (None, 4, 4, 32)     0           conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 512)          0           leaky_re_lu_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "latent (Dense)                  (None, 200)          102600      flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)       (None, 200)          0           latent[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "z_mean (Dense)                  (None, 128)          25728       leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "z_log_mean (Dense)              (None, 128)          25728       leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "z (Lambda)                      (None, 128)          0           z_mean[0][0]                     \n",
      "                                                                 z_log_mean[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 191,368\n",
      "Trainable params: 191,368\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Decoder Part*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "decoder_input (InputLayer)   [(None, 128)]             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               66048     \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 4, 4, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose (Conv2DTran (None, 8, 8, 32)          9248      \n",
      "_________________________________________________________________\n",
      "re_lu (ReLU)                 (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 16, 16, 32)        9248      \n",
      "_________________________________________________________________\n",
      "re_lu_1 (ReLU)               (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTr (None, 32, 32, 32)        9248      \n",
      "_________________________________________________________________\n",
      "re_lu_2 (ReLU)               (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTr (None, 64, 64, 32)        9248      \n",
      "_________________________________________________________________\n",
      "re_lu_3 (ReLU)               (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_4 (Conv2DTr (None, 128, 128, 32)      9248      \n",
      "_________________________________________________________________\n",
      "re_lu_4 (ReLU)               (None, 128, 128, 32)      0         \n",
      "_________________________________________________________________\n",
      "decoder_output (Conv2DTransp (None, 128, 128, 1)       289       \n",
      "=================================================================\n",
      "Total params: 112,577\n",
      "Trainable params: 112,577\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Total VAE Model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vae\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder_input (InputLayer)   [(None, 128, 128, 1)]     0         \n",
      "_________________________________________________________________\n",
      "encoder (Functional)         [(None, 128), (None, 128) 191368    \n",
      "_________________________________________________________________\n",
      "decoder (Functional)         (None, 128, 128, 1)       112577    \n",
      "=================================================================\n",
      "Total params: 303,945\n",
      "Trainable params: 303,945\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vae.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile and train model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Hyperparameters \"\"\"\n",
    "batch_size  = 256\n",
    "epochs      = 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Checkpoint callback options: save model at 1'st epoch (inital model)\n",
    "\n",
    "https://stackoverflow.com/questions/54323960/save-keras-model-at-specific-epochs\n",
    "\"\"\"\n",
    "\n",
    "class CustomSaver(keras.callbacks.Callback):\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        if epoch == 1:\n",
    "            self.model.save_weights(f\"{SAVE_FOLDER}/cp-0001.h5\")\n",
    "            \n",
    "# create and use callback:\n",
    "initial_checkpoint = CustomSaver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Checkpoint callback options: save model at every k'te epochs\n",
    "\"\"\"\n",
    "# Include the epoch in the file name (uses `str.format`):\n",
    "checkpoint_path = \"saved_models/latent128/cp-{epoch:04d}.h5\"\n",
    "\n",
    "# Create a callback that saves the model's weights every k'te epochs:\n",
    "checkpoint = ModelCheckpoint(filepath = checkpoint_path,\n",
    "                             monitor='loss',           # name of the metrics to monitor\n",
    "                             verbose=1, \n",
    "                             #save_best_only=False,     # if False, the best model will not be overridden.\n",
    "                             save_weights_only=True,   # if True, only the weights of the models will be saved.\n",
    "                                                        # if False, the whole models will be saved.\n",
    "                             #mode='auto', \n",
    "                             period=200                  # save after every k'te epochs\n",
    "                            )\n",
    "\n",
    "# Save the weights using the `checkpoint_path` format\n",
    "vae.save_weights(checkpoint_path.format(epoch=0))          # save for zero epoch (inital)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Checkpoint callback options: Early stopping\n",
    "https://www.tensorflow.org/guide/keras/custom_callback\n",
    "\"\"\"\n",
    "\n",
    "class EarlyStoppingAtMinLoss(keras.callbacks.Callback):\n",
    "    \"\"\"Stop training when the loss is at its min, i.e. the loss stops decreasing.\n",
    "\n",
    "  Arguments:\n",
    "      patience: Number of epochs to wait after min has been hit. After this\n",
    "      number of no improvement, training stops.\n",
    "  \"\"\"\n",
    "\n",
    "    def __init__(self, patience=100):\n",
    "        super(EarlyStoppingAtMinLoss, self).__init__()\n",
    "        self.patience = patience\n",
    "        # best_weights to store the weights at which the minimum loss occurs.\n",
    "        self.best_weights = None\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        # The number of epoch it has waited when loss is no longer minimum.\n",
    "        self.wait = 0\n",
    "        # The epoch the training stops at.\n",
    "        self.stopped_epoch = 0\n",
    "        # Initialize the best as infinity.\n",
    "        self.best = np.Inf\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        current = logs.get(\"loss\")\n",
    "        if np.less(current, self.best):\n",
    "            self.best = current\n",
    "            self.wait = 0\n",
    "            # Record the best weights if current results is better (less).\n",
    "            self.best_weights = self.model.get_weights()\n",
    "        else:\n",
    "            self.wait += 1\n",
    "            if self.wait >= self.patience:\n",
    "                self.stopped_epoch = epoch\n",
    "                self.model.stop_training = True\n",
    "                print(\"Restoring model weights from the end of the best epoch.\")\n",
    "                self.model.set_weights(self.best_weights)\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        if self.stopped_epoch > 0:\n",
    "            print(\"Epoch %05d: early stopping\" % (self.stopped_epoch + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Create and Compile Model\"\"\"\n",
    "# import architecture of VAE model and its hyperparameters. learning rate choosen from graph above.\n",
    "vae = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      "6/6 [==============================] - 1s 207ms/step - loss: 3514.8389 - val_loss: 3398.8804\n",
      "Epoch 2/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 3146.1670 - val_loss: 2609.1230\n",
      "Epoch 3/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 1808.4027 - val_loss: 1043.0955\n",
      "Epoch 4/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 871.1457 - val_loss: 894.7533\n",
      "Epoch 5/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 811.0364 - val_loss: 844.0389\n",
      "Epoch 6/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 735.3123 - val_loss: 744.0978\n",
      "Epoch 7/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 652.5979 - val_loss: 680.8438\n",
      "Epoch 8/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 615.9470 - val_loss: 655.5381\n",
      "Epoch 9/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 595.7975 - val_loss: 644.3500\n",
      "Epoch 10/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 584.9921 - val_loss: 633.4528\n",
      "Epoch 11/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 578.6147 - val_loss: 627.0231\n",
      "Epoch 12/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 574.0210 - val_loss: 624.3630\n",
      "Epoch 13/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 569.6882 - val_loss: 618.8286\n",
      "Epoch 14/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 566.0261 - val_loss: 613.5098\n",
      "Epoch 15/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 560.8896 - val_loss: 612.5380\n",
      "Epoch 16/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 557.3749 - val_loss: 605.2852\n",
      "Epoch 17/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 553.0334 - val_loss: 601.0602\n",
      "Epoch 18/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 549.5451 - val_loss: 595.8652\n",
      "Epoch 19/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 535.0396 - val_loss: 572.6227\n",
      "Epoch 20/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 500.7057 - val_loss: 547.8884\n",
      "Epoch 21/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 472.4389 - val_loss: 493.9098\n",
      "Epoch 22/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 428.8389 - val_loss: 430.7316\n",
      "Epoch 23/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 368.2773 - val_loss: 386.4978\n",
      "Epoch 24/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 340.4665 - val_loss: 353.9743\n",
      "Epoch 25/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 316.7604 - val_loss: 339.6148\n",
      "Epoch 26/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 302.3907 - val_loss: 321.1537\n",
      "Epoch 27/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 292.0441 - val_loss: 311.2136\n",
      "Epoch 28/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 282.6525 - val_loss: 298.6142\n",
      "Epoch 29/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 273.6507 - val_loss: 291.1383\n",
      "Epoch 30/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 267.5616 - val_loss: 280.0071\n",
      "Epoch 31/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 259.1743 - val_loss: 276.9432\n",
      "Epoch 32/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 253.7322 - val_loss: 269.2101\n",
      "Epoch 33/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 247.3837 - val_loss: 262.7864\n",
      "Epoch 34/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 242.0182 - val_loss: 256.3357\n",
      "Epoch 35/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 235.4919 - val_loss: 249.9322\n",
      "Epoch 36/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 229.3101 - val_loss: 238.0849\n",
      "Epoch 37/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 219.4190 - val_loss: 241.6281\n",
      "Epoch 38/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 218.4519 - val_loss: 234.2089\n",
      "Epoch 39/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 210.2610 - val_loss: 223.5589\n",
      "Epoch 40/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 201.2633 - val_loss: 216.0701\n",
      "Epoch 41/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 195.1340 - val_loss: 210.1710\n",
      "Epoch 42/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 190.6506 - val_loss: 203.4092\n",
      "Epoch 43/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 186.2659 - val_loss: 205.8352\n",
      "Epoch 44/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 184.4278 - val_loss: 203.5018\n",
      "Epoch 45/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 181.6073 - val_loss: 201.4492\n",
      "Epoch 46/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 177.9466 - val_loss: 196.0700\n",
      "Epoch 47/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 176.0721 - val_loss: 194.5208\n",
      "Epoch 48/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 172.9585 - val_loss: 194.3046\n",
      "Epoch 49/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 172.6349 - val_loss: 190.3086\n",
      "Epoch 50/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 171.4938 - val_loss: 186.6968\n",
      "Epoch 51/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 169.9855 - val_loss: 187.4198\n",
      "Epoch 52/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 168.5170 - val_loss: 189.3668\n",
      "Epoch 53/2000\n",
      "6/6 [==============================] - 1s 105ms/step - loss: 166.4467 - val_loss: 188.4317\n",
      "Epoch 54/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 166.4425 - val_loss: 187.2296\n",
      "Epoch 55/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 167.6174 - val_loss: 189.3440\n",
      "Epoch 56/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 165.6378 - val_loss: 184.2675\n",
      "Epoch 57/2000\n",
      "6/6 [==============================] - 1s 105ms/step - loss: 163.5478 - val_loss: 181.1441\n",
      "Epoch 58/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 163.5212 - val_loss: 183.8915\n",
      "Epoch 59/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 160.7081 - val_loss: 173.2965\n",
      "Epoch 60/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 161.0054 - val_loss: 176.8517\n",
      "Epoch 61/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 159.0338 - val_loss: 175.4789\n",
      "Epoch 62/2000\n",
      "6/6 [==============================] - 1s 106ms/step - loss: 157.9274 - val_loss: 178.7127\n",
      "Epoch 63/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 156.3338 - val_loss: 177.1720\n",
      "Epoch 64/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 155.0773 - val_loss: 173.5522\n",
      "Epoch 65/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 154.5581 - val_loss: 176.1223\n",
      "Epoch 66/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 153.4296 - val_loss: 172.2511\n",
      "Epoch 67/2000\n",
      "6/6 [==============================] - 1s 105ms/step - loss: 152.3867 - val_loss: 165.7995\n",
      "Epoch 68/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 153.4663 - val_loss: 172.4401\n",
      "Epoch 69/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 153.1201 - val_loss: 170.5704\n",
      "Epoch 70/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 151.6151 - val_loss: 172.9509\n",
      "Epoch 71/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 149.2188 - val_loss: 165.0940\n",
      "Epoch 72/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 149.4806 - val_loss: 172.0796\n",
      "Epoch 73/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 148.9679 - val_loss: 169.1456\n",
      "Epoch 74/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 147.1437 - val_loss: 163.1864\n",
      "Epoch 75/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 146.2349 - val_loss: 167.7686\n",
      "Epoch 76/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 144.0804 - val_loss: 162.7409\n",
      "Epoch 77/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 144.4298 - val_loss: 158.2936\n",
      "Epoch 78/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 142.0048 - val_loss: 155.2755\n",
      "Epoch 79/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 141.6052 - val_loss: 156.8277\n",
      "Epoch 80/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 141.8361 - val_loss: 156.3541\n",
      "Epoch 81/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 141.3300 - val_loss: 157.6186\n",
      "Epoch 82/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 140.5053 - val_loss: 154.1391\n",
      "Epoch 83/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 138.5159 - val_loss: 155.6008\n",
      "Epoch 84/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 137.5448 - val_loss: 156.1180\n",
      "Epoch 85/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 138.6136 - val_loss: 157.1661\n",
      "Epoch 86/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 138.0835 - val_loss: 150.7945\n",
      "Epoch 87/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 134.7564 - val_loss: 151.6136\n",
      "Epoch 88/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 133.4613 - val_loss: 149.5485\n",
      "Epoch 89/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 133.8962 - val_loss: 152.1491\n",
      "Epoch 90/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 132.4200 - val_loss: 151.9171\n",
      "Epoch 91/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 132.5701 - val_loss: 148.8968\n",
      "Epoch 92/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 131.8441 - val_loss: 156.2375\n",
      "Epoch 93/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 130.8902 - val_loss: 148.4502\n",
      "Epoch 94/2000\n",
      "6/6 [==============================] - 1s 105ms/step - loss: 130.7754 - val_loss: 143.5065\n",
      "Epoch 95/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 128.5201 - val_loss: 144.5329\n",
      "Epoch 96/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 127.4180 - val_loss: 143.0782\n",
      "Epoch 97/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 127.8856 - val_loss: 142.5330\n",
      "Epoch 98/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 126.1970 - val_loss: 142.4267\n",
      "Epoch 99/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 125.1725 - val_loss: 142.7060\n",
      "Epoch 100/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 126.6799 - val_loss: 140.1442\n",
      "Epoch 101/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 125.5526 - val_loss: 141.2969\n",
      "Epoch 102/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 125.4512 - val_loss: 143.2372\n",
      "Epoch 103/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 124.0010 - val_loss: 136.6523\n",
      "Epoch 104/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 122.8828 - val_loss: 139.5957\n",
      "Epoch 105/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 121.6649 - val_loss: 138.9737\n",
      "Epoch 106/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 122.5823 - val_loss: 135.9269\n",
      "Epoch 107/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 121.7566 - val_loss: 142.3244\n",
      "Epoch 108/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 120.3471 - val_loss: 136.1914\n",
      "Epoch 109/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 119.8274 - val_loss: 136.7944\n",
      "Epoch 110/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 118.9545 - val_loss: 138.7038\n",
      "Epoch 111/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 120.3840 - val_loss: 132.6672\n",
      "Epoch 112/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 118.4449 - val_loss: 131.8493\n",
      "Epoch 113/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 116.8292 - val_loss: 134.6920\n",
      "Epoch 114/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 117.7476 - val_loss: 133.0524\n",
      "Epoch 115/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 115.9856 - val_loss: 130.3637\n",
      "Epoch 116/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 115.2332 - val_loss: 132.7156\n",
      "Epoch 117/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 114.8957 - val_loss: 130.9883\n",
      "Epoch 118/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 114.6070 - val_loss: 129.0141\n",
      "Epoch 119/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 112.9999 - val_loss: 128.5249\n",
      "Epoch 120/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 113.4116 - val_loss: 127.5535\n",
      "Epoch 121/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 112.2911 - val_loss: 125.6507\n",
      "Epoch 122/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 111.9907 - val_loss: 123.1068\n",
      "Epoch 123/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 111.9545 - val_loss: 126.9713\n",
      "Epoch 124/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 111.1851 - val_loss: 126.8698\n",
      "Epoch 125/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 110.5119 - val_loss: 124.4749\n",
      "Epoch 126/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 109.5757 - val_loss: 126.4107\n",
      "Epoch 127/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 109.9420 - val_loss: 121.2719\n",
      "Epoch 128/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 108.0785 - val_loss: 124.6102\n",
      "Epoch 129/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 107.9758 - val_loss: 121.5901\n",
      "Epoch 130/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 105.9647 - val_loss: 122.9375\n",
      "Epoch 131/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 107.1514 - val_loss: 123.7084\n",
      "Epoch 132/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 106.7108 - val_loss: 122.0158\n",
      "Epoch 133/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 105.3176 - val_loss: 119.3951\n",
      "Epoch 134/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 105.2284 - val_loss: 118.8206\n",
      "Epoch 135/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 103.4644 - val_loss: 119.9528\n",
      "Epoch 136/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 104.5616 - val_loss: 120.1900\n",
      "Epoch 137/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 103.6360 - val_loss: 117.8797\n",
      "Epoch 138/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 103.0228 - val_loss: 117.0971\n",
      "Epoch 139/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 103.7206 - val_loss: 117.2569\n",
      "Epoch 140/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 103.1715 - val_loss: 116.1729\n",
      "Epoch 141/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 103.0641 - val_loss: 114.9066\n",
      "Epoch 142/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 100.7933 - val_loss: 114.1144\n",
      "Epoch 143/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 101.6030 - val_loss: 117.0847\n",
      "Epoch 144/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 100.6810 - val_loss: 113.6020\n",
      "Epoch 145/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 100.0670 - val_loss: 115.0835\n",
      "Epoch 146/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 100.7272 - val_loss: 112.3676\n",
      "Epoch 147/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 100.0864 - val_loss: 112.5884\n",
      "Epoch 148/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 98.2279 - val_loss: 114.8561\n",
      "Epoch 149/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 97.9165 - val_loss: 110.8093\n",
      "Epoch 150/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 96.7409 - val_loss: 111.7922\n",
      "Epoch 151/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 96.8987 - val_loss: 109.4219\n",
      "Epoch 152/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 97.0338 - val_loss: 110.0555\n",
      "Epoch 153/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 95.5652 - val_loss: 109.2031\n",
      "Epoch 154/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 95.5859 - val_loss: 107.7141\n",
      "Epoch 155/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 95.3762 - val_loss: 105.8037\n",
      "Epoch 156/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 93.8187 - val_loss: 111.9520\n",
      "Epoch 157/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 95.0166 - val_loss: 107.5047\n",
      "Epoch 158/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 93.5263 - val_loss: 112.3900\n",
      "Epoch 159/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 93.1719 - val_loss: 106.4601\n",
      "Epoch 160/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 93.6204 - val_loss: 105.5773\n",
      "Epoch 161/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 92.2190 - val_loss: 103.8576\n",
      "Epoch 162/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 91.4023 - val_loss: 106.1820\n",
      "Epoch 163/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 91.9026 - val_loss: 105.2220\n",
      "Epoch 164/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 91.2848 - val_loss: 107.3587\n",
      "Epoch 165/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 91.4935 - val_loss: 102.9837\n",
      "Epoch 166/2000\n",
      "6/6 [==============================] - 1s 105ms/step - loss: 90.3519 - val_loss: 103.2159\n",
      "Epoch 167/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 89.6247 - val_loss: 103.7834\n",
      "Epoch 168/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 89.8499 - val_loss: 101.0944\n",
      "Epoch 169/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 88.8959 - val_loss: 101.9910\n",
      "Epoch 170/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 88.3311 - val_loss: 101.3543\n",
      "Epoch 171/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 87.9455 - val_loss: 102.7635\n",
      "Epoch 172/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 87.0523 - val_loss: 99.6335\n",
      "Epoch 173/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 86.3858 - val_loss: 99.1500\n",
      "Epoch 174/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 86.0402 - val_loss: 99.3734\n",
      "Epoch 175/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 86.0778 - val_loss: 99.7442\n",
      "Epoch 176/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 85.5712 - val_loss: 97.4121\n",
      "Epoch 177/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 85.2909 - val_loss: 97.3986\n",
      "Epoch 178/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 85.4520 - val_loss: 96.4378\n",
      "Epoch 179/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 84.9675 - val_loss: 95.8399\n",
      "Epoch 180/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 83.5805 - val_loss: 96.3834\n",
      "Epoch 181/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 83.0544 - val_loss: 94.2707\n",
      "Epoch 182/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 83.0175 - val_loss: 94.8190\n",
      "Epoch 183/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 83.2321 - val_loss: 94.4220\n",
      "Epoch 184/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 82.5527 - val_loss: 96.0608\n",
      "Epoch 185/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 82.5873 - val_loss: 94.8034\n",
      "Epoch 186/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 81.9473 - val_loss: 94.3052\n",
      "Epoch 187/2000\n",
      "6/6 [==============================] - 1s 105ms/step - loss: 80.7139 - val_loss: 93.1003\n",
      "Epoch 188/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 80.7200 - val_loss: 93.7068\n",
      "Epoch 189/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 80.9835 - val_loss: 93.1991\n",
      "Epoch 190/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 78.8744 - val_loss: 90.7507\n",
      "Epoch 191/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 77.1365 - val_loss: 86.2038\n",
      "Epoch 192/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 73.0631 - val_loss: 79.4265\n",
      "Epoch 193/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 68.3919 - val_loss: 74.0078\n",
      "Epoch 194/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 65.2023 - val_loss: 69.0289\n",
      "Epoch 195/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 63.9395 - val_loss: 68.5749\n",
      "Epoch 196/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 62.4508 - val_loss: 61.4100\n",
      "Epoch 197/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 61.3877 - val_loss: 63.5022\n",
      "Epoch 198/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 61.4943 - val_loss: 61.7768\n",
      "Epoch 199/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 61.5594 - val_loss: 64.0416\n",
      "Epoch 200/2000\n",
      "6/6 [==============================] - ETA: 0s - loss: 60.7514\n",
      "Epoch 00200: saving model to saved_models/latent128/cp-0200.h5\n",
      "6/6 [==============================] - 1s 165ms/step - loss: 60.7514 - val_loss: 61.4293\n",
      "Epoch 201/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 60.8304 - val_loss: 61.0100\n",
      "Epoch 202/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 60.0558 - val_loss: 61.3847\n",
      "Epoch 203/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 59.7459 - val_loss: 62.0410\n",
      "Epoch 204/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 59.4451 - val_loss: 60.6033\n",
      "Epoch 205/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 59.0955 - val_loss: 59.6690\n",
      "Epoch 206/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 58.5565 - val_loss: 60.9112\n",
      "Epoch 207/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 58.8483 - val_loss: 59.8154\n",
      "Epoch 208/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 57.3703 - val_loss: 59.4690\n",
      "Epoch 209/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 58.1435 - val_loss: 57.1056\n",
      "Epoch 210/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 57.7547 - val_loss: 58.7510\n",
      "Epoch 211/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 57.8385 - val_loss: 58.7952\n",
      "Epoch 212/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 57.1231 - val_loss: 58.5482\n",
      "Epoch 213/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 56.6066 - val_loss: 59.7584\n",
      "Epoch 214/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 56.2477 - val_loss: 56.7095\n",
      "Epoch 215/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 55.9953 - val_loss: 57.4616\n",
      "Epoch 216/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 56.4636 - val_loss: 56.5184\n",
      "Epoch 217/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 55.0828 - val_loss: 56.1594\n",
      "Epoch 218/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 55.4463 - val_loss: 55.3648\n",
      "Epoch 219/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 54.3259 - val_loss: 55.8791\n",
      "Epoch 220/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 54.1510 - val_loss: 54.1243\n",
      "Epoch 221/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 53.6519 - val_loss: 54.9230\n",
      "Epoch 222/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 54.0903 - val_loss: 55.5300\n",
      "Epoch 223/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 53.7743 - val_loss: 55.1400\n",
      "Epoch 224/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 53.4566 - val_loss: 55.5801\n",
      "Epoch 225/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 52.8809 - val_loss: 53.5791\n",
      "Epoch 226/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 52.3606 - val_loss: 53.1635\n",
      "Epoch 227/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 52.1622 - val_loss: 52.0915\n",
      "Epoch 228/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 51.8425 - val_loss: 52.9217\n",
      "Epoch 229/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 51.4921 - val_loss: 53.2756\n",
      "Epoch 230/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 51.0591 - val_loss: 53.0999\n",
      "Epoch 231/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 50.8042 - val_loss: 52.8362\n",
      "Epoch 232/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 50.1283 - val_loss: 52.9487\n",
      "Epoch 233/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 51.0012 - val_loss: 53.5462\n",
      "Epoch 234/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 50.2927 - val_loss: 53.6248\n",
      "Epoch 235/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 50.3284 - val_loss: 51.6696\n",
      "Epoch 236/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 49.7546 - val_loss: 51.0405\n",
      "Epoch 237/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 49.9657 - val_loss: 51.9207\n",
      "Epoch 238/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 49.8283 - val_loss: 50.1878\n",
      "Epoch 239/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 49.0997 - val_loss: 50.8660\n",
      "Epoch 240/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 49.1131 - val_loss: 50.7491\n",
      "Epoch 241/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 48.7831 - val_loss: 50.5658\n",
      "Epoch 242/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 48.7133 - val_loss: 49.6839\n",
      "Epoch 243/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 48.3392 - val_loss: 51.3539\n",
      "Epoch 244/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 49.0322 - val_loss: 50.2948\n",
      "Epoch 245/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 48.3328 - val_loss: 51.7682\n",
      "Epoch 246/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 47.7432 - val_loss: 49.7143\n",
      "Epoch 247/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 48.0727 - val_loss: 48.2870\n",
      "Epoch 248/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 47.9623 - val_loss: 48.6920\n",
      "Epoch 249/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 47.5860 - val_loss: 50.1270\n",
      "Epoch 250/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 47.4862 - val_loss: 48.4345\n",
      "Epoch 251/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 47.7036 - val_loss: 49.3401\n",
      "Epoch 252/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 47.6865 - val_loss: 47.8393\n",
      "Epoch 253/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 46.8245 - val_loss: 50.1418\n",
      "Epoch 254/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 46.7819 - val_loss: 49.2331\n",
      "Epoch 255/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.8112 - val_loss: 49.4886\n",
      "Epoch 256/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 46.5525 - val_loss: 49.3151\n",
      "Epoch 257/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 46.2736 - val_loss: 47.9789\n",
      "Epoch 258/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.4330 - val_loss: 47.7280\n",
      "Epoch 259/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 45.8027 - val_loss: 48.1701\n",
      "Epoch 260/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.0345 - val_loss: 47.9254\n",
      "Epoch 261/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.7161 - val_loss: 50.2486\n",
      "Epoch 262/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.5708 - val_loss: 46.5939\n",
      "Epoch 263/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 45.6792 - val_loss: 47.0304\n",
      "Epoch 264/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 45.5666 - val_loss: 47.3017\n",
      "Epoch 265/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 45.1494 - val_loss: 48.0871\n",
      "Epoch 266/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 45.5535 - val_loss: 46.8287\n",
      "Epoch 267/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 45.2157 - val_loss: 46.0684\n",
      "Epoch 268/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 45.0213 - val_loss: 47.9275\n",
      "Epoch 269/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 44.9298 - val_loss: 45.8728\n",
      "Epoch 270/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 45.0167 - val_loss: 48.0421\n",
      "Epoch 271/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 45.6466 - val_loss: 46.6819\n",
      "Epoch 272/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 45.7461 - val_loss: 47.6062\n",
      "Epoch 273/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 45.5064 - val_loss: 48.4818\n",
      "Epoch 274/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 45.2664 - val_loss: 46.5449\n",
      "Epoch 275/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 44.2739 - val_loss: 46.0140\n",
      "Epoch 276/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 44.4253 - val_loss: 47.2448\n",
      "Epoch 277/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 44.6715 - val_loss: 46.0930\n",
      "Epoch 278/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 44.0537 - val_loss: 46.2764\n",
      "Epoch 279/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 44.1090 - val_loss: 47.4162\n",
      "Epoch 280/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 43.6754 - val_loss: 45.1884\n",
      "Epoch 281/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 43.8792 - val_loss: 45.5319\n",
      "Epoch 282/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 43.7043 - val_loss: 45.8794\n",
      "Epoch 283/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 43.8843 - val_loss: 45.7233\n",
      "Epoch 284/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 43.3332 - val_loss: 43.5437\n",
      "Epoch 285/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 43.3563 - val_loss: 45.6654\n",
      "Epoch 286/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 43.6549 - val_loss: 45.0137\n",
      "Epoch 287/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 43.3046 - val_loss: 46.4366\n",
      "Epoch 288/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 43.1368 - val_loss: 46.4994\n",
      "Epoch 289/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 43.9716 - val_loss: 46.7508\n",
      "Epoch 290/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 43.4804 - val_loss: 44.7396\n",
      "Epoch 291/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 43.0490 - val_loss: 43.9284\n",
      "Epoch 292/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 42.6860 - val_loss: 44.5401\n",
      "Epoch 293/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 43.0098 - val_loss: 44.8955\n",
      "Epoch 294/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 42.6309 - val_loss: 45.2161\n",
      "Epoch 295/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 42.8073 - val_loss: 44.2203\n",
      "Epoch 296/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 43.2025 - val_loss: 45.7571\n",
      "Epoch 297/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 43.2790 - val_loss: 44.9616\n",
      "Epoch 298/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 42.9978 - val_loss: 45.3034\n",
      "Epoch 299/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 43.0835 - val_loss: 44.3200\n",
      "Epoch 300/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 42.2244 - val_loss: 45.6644\n",
      "Epoch 301/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 41.8452 - val_loss: 43.4603\n",
      "Epoch 302/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 41.9924 - val_loss: 44.9218\n",
      "Epoch 303/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 41.5579 - val_loss: 44.1221\n",
      "Epoch 304/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 41.9218 - val_loss: 43.6463\n",
      "Epoch 305/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 41.4850 - val_loss: 44.0090\n",
      "Epoch 306/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 41.6120 - val_loss: 43.2113\n",
      "Epoch 307/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 41.5181 - val_loss: 43.1960\n",
      "Epoch 308/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 41.3107 - val_loss: 44.2149\n",
      "Epoch 309/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 40.6125 - val_loss: 42.3101\n",
      "Epoch 310/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 41.0818 - val_loss: 42.4460\n",
      "Epoch 311/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 40.6292 - val_loss: 43.6904\n",
      "Epoch 312/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 40.7603 - val_loss: 43.1173\n",
      "Epoch 313/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 40.5316 - val_loss: 43.3512\n",
      "Epoch 314/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 40.8635 - val_loss: 42.5481\n",
      "Epoch 315/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 40.7887 - val_loss: 42.7910\n",
      "Epoch 316/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 40.9720 - val_loss: 41.4770\n",
      "Epoch 317/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 40.6802 - val_loss: 42.2214\n",
      "Epoch 318/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 40.6584 - val_loss: 43.9083\n",
      "Epoch 319/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 40.5682 - val_loss: 42.2208\n",
      "Epoch 320/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 40.1013 - val_loss: 41.8926\n",
      "Epoch 321/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 40.2775 - val_loss: 41.8361\n",
      "Epoch 322/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 40.2061 - val_loss: 42.2765\n",
      "Epoch 323/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 40.1868 - val_loss: 42.3393\n",
      "Epoch 324/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 39.9197 - val_loss: 41.2917\n",
      "Epoch 325/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 40.1516 - val_loss: 42.9821\n",
      "Epoch 326/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 39.8913 - val_loss: 41.7762\n",
      "Epoch 327/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 40.1408 - val_loss: 40.9657\n",
      "Epoch 328/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 40.0146 - val_loss: 41.1488\n",
      "Epoch 329/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 40.1419 - val_loss: 41.7629\n",
      "Epoch 330/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 40.3036 - val_loss: 42.3667\n",
      "Epoch 331/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 40.2116 - val_loss: 41.8912\n",
      "Epoch 332/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 39.4684 - val_loss: 42.0224\n",
      "Epoch 333/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 39.2886 - val_loss: 40.8025\n",
      "Epoch 334/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 39.1200 - val_loss: 41.3808\n",
      "Epoch 335/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 39.6661 - val_loss: 41.9570\n",
      "Epoch 336/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 39.7939 - val_loss: 41.5554\n",
      "Epoch 337/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 39.5002 - val_loss: 41.5580\n",
      "Epoch 338/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 39.3110 - val_loss: 40.8395\n",
      "Epoch 339/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 39.3060 - val_loss: 41.3664\n",
      "Epoch 340/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 38.8156 - val_loss: 42.1490\n",
      "Epoch 341/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 38.9695 - val_loss: 42.4634\n",
      "Epoch 342/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 38.8827 - val_loss: 40.6199\n",
      "Epoch 343/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 39.2385 - val_loss: 40.8366\n",
      "Epoch 344/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 38.7808 - val_loss: 40.1674\n",
      "Epoch 345/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 38.9898 - val_loss: 42.2312\n",
      "Epoch 346/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 39.2555 - val_loss: 40.5104\n",
      "Epoch 347/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 39.2178 - val_loss: 41.6872\n",
      "Epoch 348/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 38.8699 - val_loss: 41.5144\n",
      "Epoch 349/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 39.1580 - val_loss: 40.7121\n",
      "Epoch 350/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 38.6224 - val_loss: 40.2363\n",
      "Epoch 351/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 39.2412 - val_loss: 39.6786\n",
      "Epoch 352/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 38.3310 - val_loss: 40.3389\n",
      "Epoch 353/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 38.2034 - val_loss: 39.7302\n",
      "Epoch 354/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 37.8690 - val_loss: 41.3290\n",
      "Epoch 355/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 37.9393 - val_loss: 40.4165\n",
      "Epoch 356/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 37.7922 - val_loss: 39.2166\n",
      "Epoch 357/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 38.0642 - val_loss: 40.1796\n",
      "Epoch 358/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 38.3097 - val_loss: 39.6775\n",
      "Epoch 359/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 38.1309 - val_loss: 39.5400\n",
      "Epoch 360/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 38.0701 - val_loss: 40.1862\n",
      "Epoch 361/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 38.0847 - val_loss: 40.7000\n",
      "Epoch 362/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 37.7222 - val_loss: 39.2628\n",
      "Epoch 363/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 37.4122 - val_loss: 38.9669\n",
      "Epoch 364/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 38.0772 - val_loss: 40.6203\n",
      "Epoch 365/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 37.7970 - val_loss: 39.2212\n",
      "Epoch 366/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 37.5007 - val_loss: 39.5554\n",
      "Epoch 367/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 37.6530 - val_loss: 38.6940\n",
      "Epoch 368/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 37.6079 - val_loss: 38.9177\n",
      "Epoch 369/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 37.1851 - val_loss: 39.1661\n",
      "Epoch 370/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 37.3849 - val_loss: 39.1790\n",
      "Epoch 371/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 37.2374 - val_loss: 39.5960\n",
      "Epoch 372/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 37.3049 - val_loss: 38.9620\n",
      "Epoch 373/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 36.9640 - val_loss: 38.4896\n",
      "Epoch 374/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 36.7380 - val_loss: 40.2551\n",
      "Epoch 375/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 37.2584 - val_loss: 38.6540\n",
      "Epoch 376/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 36.9069 - val_loss: 39.0747\n",
      "Epoch 377/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 37.3003 - val_loss: 38.5457\n",
      "Epoch 378/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 37.1732 - val_loss: 38.9029\n",
      "Epoch 379/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 36.7355 - val_loss: 37.9621\n",
      "Epoch 380/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 36.7619 - val_loss: 38.8794\n",
      "Epoch 381/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 36.6939 - val_loss: 39.1062\n",
      "Epoch 382/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 36.8095 - val_loss: 39.4841\n",
      "Epoch 383/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 36.6894 - val_loss: 39.0448\n",
      "Epoch 384/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 36.8870 - val_loss: 39.3259\n",
      "Epoch 385/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 36.9493 - val_loss: 39.0140\n",
      "Epoch 386/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 36.9687 - val_loss: 38.8862\n",
      "Epoch 387/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 36.5494 - val_loss: 38.9864\n",
      "Epoch 388/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 36.3001 - val_loss: 37.8035\n",
      "Epoch 389/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 36.2712 - val_loss: 38.8472\n",
      "Epoch 390/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 36.3529 - val_loss: 37.1582\n",
      "Epoch 391/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 36.3353 - val_loss: 38.9955\n",
      "Epoch 392/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 36.4242 - val_loss: 39.0406\n",
      "Epoch 393/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 36.7823 - val_loss: 37.8822\n",
      "Epoch 394/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 36.0022 - val_loss: 38.5598\n",
      "Epoch 395/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 36.3923 - val_loss: 38.8995\n",
      "Epoch 396/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 36.3956 - val_loss: 37.9477\n",
      "Epoch 397/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 36.0794 - val_loss: 39.9184\n",
      "Epoch 398/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 35.8366 - val_loss: 37.9402\n",
      "Epoch 399/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 35.9226 - val_loss: 38.1585\n",
      "Epoch 400/2000\n",
      "6/6 [==============================] - ETA: 0s - loss: 35.8881\n",
      "Epoch 00400: saving model to saved_models/latent128/cp-0400.h5\n",
      "6/6 [==============================] - 1s 144ms/step - loss: 35.8881 - val_loss: 37.3052\n",
      "Epoch 401/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 35.7378 - val_loss: 38.7413\n",
      "Epoch 402/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 35.5849 - val_loss: 37.2558\n",
      "Epoch 403/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 35.5976 - val_loss: 37.8481\n",
      "Epoch 404/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 35.3683 - val_loss: 37.5215\n",
      "Epoch 405/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 35.5426 - val_loss: 38.3926\n",
      "Epoch 406/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 35.3394 - val_loss: 37.6188\n",
      "Epoch 407/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 35.5698 - val_loss: 37.6855\n",
      "Epoch 408/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 35.5585 - val_loss: 37.3307\n",
      "Epoch 409/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 35.9552 - val_loss: 37.4510\n",
      "Epoch 410/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 35.9823 - val_loss: 37.5368\n",
      "Epoch 411/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 35.9555 - val_loss: 37.7976\n",
      "Epoch 412/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 35.4993 - val_loss: 37.7091\n",
      "Epoch 413/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 35.3528 - val_loss: 37.7738\n",
      "Epoch 414/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 35.2506 - val_loss: 36.7978\n",
      "Epoch 415/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 35.2501 - val_loss: 37.2077\n",
      "Epoch 416/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 35.3190 - val_loss: 37.4216\n",
      "Epoch 417/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 35.2767 - val_loss: 37.4123\n",
      "Epoch 418/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 35.2214 - val_loss: 37.1086\n",
      "Epoch 419/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 35.2426 - val_loss: 36.9941\n",
      "Epoch 420/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 35.8670 - val_loss: 38.6708\n",
      "Epoch 421/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 35.4654 - val_loss: 37.2382\n",
      "Epoch 422/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 35.3931 - val_loss: 37.6256\n",
      "Epoch 423/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 35.0264 - val_loss: 36.2572\n",
      "Epoch 424/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 34.8126 - val_loss: 36.5007\n",
      "Epoch 425/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 34.6869 - val_loss: 36.9359\n",
      "Epoch 426/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 34.6176 - val_loss: 36.3780\n",
      "Epoch 427/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 34.7604 - val_loss: 37.3728\n",
      "Epoch 428/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 34.8032 - val_loss: 37.4532\n",
      "Epoch 429/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 34.7444 - val_loss: 37.6336\n",
      "Epoch 430/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 34.6968 - val_loss: 36.9690\n",
      "Epoch 431/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 34.7420 - val_loss: 36.7092\n",
      "Epoch 432/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 34.8711 - val_loss: 36.2576\n",
      "Epoch 433/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 34.6133 - val_loss: 36.4691\n",
      "Epoch 434/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 34.2805 - val_loss: 36.9742\n",
      "Epoch 435/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 34.6792 - val_loss: 35.4279\n",
      "Epoch 436/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 34.7331 - val_loss: 35.3512\n",
      "Epoch 437/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 34.3665 - val_loss: 36.2633\n",
      "Epoch 438/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 34.1022 - val_loss: 36.0814\n",
      "Epoch 439/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 34.6636 - val_loss: 35.9982\n",
      "Epoch 440/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 33.9421 - val_loss: 35.9415\n",
      "Epoch 441/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 34.2073 - val_loss: 36.4701\n",
      "Epoch 442/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 34.0625 - val_loss: 35.1365\n",
      "Epoch 443/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 33.9470 - val_loss: 36.2454\n",
      "Epoch 444/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 33.9191 - val_loss: 37.7308\n",
      "Epoch 445/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 34.4117 - val_loss: 36.0750\n",
      "Epoch 446/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 33.7846 - val_loss: 37.5660\n",
      "Epoch 447/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 33.8955 - val_loss: 35.0787\n",
      "Epoch 448/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 33.7649 - val_loss: 37.4692\n",
      "Epoch 449/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 33.6301 - val_loss: 35.9710\n",
      "Epoch 450/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 33.3934 - val_loss: 36.6192\n",
      "Epoch 451/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 33.8634 - val_loss: 35.0314\n",
      "Epoch 452/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 33.7478 - val_loss: 35.4042\n",
      "Epoch 453/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 33.6141 - val_loss: 36.5571\n",
      "Epoch 454/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 33.6130 - val_loss: 36.8226\n",
      "Epoch 455/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 33.7362 - val_loss: 35.3289\n",
      "Epoch 456/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 33.8423 - val_loss: 35.8706\n",
      "Epoch 457/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 33.6652 - val_loss: 34.9540\n",
      "Epoch 458/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 33.8194 - val_loss: 35.6769\n",
      "Epoch 459/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 34.1236 - val_loss: 35.6149\n",
      "Epoch 460/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 33.6919 - val_loss: 36.9724\n",
      "Epoch 461/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 33.6311 - val_loss: 36.7629\n",
      "Epoch 462/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 33.7170 - val_loss: 34.9160\n",
      "Epoch 463/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 33.4009 - val_loss: 34.7983\n",
      "Epoch 464/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 33.5299 - val_loss: 35.6638\n",
      "Epoch 465/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 33.7271 - val_loss: 35.2411\n",
      "Epoch 466/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 33.8314 - val_loss: 35.1160\n",
      "Epoch 467/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 33.3181 - val_loss: 35.4789\n",
      "Epoch 468/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 33.2533 - val_loss: 36.0820\n",
      "Epoch 469/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 33.3232 - val_loss: 35.3015\n",
      "Epoch 470/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 33.2644 - val_loss: 35.8961\n",
      "Epoch 471/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 33.2893 - val_loss: 33.6862\n",
      "Epoch 472/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 33.2302 - val_loss: 34.7626\n",
      "Epoch 473/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 33.3102 - val_loss: 36.0114\n",
      "Epoch 474/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 33.0597 - val_loss: 35.0040\n",
      "Epoch 475/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 33.3155 - val_loss: 35.4877\n",
      "Epoch 476/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 33.0968 - val_loss: 35.6345\n",
      "Epoch 477/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 33.0016 - val_loss: 34.9410\n",
      "Epoch 478/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 32.8305 - val_loss: 34.0939\n",
      "Epoch 479/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 32.9423 - val_loss: 34.3699\n",
      "Epoch 480/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 32.9126 - val_loss: 35.1794\n",
      "Epoch 481/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 32.8274 - val_loss: 35.3739\n",
      "Epoch 482/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 33.1575 - val_loss: 34.9069\n",
      "Epoch 483/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 32.7243 - val_loss: 34.2673\n",
      "Epoch 484/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 32.8358 - val_loss: 34.9916\n",
      "Epoch 485/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 32.7165 - val_loss: 34.3059\n",
      "Epoch 486/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 32.9099 - val_loss: 34.4193\n",
      "Epoch 487/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 32.9168 - val_loss: 34.8571\n",
      "Epoch 488/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 32.9484 - val_loss: 35.7282\n",
      "Epoch 489/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 32.9745 - val_loss: 35.1272\n",
      "Epoch 490/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 33.0186 - val_loss: 35.2776\n",
      "Epoch 491/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 32.7728 - val_loss: 34.5954\n",
      "Epoch 492/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 33.1065 - val_loss: 35.0436\n",
      "Epoch 493/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 32.8939 - val_loss: 35.8045\n",
      "Epoch 494/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 32.8152 - val_loss: 35.0612\n",
      "Epoch 495/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 32.6467 - val_loss: 34.0214\n",
      "Epoch 496/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 32.6329 - val_loss: 33.8636\n",
      "Epoch 497/2000\n",
      "6/6 [==============================] - 1s 105ms/step - loss: 32.1225 - val_loss: 33.6091\n",
      "Epoch 498/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 32.1299 - val_loss: 34.1499\n",
      "Epoch 499/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 32.2970 - val_loss: 34.5221\n",
      "Epoch 500/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 32.1341 - val_loss: 35.2511\n",
      "Epoch 501/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 32.3283 - val_loss: 34.3458\n",
      "Epoch 502/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 32.6197 - val_loss: 33.8063\n",
      "Epoch 503/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 32.2329 - val_loss: 35.1471\n",
      "Epoch 504/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 32.7363 - val_loss: 33.9892\n",
      "Epoch 505/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 32.2162 - val_loss: 35.1643\n",
      "Epoch 506/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 31.9296 - val_loss: 33.7891\n",
      "Epoch 507/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.9511 - val_loss: 33.8461\n",
      "Epoch 508/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 31.8974 - val_loss: 33.2238\n",
      "Epoch 509/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 32.2449 - val_loss: 34.8443\n",
      "Epoch 510/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 32.2487 - val_loss: 33.8952\n",
      "Epoch 511/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 32.0768 - val_loss: 34.8122\n",
      "Epoch 512/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 32.0106 - val_loss: 34.5088\n",
      "Epoch 513/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.9188 - val_loss: 33.9653\n",
      "Epoch 514/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 31.7813 - val_loss: 33.5077\n",
      "Epoch 515/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.9561 - val_loss: 33.8687\n",
      "Epoch 516/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.8886 - val_loss: 33.1910\n",
      "Epoch 517/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.8462 - val_loss: 33.8069\n",
      "Epoch 518/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.8150 - val_loss: 34.2630\n",
      "Epoch 519/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.9554 - val_loss: 33.4854\n",
      "Epoch 520/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 31.7209 - val_loss: 33.4703\n",
      "Epoch 521/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 31.6137 - val_loss: 34.2590\n",
      "Epoch 522/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.6455 - val_loss: 34.0059\n",
      "Epoch 523/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.9883 - val_loss: 33.0840\n",
      "Epoch 524/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.9225 - val_loss: 33.4207\n",
      "Epoch 525/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 31.4825 - val_loss: 33.1843\n",
      "Epoch 526/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.6822 - val_loss: 34.3441\n",
      "Epoch 527/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.6110 - val_loss: 34.0331\n",
      "Epoch 528/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.6954 - val_loss: 34.0220\n",
      "Epoch 529/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.8236 - val_loss: 32.8995\n",
      "Epoch 530/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.7148 - val_loss: 33.6710\n",
      "Epoch 531/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.5471 - val_loss: 33.2144\n",
      "Epoch 532/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.5304 - val_loss: 34.1919\n",
      "Epoch 533/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.8653 - val_loss: 34.6281\n",
      "Epoch 534/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.8332 - val_loss: 33.6495\n",
      "Epoch 535/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.7054 - val_loss: 33.2063\n",
      "Epoch 536/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.7403 - val_loss: 34.5068\n",
      "Epoch 537/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.6890 - val_loss: 33.0778\n",
      "Epoch 538/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.6836 - val_loss: 33.8534\n",
      "Epoch 539/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.6235 - val_loss: 33.8131\n",
      "Epoch 540/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.5770 - val_loss: 33.9452\n",
      "Epoch 541/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.5241 - val_loss: 33.1946\n",
      "Epoch 542/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 31.4134 - val_loss: 32.3517\n",
      "Epoch 543/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.5810 - val_loss: 33.4626\n",
      "Epoch 544/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 31.1448 - val_loss: 32.2585\n",
      "Epoch 545/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.3770 - val_loss: 33.8068\n",
      "Epoch 546/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.6638 - val_loss: 33.5630\n",
      "Epoch 547/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.4248 - val_loss: 33.1663\n",
      "Epoch 548/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.4550 - val_loss: 32.8741\n",
      "Epoch 549/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.4155 - val_loss: 33.2595\n",
      "Epoch 550/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.3205 - val_loss: 32.9889\n",
      "Epoch 551/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.2273 - val_loss: 33.6582\n",
      "Epoch 552/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 31.1203 - val_loss: 32.6551\n",
      "Epoch 553/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 30.9958 - val_loss: 32.6324\n",
      "Epoch 554/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.1040 - val_loss: 33.7761\n",
      "Epoch 555/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.0684 - val_loss: 33.2053\n",
      "Epoch 556/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.0135 - val_loss: 32.7024\n",
      "Epoch 557/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.1611 - val_loss: 34.2345\n",
      "Epoch 558/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.6048 - val_loss: 35.0476\n",
      "Epoch 559/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.5872 - val_loss: 32.6712\n",
      "Epoch 560/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.1917 - val_loss: 33.3610\n",
      "Epoch 561/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.1842 - val_loss: 33.0161\n",
      "Epoch 562/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 31.1339 - val_loss: 33.6461\n",
      "Epoch 563/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.0588 - val_loss: 33.8497\n",
      "Epoch 564/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 31.3249 - val_loss: 33.0250\n",
      "Epoch 565/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 30.8496 - val_loss: 33.0990\n",
      "Epoch 566/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.8599 - val_loss: 32.5205\n",
      "Epoch 567/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.9562 - val_loss: 32.9393\n",
      "Epoch 568/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 30.6683 - val_loss: 32.3997\n",
      "Epoch 569/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.8842 - val_loss: 32.1655\n",
      "Epoch 570/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 30.4632 - val_loss: 33.6003\n",
      "Epoch 571/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.6286 - val_loss: 32.3169\n",
      "Epoch 572/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.6564 - val_loss: 32.9951\n",
      "Epoch 573/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 30.4540 - val_loss: 31.7719\n",
      "Epoch 574/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.5049 - val_loss: 33.0706\n",
      "Epoch 575/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 30.3938 - val_loss: 32.4725\n",
      "Epoch 576/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.7564 - val_loss: 33.2383\n",
      "Epoch 577/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.7719 - val_loss: 32.7074\n",
      "Epoch 578/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.7277 - val_loss: 32.0005\n",
      "Epoch 579/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 30.9043 - val_loss: 32.7170\n",
      "Epoch 580/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.7596 - val_loss: 34.3853\n",
      "Epoch 581/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.9500 - val_loss: 32.6640\n",
      "Epoch 582/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.8070 - val_loss: 32.7433\n",
      "Epoch 583/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.4833 - val_loss: 32.8600\n",
      "Epoch 584/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.4142 - val_loss: 32.8778\n",
      "Epoch 585/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 30.3622 - val_loss: 32.3889\n",
      "Epoch 586/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 30.2177 - val_loss: 32.7346\n",
      "Epoch 587/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.5521 - val_loss: 32.6692\n",
      "Epoch 588/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.6165 - val_loss: 31.8573\n",
      "Epoch 589/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.4217 - val_loss: 32.4162\n",
      "Epoch 590/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.6666 - val_loss: 32.4521\n",
      "Epoch 591/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.4335 - val_loss: 32.7075\n",
      "Epoch 592/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.5101 - val_loss: 32.1064\n",
      "Epoch 593/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.3490 - val_loss: 31.8122\n",
      "Epoch 594/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 30.4313 - val_loss: 33.4503\n",
      "Epoch 595/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 30.5735 - val_loss: 32.2886\n",
      "Epoch 596/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.4489 - val_loss: 32.9155\n",
      "Epoch 597/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.5122 - val_loss: 32.1986\n",
      "Epoch 598/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.5857 - val_loss: 33.3186\n",
      "Epoch 599/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.4907 - val_loss: 33.4485\n",
      "Epoch 600/2000\n",
      "6/6 [==============================] - ETA: 0s - loss: 30.3813\n",
      "Epoch 00600: saving model to saved_models/latent128/cp-0600.h5\n",
      "6/6 [==============================] - 1s 150ms/step - loss: 30.3813 - val_loss: 32.4683\n",
      "Epoch 601/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.3838 - val_loss: 32.0219\n",
      "Epoch 602/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.5453 - val_loss: 32.4995\n",
      "Epoch 603/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.6039 - val_loss: 32.5354\n",
      "Epoch 604/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.4159 - val_loss: 31.9143\n",
      "Epoch 605/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.2402 - val_loss: 32.1820\n",
      "Epoch 606/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 30.1479 - val_loss: 33.6644\n",
      "Epoch 607/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.5186 - val_loss: 32.3210\n",
      "Epoch 608/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.2729 - val_loss: 31.9763\n",
      "Epoch 609/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 30.0110 - val_loss: 32.6496\n",
      "Epoch 610/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.0991 - val_loss: 32.3559\n",
      "Epoch 611/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 29.8704 - val_loss: 31.6896\n",
      "Epoch 612/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.9676 - val_loss: 32.3585\n",
      "Epoch 613/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.0323 - val_loss: 32.5705\n",
      "Epoch 614/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.2878 - val_loss: 33.2808\n",
      "Epoch 615/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.3426 - val_loss: 32.4860\n",
      "Epoch 616/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 30.2319 - val_loss: 32.3287\n",
      "Epoch 617/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.0015 - val_loss: 32.7888\n",
      "Epoch 618/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.0713 - val_loss: 34.3909\n",
      "Epoch 619/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 30.4294 - val_loss: 33.0703\n",
      "Epoch 620/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 30.6802 - val_loss: 32.5968\n",
      "Epoch 621/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.3647 - val_loss: 32.3641\n",
      "Epoch 622/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 30.7426 - val_loss: 32.2538\n",
      "Epoch 623/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.2414 - val_loss: 31.5392\n",
      "Epoch 624/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 30.1854 - val_loss: 32.0755\n",
      "Epoch 625/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.0473 - val_loss: 32.3691\n",
      "Epoch 626/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.0910 - val_loss: 31.1870\n",
      "Epoch 627/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.2272 - val_loss: 32.9625\n",
      "Epoch 628/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.2900 - val_loss: 32.3475\n",
      "Epoch 629/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 29.9157 - val_loss: 32.4607\n",
      "Epoch 630/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.8809 - val_loss: 31.3048\n",
      "Epoch 631/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.0616 - val_loss: 30.7899\n",
      "Epoch 632/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 29.5373 - val_loss: 30.7769\n",
      "Epoch 633/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.9184 - val_loss: 32.8316\n",
      "Epoch 634/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.7616 - val_loss: 32.4096\n",
      "Epoch 635/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.2973 - val_loss: 32.5125\n",
      "Epoch 636/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.8025 - val_loss: 32.1460\n",
      "Epoch 637/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 29.9487 - val_loss: 31.7595\n",
      "Epoch 638/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.8838 - val_loss: 31.5264\n",
      "Epoch 639/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.7341 - val_loss: 31.2202\n",
      "Epoch 640/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.8350 - val_loss: 31.1548\n",
      "Epoch 641/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 29.5049 - val_loss: 31.6444\n",
      "Epoch 642/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.8072 - val_loss: 33.4351\n",
      "Epoch 643/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.6003 - val_loss: 31.5946\n",
      "Epoch 644/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.6892 - val_loss: 31.8540\n",
      "Epoch 645/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.5602 - val_loss: 30.9892\n",
      "Epoch 646/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.5406 - val_loss: 31.6287\n",
      "Epoch 647/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.7124 - val_loss: 31.5892\n",
      "Epoch 648/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.5643 - val_loss: 31.5987\n",
      "Epoch 649/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.6271 - val_loss: 32.1664\n",
      "Epoch 650/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.7824 - val_loss: 31.3932\n",
      "Epoch 651/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.7506 - val_loss: 32.6149\n",
      "Epoch 652/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.6331 - val_loss: 31.4550\n",
      "Epoch 653/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.5608 - val_loss: 30.6053\n",
      "Epoch 654/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 29.3323 - val_loss: 32.1454\n",
      "Epoch 655/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.5180 - val_loss: 32.0663\n",
      "Epoch 656/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.5937 - val_loss: 31.7008\n",
      "Epoch 657/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 29.6093 - val_loss: 31.7687\n",
      "Epoch 658/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 29.3204 - val_loss: 31.4807\n",
      "Epoch 659/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 29.1808 - val_loss: 30.9671\n",
      "Epoch 660/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.2707 - val_loss: 31.8908\n",
      "Epoch 661/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.6782 - val_loss: 31.9820\n",
      "Epoch 662/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.9459 - val_loss: 32.1799\n",
      "Epoch 663/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 30.2946 - val_loss: 32.4562\n",
      "Epoch 664/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.8348 - val_loss: 32.4178\n",
      "Epoch 665/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.6128 - val_loss: 31.7548\n",
      "Epoch 666/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.3758 - val_loss: 31.7797\n",
      "Epoch 667/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 29.6340 - val_loss: 31.1838\n",
      "Epoch 668/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.3814 - val_loss: 31.7023\n",
      "Epoch 669/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.3691 - val_loss: 31.0362\n",
      "Epoch 670/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.2526 - val_loss: 31.5743\n",
      "Epoch 671/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 29.0481 - val_loss: 31.4711\n",
      "Epoch 672/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.2932 - val_loss: 31.5221\n",
      "Epoch 673/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.2608 - val_loss: 31.7880\n",
      "Epoch 674/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.2603 - val_loss: 31.4466\n",
      "Epoch 675/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.2832 - val_loss: 31.4494\n",
      "Epoch 676/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.3565 - val_loss: 30.6751\n",
      "Epoch 677/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.3535 - val_loss: 31.0314\n",
      "Epoch 678/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.3932 - val_loss: 31.5364\n",
      "Epoch 679/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.1475 - val_loss: 31.0187\n",
      "Epoch 680/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.5824 - val_loss: 30.4276\n",
      "Epoch 681/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.1722 - val_loss: 29.9904\n",
      "Epoch 682/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.9891 - val_loss: 30.9058\n",
      "Epoch 683/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.2019 - val_loss: 31.2848\n",
      "Epoch 684/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.3171 - val_loss: 31.5639\n",
      "Epoch 685/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.2333 - val_loss: 32.0697\n",
      "Epoch 686/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.2339 - val_loss: 31.3108\n",
      "Epoch 687/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.5980 - val_loss: 32.1464\n",
      "Epoch 688/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.4793 - val_loss: 32.1395\n",
      "Epoch 689/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.4605 - val_loss: 32.6440\n",
      "Epoch 690/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.2875 - val_loss: 30.8524\n",
      "Epoch 691/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.1125 - val_loss: 31.3776\n",
      "Epoch 692/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.1198 - val_loss: 31.1842\n",
      "Epoch 693/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.0943 - val_loss: 31.6950\n",
      "Epoch 694/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.2386 - val_loss: 31.8842\n",
      "Epoch 695/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.7187 - val_loss: 31.6395\n",
      "Epoch 696/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 29.2187 - val_loss: 30.7147\n",
      "Epoch 697/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.2962 - val_loss: 30.7074\n",
      "Epoch 698/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.2315 - val_loss: 31.8566\n",
      "Epoch 699/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.3600 - val_loss: 31.3891\n",
      "Epoch 700/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.3273 - val_loss: 30.9482\n",
      "Epoch 701/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.0054 - val_loss: 30.9938\n",
      "Epoch 702/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.2971 - val_loss: 31.0295\n",
      "Epoch 703/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.2158 - val_loss: 31.5743\n",
      "Epoch 704/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.0191 - val_loss: 31.7129\n",
      "Epoch 705/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.9193 - val_loss: 31.1035\n",
      "Epoch 706/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.2803 - val_loss: 31.6890\n",
      "Epoch 707/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.1851 - val_loss: 31.2708\n",
      "Epoch 708/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.3357 - val_loss: 30.3892\n",
      "Epoch 709/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.8969 - val_loss: 30.8750\n",
      "Epoch 710/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.9983 - val_loss: 31.1256\n",
      "Epoch 711/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.1343 - val_loss: 31.4860\n",
      "Epoch 712/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.1084 - val_loss: 30.5023\n",
      "Epoch 713/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.6927 - val_loss: 31.2075\n",
      "Epoch 714/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.0302 - val_loss: 31.3882\n",
      "Epoch 715/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.0411 - val_loss: 31.5833\n",
      "Epoch 716/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.9157 - val_loss: 31.0484\n",
      "Epoch 717/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.9338 - val_loss: 30.9752\n",
      "Epoch 718/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.0397 - val_loss: 31.2720\n",
      "Epoch 719/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.8948 - val_loss: 30.8780\n",
      "Epoch 720/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.0855 - val_loss: 32.6146\n",
      "Epoch 721/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.3458 - val_loss: 31.9459\n",
      "Epoch 722/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.2023 - val_loss: 31.6420\n",
      "Epoch 723/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.9418 - val_loss: 31.6226\n",
      "Epoch 724/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.0257 - val_loss: 31.0044\n",
      "Epoch 725/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.1794 - val_loss: 31.2561\n",
      "Epoch 726/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.7964 - val_loss: 30.6524\n",
      "Epoch 727/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.7842 - val_loss: 30.9623\n",
      "Epoch 728/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.9231 - val_loss: 30.9061\n",
      "Epoch 729/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.8487 - val_loss: 31.7823\n",
      "Epoch 730/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 29.0648 - val_loss: 31.4405\n",
      "Epoch 731/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.9626 - val_loss: 30.7675\n",
      "Epoch 732/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.0288 - val_loss: 31.5809\n",
      "Epoch 733/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.1607 - val_loss: 31.1482\n",
      "Epoch 734/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.1848 - val_loss: 31.4392\n",
      "Epoch 735/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.9285 - val_loss: 30.8486\n",
      "Epoch 736/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.0618 - val_loss: 30.8253\n",
      "Epoch 737/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.0254 - val_loss: 30.4985\n",
      "Epoch 738/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.1530 - val_loss: 31.3505\n",
      "Epoch 739/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.0208 - val_loss: 31.2901\n",
      "Epoch 740/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.7178 - val_loss: 30.2713\n",
      "Epoch 741/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.6581 - val_loss: 30.5579\n",
      "Epoch 742/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.5242 - val_loss: 31.4747\n",
      "Epoch 743/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.9312 - val_loss: 30.5071\n",
      "Epoch 744/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.7637 - val_loss: 30.6416\n",
      "Epoch 745/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.7413 - val_loss: 31.3023\n",
      "Epoch 746/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.8211 - val_loss: 30.7664\n",
      "Epoch 747/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.8985 - val_loss: 31.3892\n",
      "Epoch 748/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.6536 - val_loss: 31.0694\n",
      "Epoch 749/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.8397 - val_loss: 31.2417\n",
      "Epoch 750/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.9853 - val_loss: 30.3069\n",
      "Epoch 751/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.6967 - val_loss: 30.9818\n",
      "Epoch 752/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.9721 - val_loss: 31.0885\n",
      "Epoch 753/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.0830 - val_loss: 30.9127\n",
      "Epoch 754/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.8039 - val_loss: 32.2460\n",
      "Epoch 755/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.0266 - val_loss: 31.8996\n",
      "Epoch 756/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.9686 - val_loss: 30.8991\n",
      "Epoch 757/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.8568 - val_loss: 31.4279\n",
      "Epoch 758/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 29.0606 - val_loss: 30.8731\n",
      "Epoch 759/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.6442 - val_loss: 32.0262\n",
      "Epoch 760/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.9411 - val_loss: 31.2171\n",
      "Epoch 761/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.7338 - val_loss: 31.3021\n",
      "Epoch 762/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.8177 - val_loss: 30.6190\n",
      "Epoch 763/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.7260 - val_loss: 31.3070\n",
      "Epoch 764/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.7163 - val_loss: 30.5477\n",
      "Epoch 765/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.7441 - val_loss: 30.9364\n",
      "Epoch 766/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.7678 - val_loss: 30.1866\n",
      "Epoch 767/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 28.4928 - val_loss: 29.8880\n",
      "Epoch 768/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.3737 - val_loss: 30.8589\n",
      "Epoch 769/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.5318 - val_loss: 29.8287\n",
      "Epoch 770/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.7387 - val_loss: 31.1094\n",
      "Epoch 771/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.7993 - val_loss: 30.3261\n",
      "Epoch 772/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.2611 - val_loss: 30.9212\n",
      "Epoch 773/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.5843 - val_loss: 29.8059\n",
      "Epoch 774/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.5497 - val_loss: 30.6051\n",
      "Epoch 775/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.1448 - val_loss: 30.2702\n",
      "Epoch 776/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.4810 - val_loss: 30.5270\n",
      "Epoch 777/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.4449 - val_loss: 31.1406\n",
      "Epoch 778/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.7357 - val_loss: 31.1019\n",
      "Epoch 779/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.5958 - val_loss: 30.4803\n",
      "Epoch 780/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.4595 - val_loss: 30.3312\n",
      "Epoch 781/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.4604 - val_loss: 31.7701\n",
      "Epoch 782/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.4703 - val_loss: 30.5229\n",
      "Epoch 783/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.4038 - val_loss: 30.6889\n",
      "Epoch 784/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.5086 - val_loss: 30.5347\n",
      "Epoch 785/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.5804 - val_loss: 31.4516\n",
      "Epoch 786/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.6550 - val_loss: 30.4831\n",
      "Epoch 787/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.6386 - val_loss: 30.5988\n",
      "Epoch 788/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.7735 - val_loss: 32.2950\n",
      "Epoch 789/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.7346 - val_loss: 30.7690\n",
      "Epoch 790/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.9025 - val_loss: 30.6459\n",
      "Epoch 791/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.6675 - val_loss: 31.2380\n",
      "Epoch 792/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.5967 - val_loss: 32.2905\n",
      "Epoch 793/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.7658 - val_loss: 31.4033\n",
      "Epoch 794/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.4988 - val_loss: 29.9927\n",
      "Epoch 795/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.3485 - val_loss: 31.1710\n",
      "Epoch 796/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.1606 - val_loss: 29.9753\n",
      "Epoch 797/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.2541 - val_loss: 31.1467\n",
      "Epoch 798/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.2438 - val_loss: 31.3059\n",
      "Epoch 799/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.1577 - val_loss: 29.7016\n",
      "Epoch 800/2000\n",
      "6/6 [==============================] - ETA: 0s - loss: 28.3454\n",
      "Epoch 00800: saving model to saved_models/latent128/cp-0800.h5\n",
      "6/6 [==============================] - 1s 155ms/step - loss: 28.3454 - val_loss: 31.0103\n",
      "Epoch 801/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.4265 - val_loss: 30.0272\n",
      "Epoch 802/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.4608 - val_loss: 30.2445\n",
      "Epoch 803/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.4267 - val_loss: 30.6365\n",
      "Epoch 804/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.3275 - val_loss: 30.4772\n",
      "Epoch 805/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.6093 - val_loss: 30.6947\n",
      "Epoch 806/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.6304 - val_loss: 31.0899\n",
      "Epoch 807/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.4413 - val_loss: 31.1045\n",
      "Epoch 808/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.2990 - val_loss: 30.5602\n",
      "Epoch 809/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.5250 - val_loss: 31.2544\n",
      "Epoch 810/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.6912 - val_loss: 30.3536\n",
      "Epoch 811/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.8720 - val_loss: 31.0175\n",
      "Epoch 812/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.5954 - val_loss: 31.5979\n",
      "Epoch 813/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.7193 - val_loss: 31.0998\n",
      "Epoch 814/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.6588 - val_loss: 30.6202\n",
      "Epoch 815/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.4466 - val_loss: 30.3202\n",
      "Epoch 816/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.1910 - val_loss: 30.2327\n",
      "Epoch 817/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.2304 - val_loss: 29.6718\n",
      "Epoch 818/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.3630 - val_loss: 31.2861\n",
      "Epoch 819/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.2780 - val_loss: 30.1930\n",
      "Epoch 820/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.1683 - val_loss: 29.5723\n",
      "Epoch 821/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 27.9697 - val_loss: 30.3621\n",
      "Epoch 822/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.1442 - val_loss: 30.1263\n",
      "Epoch 823/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.9809 - val_loss: 30.2483\n",
      "Epoch 824/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.1320 - val_loss: 30.4324\n",
      "Epoch 825/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.1167 - val_loss: 30.0207\n",
      "Epoch 826/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.9392 - val_loss: 30.4216\n",
      "Epoch 827/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.8704 - val_loss: 29.0281\n",
      "Epoch 828/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.8672 - val_loss: 30.8118\n",
      "Epoch 829/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.0525 - val_loss: 30.0112\n",
      "Epoch 830/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.2187 - val_loss: 30.6108\n",
      "Epoch 831/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.1939 - val_loss: 30.7125\n",
      "Epoch 832/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.2293 - val_loss: 29.9229\n",
      "Epoch 833/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.9547 - val_loss: 29.9262\n",
      "Epoch 834/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.0287 - val_loss: 30.1944\n",
      "Epoch 835/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.2284 - val_loss: 30.1221\n",
      "Epoch 836/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.8965 - val_loss: 30.1037\n",
      "Epoch 837/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 27.8595 - val_loss: 30.4229\n",
      "Epoch 838/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.1988 - val_loss: 30.1606\n",
      "Epoch 839/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.1384 - val_loss: 30.9224\n",
      "Epoch 840/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.9997 - val_loss: 30.9119\n",
      "Epoch 841/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.1896 - val_loss: 30.2848\n",
      "Epoch 842/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.0601 - val_loss: 29.5014\n",
      "Epoch 843/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.9285 - val_loss: 30.0389\n",
      "Epoch 844/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.9333 - val_loss: 31.8837\n",
      "Epoch 845/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.2004 - val_loss: 30.5236\n",
      "Epoch 846/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.1754 - val_loss: 30.0795\n",
      "Epoch 847/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.9390 - val_loss: 30.0153\n",
      "Epoch 848/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.9354 - val_loss: 30.1752\n",
      "Epoch 849/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.9574 - val_loss: 30.0279\n",
      "Epoch 850/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.0899 - val_loss: 30.1875\n",
      "Epoch 851/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.1347 - val_loss: 30.6492\n",
      "Epoch 852/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.1784 - val_loss: 30.4837\n",
      "Epoch 853/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.6998 - val_loss: 30.6456\n",
      "Epoch 854/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.3545 - val_loss: 29.5471\n",
      "Epoch 855/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.0946 - val_loss: 31.6425\n",
      "Epoch 856/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.0786 - val_loss: 30.0501\n",
      "Epoch 857/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.9867 - val_loss: 30.7032\n",
      "Epoch 858/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.0878 - val_loss: 29.6972\n",
      "Epoch 859/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 27.7413 - val_loss: 30.4315\n",
      "Epoch 860/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.6856 - val_loss: 30.7306\n",
      "Epoch 861/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.9747 - val_loss: 29.7394\n",
      "Epoch 862/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.7917 - val_loss: 29.6285\n",
      "Epoch 863/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.7470 - val_loss: 30.0292\n",
      "Epoch 864/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.9096 - val_loss: 30.8112\n",
      "Epoch 865/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.0345 - val_loss: 30.3849\n",
      "Epoch 866/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.0987 - val_loss: 29.3929\n",
      "Epoch 867/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.1596 - val_loss: 30.6830\n",
      "Epoch 868/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.9058 - val_loss: 30.3706\n",
      "Epoch 869/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.7233 - val_loss: 30.5412\n",
      "Epoch 870/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.1281 - val_loss: 30.3089\n",
      "Epoch 871/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.1119 - val_loss: 29.6667\n",
      "Epoch 872/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.8893 - val_loss: 29.7777\n",
      "Epoch 873/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.2455 - val_loss: 30.7985\n",
      "Epoch 874/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.0677 - val_loss: 30.4118\n",
      "Epoch 875/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.9203 - val_loss: 29.5655\n",
      "Epoch 876/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.7417 - val_loss: 30.3117\n",
      "Epoch 877/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.7454 - val_loss: 30.0624\n",
      "Epoch 878/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.6969 - val_loss: 30.2447\n",
      "Epoch 879/2000\n",
      "6/6 [==============================] - 1s 105ms/step - loss: 27.6415 - val_loss: 29.8244\n",
      "Epoch 880/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.8483 - val_loss: 30.0529\n",
      "Epoch 881/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.7001 - val_loss: 29.9862\n",
      "Epoch 882/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.8897 - val_loss: 29.4414\n",
      "Epoch 883/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.6770 - val_loss: 30.2776\n",
      "Epoch 884/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.6948 - val_loss: 30.0011\n",
      "Epoch 885/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.5856 - val_loss: 29.8399\n",
      "Epoch 886/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.6961 - val_loss: 29.4597\n",
      "Epoch 887/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.7996 - val_loss: 30.2930\n",
      "Epoch 888/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.4898 - val_loss: 29.9259\n",
      "Epoch 889/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.9835 - val_loss: 29.4322\n",
      "Epoch 890/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.2172 - val_loss: 31.5355\n",
      "Epoch 891/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.5429 - val_loss: 30.7119\n",
      "Epoch 892/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.8225 - val_loss: 30.1406\n",
      "Epoch 893/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.8132 - val_loss: 30.4779\n",
      "Epoch 894/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.8442 - val_loss: 29.3862\n",
      "Epoch 895/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.6760 - val_loss: 29.6482\n",
      "Epoch 896/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.5897 - val_loss: 30.0241\n",
      "Epoch 897/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.6950 - val_loss: 30.1089\n",
      "Epoch 898/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.7786 - val_loss: 29.5469\n",
      "Epoch 899/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.9885 - val_loss: 30.9759\n",
      "Epoch 900/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.0362 - val_loss: 30.8360\n",
      "Epoch 901/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.3645 - val_loss: 31.1616\n",
      "Epoch 902/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 28.3999 - val_loss: 29.9390\n",
      "Epoch 903/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.8341 - val_loss: 30.3271\n",
      "Epoch 904/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.9068 - val_loss: 29.3326\n",
      "Epoch 905/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.6069 - val_loss: 29.9239\n",
      "Epoch 906/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.8409 - val_loss: 29.7515\n",
      "Epoch 907/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.8258 - val_loss: 29.9608\n",
      "Epoch 908/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.8429 - val_loss: 30.3890\n",
      "Epoch 909/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.7649 - val_loss: 30.3255\n",
      "Epoch 910/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.7190 - val_loss: 30.3095\n",
      "Epoch 911/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.7014 - val_loss: 30.0895\n",
      "Epoch 912/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.5737 - val_loss: 29.8084\n",
      "Epoch 913/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.2037 - val_loss: 29.7897\n",
      "Epoch 914/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.6556 - val_loss: 29.9541\n",
      "Epoch 915/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.7778 - val_loss: 30.2445\n",
      "Epoch 916/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.0613 - val_loss: 29.5590\n",
      "Epoch 917/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.9362 - val_loss: 29.5232\n",
      "Epoch 918/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.7245 - val_loss: 30.0008\n",
      "Epoch 919/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.6862 - val_loss: 29.9335\n",
      "Epoch 920/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.8063 - val_loss: 30.8107\n",
      "Epoch 921/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.6154 - val_loss: 29.1381\n",
      "Epoch 922/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.5052 - val_loss: 30.2189\n",
      "Epoch 923/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.4996 - val_loss: 30.1040\n",
      "Epoch 924/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.5385 - val_loss: 30.2211\n",
      "Epoch 925/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.5488 - val_loss: 29.0650\n",
      "Epoch 926/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.6376 - val_loss: 29.6991\n",
      "Epoch 927/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.6630 - val_loss: 29.2808\n",
      "Epoch 928/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.4622 - val_loss: 30.0830\n",
      "Epoch 929/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.6095 - val_loss: 30.4509\n",
      "Epoch 930/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.6317 - val_loss: 30.1864\n",
      "Epoch 931/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.9637 - val_loss: 29.5417\n",
      "Epoch 932/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.7792 - val_loss: 30.5141\n",
      "Epoch 933/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.8734 - val_loss: 29.9323\n",
      "Epoch 934/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.0664 - val_loss: 29.4214\n",
      "Epoch 935/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.9713 - val_loss: 29.9926\n",
      "Epoch 936/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.0779 - val_loss: 29.8943\n",
      "Epoch 937/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.8074 - val_loss: 29.2450\n",
      "Epoch 938/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.6260 - val_loss: 30.2575\n",
      "Epoch 939/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.3857 - val_loss: 29.7711\n",
      "Epoch 940/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.6504 - val_loss: 29.7997\n",
      "Epoch 941/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.4461 - val_loss: 29.5107\n",
      "Epoch 942/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.6673 - val_loss: 29.8498\n",
      "Epoch 943/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.4981 - val_loss: 29.8499\n",
      "Epoch 944/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.5007 - val_loss: 29.2913\n",
      "Epoch 945/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.5941 - val_loss: 29.5053\n",
      "Epoch 946/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.4980 - val_loss: 30.1961\n",
      "Epoch 947/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.6545 - val_loss: 29.6919\n",
      "Epoch 948/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.7270 - val_loss: 30.4498\n",
      "Epoch 949/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.0981 - val_loss: 30.1314\n",
      "Epoch 950/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.3289 - val_loss: 30.4639\n",
      "Epoch 951/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 28.1480 - val_loss: 29.0443\n",
      "Epoch 952/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.6217 - val_loss: 29.4954\n",
      "Epoch 953/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.4815 - val_loss: 29.4623\n",
      "Epoch 954/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.3479 - val_loss: 29.5377\n",
      "Epoch 955/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.4565 - val_loss: 29.9174\n",
      "Epoch 956/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.6912 - val_loss: 30.1834\n",
      "Epoch 957/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.7714 - val_loss: 29.2960\n",
      "Epoch 958/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.5867 - val_loss: 29.1904\n",
      "Epoch 959/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.4983 - val_loss: 29.3995\n",
      "Epoch 960/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.2971 - val_loss: 29.2075\n",
      "Epoch 961/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 27.0312 - val_loss: 29.1167\n",
      "Epoch 962/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.3610 - val_loss: 29.2437\n",
      "Epoch 963/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.2060 - val_loss: 29.5570\n",
      "Epoch 964/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.4177 - val_loss: 28.3352\n",
      "Epoch 965/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.2773 - val_loss: 29.8221\n",
      "Epoch 966/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.3942 - val_loss: 29.2344\n",
      "Epoch 967/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.2213 - val_loss: 30.1193\n",
      "Epoch 968/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.4468 - val_loss: 29.0766\n",
      "Epoch 969/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.2657 - val_loss: 29.7546\n",
      "Epoch 970/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.2291 - val_loss: 29.3820\n",
      "Epoch 971/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.2767 - val_loss: 29.3001\n",
      "Epoch 972/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.1672 - val_loss: 30.1132\n",
      "Epoch 973/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.4924 - val_loss: 29.1226\n",
      "Epoch 974/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.1700 - val_loss: 29.8951\n",
      "Epoch 975/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.3808 - val_loss: 29.3064\n",
      "Epoch 976/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.3369 - val_loss: 29.3615\n",
      "Epoch 977/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.4808 - val_loss: 29.2495\n",
      "Epoch 978/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.1511 - val_loss: 29.3473\n",
      "Epoch 979/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.3168 - val_loss: 29.6143\n",
      "Epoch 980/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.4052 - val_loss: 28.9701\n",
      "Epoch 981/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.0737 - val_loss: 29.6929\n",
      "Epoch 982/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 26.9699 - val_loss: 29.5411\n",
      "Epoch 983/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.3033 - val_loss: 30.5775\n",
      "Epoch 984/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.2409 - val_loss: 29.3131\n",
      "Epoch 985/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.1357 - val_loss: 29.7841\n",
      "Epoch 986/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.0679 - val_loss: 30.3394\n",
      "Epoch 987/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.5625 - val_loss: 29.1346\n",
      "Epoch 988/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.2696 - val_loss: 29.8082\n",
      "Epoch 989/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.3819 - val_loss: 29.3759\n",
      "Epoch 990/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.3628 - val_loss: 28.6410\n",
      "Epoch 991/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.0235 - val_loss: 29.2052\n",
      "Epoch 992/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.5150 - val_loss: 30.1867\n",
      "Epoch 993/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.6659 - val_loss: 29.4621\n",
      "Epoch 994/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.7061 - val_loss: 29.3664\n",
      "Epoch 995/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.2690 - val_loss: 29.4541\n",
      "Epoch 996/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.1902 - val_loss: 28.9253\n",
      "Epoch 997/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.0331 - val_loss: 28.8356\n",
      "Epoch 998/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.0956 - val_loss: 29.5634\n",
      "Epoch 999/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.0467 - val_loss: 30.6856\n",
      "Epoch 1000/2000\n",
      "6/6 [==============================] - ETA: 0s - loss: 27.2681\n",
      "Epoch 01000: saving model to saved_models/latent128/cp-1000.h5\n",
      "6/6 [==============================] - 1s 166ms/step - loss: 27.2681 - val_loss: 29.0483\n",
      "Epoch 1001/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 26.9570 - val_loss: 28.8654\n",
      "Epoch 1002/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.2354 - val_loss: 29.3660\n",
      "Epoch 1003/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.2184 - val_loss: 29.1740\n",
      "Epoch 1004/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.4175 - val_loss: 29.0506\n",
      "Epoch 1005/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.4036 - val_loss: 29.1139\n",
      "Epoch 1006/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.2346 - val_loss: 29.4256\n",
      "Epoch 1007/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 27.1784 - val_loss: 29.8438\n",
      "Epoch 1008/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.8413 - val_loss: 29.3289\n",
      "Epoch 1009/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.0153 - val_loss: 29.5309\n",
      "Epoch 1010/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.8460 - val_loss: 29.1959\n",
      "Epoch 1011/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.0193 - val_loss: 29.0679\n",
      "Epoch 1012/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.1257 - val_loss: 29.3116\n",
      "Epoch 1013/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.0048 - val_loss: 28.2450\n",
      "Epoch 1014/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.9482 - val_loss: 29.0024\n",
      "Epoch 1015/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.9632 - val_loss: 29.1117\n",
      "Epoch 1016/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.8945 - val_loss: 29.2904\n",
      "Epoch 1017/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.3587 - val_loss: 28.2582\n",
      "Epoch 1018/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.0795 - val_loss: 29.6588\n",
      "Epoch 1019/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.0590 - val_loss: 28.9181\n",
      "Epoch 1020/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.2252 - val_loss: 29.1518\n",
      "Epoch 1021/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.0504 - val_loss: 29.3976\n",
      "Epoch 1022/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.1164 - val_loss: 29.0233\n",
      "Epoch 1023/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.0280 - val_loss: 29.4751\n",
      "Epoch 1024/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.0762 - val_loss: 29.2374\n",
      "Epoch 1025/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.1608 - val_loss: 28.8711\n",
      "Epoch 1026/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.8638 - val_loss: 30.2813\n",
      "Epoch 1027/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.9970 - val_loss: 30.1899\n",
      "Epoch 1028/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.6671 - val_loss: 29.6333\n",
      "Epoch 1029/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.2606 - val_loss: 29.1471\n",
      "Epoch 1030/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.2966 - val_loss: 29.4356\n",
      "Epoch 1031/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.1876 - val_loss: 30.0250\n",
      "Epoch 1032/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.2706 - val_loss: 29.6876\n",
      "Epoch 1033/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.3780 - val_loss: 29.4156\n",
      "Epoch 1034/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.2068 - val_loss: 28.9865\n",
      "Epoch 1035/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.0230 - val_loss: 28.8389\n",
      "Epoch 1036/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.1659 - val_loss: 29.4999\n",
      "Epoch 1037/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.2328 - val_loss: 29.1249\n",
      "Epoch 1038/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.2571 - val_loss: 30.1046\n",
      "Epoch 1039/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.1727 - val_loss: 30.2577\n",
      "Epoch 1040/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.2760 - val_loss: 29.8559\n",
      "Epoch 1041/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.0987 - val_loss: 29.1153\n",
      "Epoch 1042/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.2874 - val_loss: 29.3052\n",
      "Epoch 1043/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.1019 - val_loss: 29.4301\n",
      "Epoch 1044/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.9784 - val_loss: 29.2559\n",
      "Epoch 1045/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.9778 - val_loss: 29.3481\n",
      "Epoch 1046/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.8800 - val_loss: 29.0257\n",
      "Epoch 1047/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 26.6303 - val_loss: 29.0731\n",
      "Epoch 1048/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.9521 - val_loss: 29.1367\n",
      "Epoch 1049/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.9597 - val_loss: 29.1958\n",
      "Epoch 1050/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.0925 - val_loss: 28.9059\n",
      "Epoch 1051/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.0610 - val_loss: 29.5838\n",
      "Epoch 1052/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.0506 - val_loss: 29.4276\n",
      "Epoch 1053/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.1612 - val_loss: 29.2908\n",
      "Epoch 1054/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.9917 - val_loss: 28.7471\n",
      "Epoch 1055/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.9503 - val_loss: 29.9293\n",
      "Epoch 1056/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.9883 - val_loss: 29.2446\n",
      "Epoch 1057/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.9017 - val_loss: 29.1301\n",
      "Epoch 1058/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.7997 - val_loss: 29.4461\n",
      "Epoch 1059/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.0277 - val_loss: 29.3478\n",
      "Epoch 1060/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.0638 - val_loss: 29.1442\n",
      "Epoch 1061/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.1876 - val_loss: 28.8505\n",
      "Epoch 1062/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.1985 - val_loss: 29.8903\n",
      "Epoch 1063/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 27.2090 - val_loss: 29.1413\n",
      "Epoch 1064/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.0850 - val_loss: 30.7645\n",
      "Epoch 1065/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.3099 - val_loss: 29.5603\n",
      "Epoch 1066/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.3913 - val_loss: 30.3282\n",
      "Epoch 1067/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.5779 - val_loss: 29.7546\n",
      "Epoch 1068/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.3265 - val_loss: 30.0284\n",
      "Epoch 1069/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.5443 - val_loss: 30.6286\n",
      "Epoch 1070/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.3470 - val_loss: 29.4894\n",
      "Epoch 1071/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.5200 - val_loss: 29.0872\n",
      "Epoch 1072/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.9503 - val_loss: 30.0977\n",
      "Epoch 1073/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.9405 - val_loss: 29.1393\n",
      "Epoch 1074/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.7045 - val_loss: 29.1645\n",
      "Epoch 1075/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.9836 - val_loss: 29.3029\n",
      "Epoch 1076/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.8163 - val_loss: 29.5923\n",
      "Epoch 1077/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.0017 - val_loss: 29.9753\n",
      "Epoch 1078/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.8107 - val_loss: 28.8808\n",
      "Epoch 1079/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.9753 - val_loss: 29.3050\n",
      "Epoch 1080/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.0718 - val_loss: 29.4697\n",
      "Epoch 1081/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.8212 - val_loss: 29.4313\n",
      "Epoch 1082/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.7653 - val_loss: 28.9651\n",
      "Epoch 1083/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.1855 - val_loss: 29.4111\n",
      "Epoch 1084/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.8863 - val_loss: 29.1038\n",
      "Epoch 1085/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.8521 - val_loss: 28.2398\n",
      "Epoch 1086/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.7768 - val_loss: 28.7891\n",
      "Epoch 1087/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.9542 - val_loss: 28.7816\n",
      "Epoch 1088/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.6407 - val_loss: 28.2718\n",
      "Epoch 1089/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 26.5349 - val_loss: 29.0901\n",
      "Epoch 1090/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.6174 - val_loss: 29.0507\n",
      "Epoch 1091/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.7494 - val_loss: 30.0059\n",
      "Epoch 1092/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.8322 - val_loss: 29.2725\n",
      "Epoch 1093/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.6744 - val_loss: 29.6774\n",
      "Epoch 1094/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.0699 - val_loss: 29.4707\n",
      "Epoch 1095/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.8555 - val_loss: 29.2967\n",
      "Epoch 1096/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.1072 - val_loss: 29.4841\n",
      "Epoch 1097/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.9427 - val_loss: 29.1140\n",
      "Epoch 1098/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.8217 - val_loss: 28.9490\n",
      "Epoch 1099/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.8270 - val_loss: 29.1530\n",
      "Epoch 1100/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.6811 - val_loss: 28.5448\n",
      "Epoch 1101/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.9598 - val_loss: 29.6455\n",
      "Epoch 1102/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.8331 - val_loss: 30.2943\n",
      "Epoch 1103/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.0932 - val_loss: 29.1687\n",
      "Epoch 1104/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.7437 - val_loss: 29.2484\n",
      "Epoch 1105/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.8155 - val_loss: 28.8957\n",
      "Epoch 1106/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.4337 - val_loss: 29.1614\n",
      "Epoch 1107/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.9224 - val_loss: 29.6153\n",
      "Epoch 1108/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.7777 - val_loss: 28.7504\n",
      "Epoch 1109/2000\n",
      "6/6 [==============================] - 1s 105ms/step - loss: 26.3998 - val_loss: 28.7947\n",
      "Epoch 1110/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.9473 - val_loss: 28.9123\n",
      "Epoch 1111/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.8795 - val_loss: 28.4730\n",
      "Epoch 1112/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.5609 - val_loss: 29.9133\n",
      "Epoch 1113/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.6943 - val_loss: 29.4184\n",
      "Epoch 1114/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.7357 - val_loss: 28.8174\n",
      "Epoch 1115/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.6459 - val_loss: 28.6257\n",
      "Epoch 1116/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.5316 - val_loss: 29.2478\n",
      "Epoch 1117/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.6102 - val_loss: 28.6803\n",
      "Epoch 1118/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.8586 - val_loss: 28.7037\n",
      "Epoch 1119/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.4868 - val_loss: 28.6189\n",
      "Epoch 1120/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.7984 - val_loss: 29.6231\n",
      "Epoch 1121/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.8507 - val_loss: 29.1752\n",
      "Epoch 1122/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.9887 - val_loss: 28.4763\n",
      "Epoch 1123/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.7011 - val_loss: 28.9879\n",
      "Epoch 1124/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.0020 - val_loss: 29.3868\n",
      "Epoch 1125/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.7607 - val_loss: 28.9997\n",
      "Epoch 1126/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.8303 - val_loss: 28.7435\n",
      "Epoch 1127/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.6924 - val_loss: 29.2143\n",
      "Epoch 1128/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.7262 - val_loss: 29.3138\n",
      "Epoch 1129/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.6205 - val_loss: 29.8201\n",
      "Epoch 1130/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.8590 - val_loss: 29.2622\n",
      "Epoch 1131/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.8385 - val_loss: 29.4130\n",
      "Epoch 1132/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.6033 - val_loss: 28.2622\n",
      "Epoch 1133/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.5891 - val_loss: 29.3755\n",
      "Epoch 1134/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.7360 - val_loss: 28.7344\n",
      "Epoch 1135/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.5906 - val_loss: 28.3805\n",
      "Epoch 1136/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.5034 - val_loss: 28.6227\n",
      "Epoch 1137/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.5175 - val_loss: 29.0211\n",
      "Epoch 1138/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.5913 - val_loss: 29.6611\n",
      "Epoch 1139/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.6296 - val_loss: 28.6734\n",
      "Epoch 1140/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.5427 - val_loss: 28.6572\n",
      "Epoch 1141/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.5726 - val_loss: 29.1817\n",
      "Epoch 1142/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.3580 - val_loss: 28.5169\n",
      "Epoch 1143/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.4685 - val_loss: 28.6404\n",
      "Epoch 1144/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.5537 - val_loss: 28.6206\n",
      "Epoch 1145/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.6484 - val_loss: 28.2451\n",
      "Epoch 1146/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.5503 - val_loss: 28.3805\n",
      "Epoch 1147/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.6022 - val_loss: 28.1890\n",
      "Epoch 1148/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.3229 - val_loss: 28.7691\n",
      "Epoch 1149/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.3862 - val_loss: 29.0316\n",
      "Epoch 1150/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.3352 - val_loss: 28.7972\n",
      "Epoch 1151/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.4165 - val_loss: 28.8843\n",
      "Epoch 1152/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.4213 - val_loss: 28.6670\n",
      "Epoch 1153/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.6148 - val_loss: 28.9343\n",
      "Epoch 1154/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.3436 - val_loss: 29.0460\n",
      "Epoch 1155/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.5602 - val_loss: 28.5182\n",
      "Epoch 1156/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.4357 - val_loss: 29.3032\n",
      "Epoch 1157/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.8596 - val_loss: 28.6641\n",
      "Epoch 1158/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.4575 - val_loss: 28.9180\n",
      "Epoch 1159/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.5315 - val_loss: 29.3131\n",
      "Epoch 1160/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.5890 - val_loss: 29.2011\n",
      "Epoch 1161/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.4181 - val_loss: 29.3114\n",
      "Epoch 1162/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.6168 - val_loss: 29.0587\n",
      "Epoch 1163/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.6192 - val_loss: 29.5095\n",
      "Epoch 1164/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.9348 - val_loss: 29.1408\n",
      "Epoch 1165/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.6071 - val_loss: 29.3337\n",
      "Epoch 1166/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.5926 - val_loss: 28.6754\n",
      "Epoch 1167/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.3241 - val_loss: 28.8789\n",
      "Epoch 1168/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.4566 - val_loss: 28.2657\n",
      "Epoch 1169/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.5983 - val_loss: 29.3014\n",
      "Epoch 1170/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.4693 - val_loss: 29.3206\n",
      "Epoch 1171/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.5055 - val_loss: 28.2831\n",
      "Epoch 1172/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.5848 - val_loss: 29.4915\n",
      "Epoch 1173/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.5077 - val_loss: 28.6067\n",
      "Epoch 1174/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.6206 - val_loss: 28.4213\n",
      "Epoch 1175/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.4947 - val_loss: 28.8995\n",
      "Epoch 1176/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.4434 - val_loss: 28.8596\n",
      "Epoch 1177/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.4820 - val_loss: 28.6282\n",
      "Epoch 1178/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.7562 - val_loss: 29.2067\n",
      "Epoch 1179/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.5127 - val_loss: 28.5710\n",
      "Epoch 1180/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.4578 - val_loss: 28.9028\n",
      "Epoch 1181/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.5312 - val_loss: 28.4301\n",
      "Epoch 1182/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.3731 - val_loss: 30.0061\n",
      "Epoch 1183/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.2964 - val_loss: 29.4783\n",
      "Epoch 1184/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.3823 - val_loss: 28.4070\n",
      "Epoch 1185/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.3705 - val_loss: 28.2228\n",
      "Epoch 1186/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.4884 - val_loss: 28.8478\n",
      "Epoch 1187/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.7191 - val_loss: 29.1870\n",
      "Epoch 1188/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.4304 - val_loss: 29.3673\n",
      "Epoch 1189/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.4802 - val_loss: 29.2312\n",
      "Epoch 1190/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.4477 - val_loss: 29.6971\n",
      "Epoch 1191/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.4708 - val_loss: 28.9624\n",
      "Epoch 1192/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.4699 - val_loss: 28.7381\n",
      "Epoch 1193/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.3820 - val_loss: 29.9150\n",
      "Epoch 1194/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.2774 - val_loss: 28.7656\n",
      "Epoch 1195/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.1174 - val_loss: 28.8532\n",
      "Epoch 1196/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.3509 - val_loss: 28.7147\n",
      "Epoch 1197/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.3820 - val_loss: 28.5717\n",
      "Epoch 1198/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.4577 - val_loss: 28.9438\n",
      "Epoch 1199/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.6421 - val_loss: 28.5897\n",
      "Epoch 1200/2000\n",
      "6/6 [==============================] - ETA: 0s - loss: 26.4810\n",
      "Epoch 01200: saving model to saved_models/latent128/cp-1200.h5\n",
      "6/6 [==============================] - 1s 169ms/step - loss: 26.4810 - val_loss: 28.6951\n",
      "Epoch 1201/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.3154 - val_loss: 28.6819\n",
      "Epoch 1202/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.2776 - val_loss: 28.8928\n",
      "Epoch 1203/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.1183 - val_loss: 28.4986\n",
      "Epoch 1204/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.2523 - val_loss: 27.9765\n",
      "Epoch 1205/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.3953 - val_loss: 28.2496\n",
      "Epoch 1206/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.2000 - val_loss: 27.9065\n",
      "Epoch 1207/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.3185 - val_loss: 28.2099\n",
      "Epoch 1208/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.3956 - val_loss: 27.9132\n",
      "Epoch 1209/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.4861 - val_loss: 28.5867\n",
      "Epoch 1210/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.4087 - val_loss: 28.5657\n",
      "Epoch 1211/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.3434 - val_loss: 29.0913\n",
      "Epoch 1212/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.3662 - val_loss: 28.2852\n",
      "Epoch 1213/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.2432 - val_loss: 28.6203\n",
      "Epoch 1214/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.1538 - val_loss: 28.7115\n",
      "Epoch 1215/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.9312 - val_loss: 28.2748\n",
      "Epoch 1216/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.2291 - val_loss: 28.7265\n",
      "Epoch 1217/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.4449 - val_loss: 29.3104\n",
      "Epoch 1218/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.7367 - val_loss: 29.0529\n",
      "Epoch 1219/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.5830 - val_loss: 29.5325\n",
      "Epoch 1220/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.1796 - val_loss: 28.6195\n",
      "Epoch 1221/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.5060 - val_loss: 28.8042\n",
      "Epoch 1222/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.3803 - val_loss: 29.7922\n",
      "Epoch 1223/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 27.0100 - val_loss: 28.1139\n",
      "Epoch 1224/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.6007 - val_loss: 28.9728\n",
      "Epoch 1225/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.7741 - val_loss: 29.7374\n",
      "Epoch 1226/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.9897 - val_loss: 28.3429\n",
      "Epoch 1227/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.6174 - val_loss: 28.6119\n",
      "Epoch 1228/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.4366 - val_loss: 28.6109\n",
      "Epoch 1229/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.3069 - val_loss: 28.3641\n",
      "Epoch 1230/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.4493 - val_loss: 28.3569\n",
      "Epoch 1231/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.1994 - val_loss: 28.3809\n",
      "Epoch 1232/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.2585 - val_loss: 28.3515\n",
      "Epoch 1233/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.0794 - val_loss: 28.6900\n",
      "Epoch 1234/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.1776 - val_loss: 28.4910\n",
      "Epoch 1235/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.2046 - val_loss: 28.3775\n",
      "Epoch 1236/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.2309 - val_loss: 28.1702\n",
      "Epoch 1237/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.0109 - val_loss: 28.1844\n",
      "Epoch 1238/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.8611 - val_loss: 27.8340\n",
      "Epoch 1239/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.0908 - val_loss: 28.4601\n",
      "Epoch 1240/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.2882 - val_loss: 28.6658\n",
      "Epoch 1241/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.3695 - val_loss: 28.2889\n",
      "Epoch 1242/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.1950 - val_loss: 28.5278\n",
      "Epoch 1243/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.2701 - val_loss: 28.5533\n",
      "Epoch 1244/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.0788 - val_loss: 28.0729\n",
      "Epoch 1245/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.0174 - val_loss: 28.0241\n",
      "Epoch 1246/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.2721 - val_loss: 27.7382\n",
      "Epoch 1247/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.8351 - val_loss: 27.9572\n",
      "Epoch 1248/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.9058 - val_loss: 27.8345\n",
      "Epoch 1249/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.0750 - val_loss: 28.8184\n",
      "Epoch 1250/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.1064 - val_loss: 28.6068\n",
      "Epoch 1251/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.3046 - val_loss: 28.7199\n",
      "Epoch 1252/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.5164 - val_loss: 28.2945\n",
      "Epoch 1253/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.1003 - val_loss: 27.9641\n",
      "Epoch 1254/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.9962 - val_loss: 28.5237\n",
      "Epoch 1255/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.1050 - val_loss: 28.6221\n",
      "Epoch 1256/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.1149 - val_loss: 28.4222\n",
      "Epoch 1257/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.0947 - val_loss: 28.5819\n",
      "Epoch 1258/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.1649 - val_loss: 28.5634\n",
      "Epoch 1259/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.0339 - val_loss: 28.3897\n",
      "Epoch 1260/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.2797 - val_loss: 28.6972\n",
      "Epoch 1261/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.3144 - val_loss: 28.3018\n",
      "Epoch 1262/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.1267 - val_loss: 28.3573\n",
      "Epoch 1263/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.0148 - val_loss: 28.5250\n",
      "Epoch 1264/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.1034 - val_loss: 28.3434\n",
      "Epoch 1265/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.0524 - val_loss: 28.6571\n",
      "Epoch 1266/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.0349 - val_loss: 28.0289\n",
      "Epoch 1267/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.0865 - val_loss: 28.6417\n",
      "Epoch 1268/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.1838 - val_loss: 27.8637\n",
      "Epoch 1269/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.0461 - val_loss: 27.4840\n",
      "Epoch 1270/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.8736 - val_loss: 28.4678\n",
      "Epoch 1271/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.9746 - val_loss: 28.8650\n",
      "Epoch 1272/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.0752 - val_loss: 27.8702\n",
      "Epoch 1273/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.1367 - val_loss: 28.5708\n",
      "Epoch 1274/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.0767 - val_loss: 28.6313\n",
      "Epoch 1275/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.9468 - val_loss: 27.5795\n",
      "Epoch 1276/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.7868 - val_loss: 28.0370\n",
      "Epoch 1277/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.0470 - val_loss: 28.1332\n",
      "Epoch 1278/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.2581 - val_loss: 27.6442\n",
      "Epoch 1279/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.0346 - val_loss: 28.0462\n",
      "Epoch 1280/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.3131 - val_loss: 28.5430\n",
      "Epoch 1281/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.1615 - val_loss: 28.3462\n",
      "Epoch 1282/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.4013 - val_loss: 27.7520\n",
      "Epoch 1283/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.1540 - val_loss: 28.0453\n",
      "Epoch 1284/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.1829 - val_loss: 28.5999\n",
      "Epoch 1285/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.0366 - val_loss: 28.5274\n",
      "Epoch 1286/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.2337 - val_loss: 28.7039\n",
      "Epoch 1287/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.2126 - val_loss: 28.4993\n",
      "Epoch 1288/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.4830 - val_loss: 29.3612\n",
      "Epoch 1289/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.4338 - val_loss: 28.9160\n",
      "Epoch 1290/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.0246 - val_loss: 28.2274\n",
      "Epoch 1291/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.8480 - val_loss: 28.1583\n",
      "Epoch 1292/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.9887 - val_loss: 28.3523\n",
      "Epoch 1293/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.8058 - val_loss: 28.4117\n",
      "Epoch 1294/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.9966 - val_loss: 28.8633\n",
      "Epoch 1295/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.0583 - val_loss: 27.9496\n",
      "Epoch 1296/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.8668 - val_loss: 28.6350\n",
      "Epoch 1297/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.7924 - val_loss: 28.0904\n",
      "Epoch 1298/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.0077 - val_loss: 28.5412\n",
      "Epoch 1299/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.3798 - val_loss: 28.3706\n",
      "Epoch 1300/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 26.2735 - val_loss: 28.3607\n",
      "Epoch 1301/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.0594 - val_loss: 27.8645\n",
      "Epoch 1302/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.9200 - val_loss: 28.5338\n",
      "Epoch 1303/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.0383 - val_loss: 28.7971\n",
      "Epoch 1304/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.8491 - val_loss: 28.7338\n",
      "Epoch 1305/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.8447 - val_loss: 27.4630\n",
      "Epoch 1306/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.7244 - val_loss: 28.2867\n",
      "Epoch 1307/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.9388 - val_loss: 27.8698\n",
      "Epoch 1308/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.0843 - val_loss: 28.5448\n",
      "Epoch 1309/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.0215 - val_loss: 27.4706\n",
      "Epoch 1310/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.8351 - val_loss: 27.7121\n",
      "Epoch 1311/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.7287 - val_loss: 27.8755\n",
      "Epoch 1312/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.7660 - val_loss: 27.9220\n",
      "Epoch 1313/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.6749 - val_loss: 28.1257\n",
      "Epoch 1314/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.7450 - val_loss: 28.0296\n",
      "Epoch 1315/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.8327 - val_loss: 28.6503\n",
      "Epoch 1316/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.9455 - val_loss: 28.4362\n",
      "Epoch 1317/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.9688 - val_loss: 28.4029\n",
      "Epoch 1318/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 25.8973 - val_loss: 27.6672\n",
      "Epoch 1319/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.8035 - val_loss: 29.3040\n",
      "Epoch 1320/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.0796 - val_loss: 28.4624\n",
      "Epoch 1321/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.8221 - val_loss: 28.0941\n",
      "Epoch 1322/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.9530 - val_loss: 28.3747\n",
      "Epoch 1323/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.2590 - val_loss: 28.2103\n",
      "Epoch 1324/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.9596 - val_loss: 28.4200\n",
      "Epoch 1325/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.7711 - val_loss: 28.3239\n",
      "Epoch 1326/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.9119 - val_loss: 27.7971\n",
      "Epoch 1327/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.0829 - val_loss: 29.1987\n",
      "Epoch 1328/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.9791 - val_loss: 27.8339\n",
      "Epoch 1329/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.9697 - val_loss: 28.1724\n",
      "Epoch 1330/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.9930 - val_loss: 27.9341\n",
      "Epoch 1331/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.0468 - val_loss: 28.1705\n",
      "Epoch 1332/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.8702 - val_loss: 28.4865\n",
      "Epoch 1333/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.8052 - val_loss: 28.4850\n",
      "Epoch 1334/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.6989 - val_loss: 27.8606\n",
      "Epoch 1335/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.8745 - val_loss: 28.4560\n",
      "Epoch 1336/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.7327 - val_loss: 27.8760\n",
      "Epoch 1337/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 25.5632 - val_loss: 27.5345\n",
      "Epoch 1338/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.9525 - val_loss: 28.9398\n",
      "Epoch 1339/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.1827 - val_loss: 28.9189\n",
      "Epoch 1340/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.9888 - val_loss: 27.6789\n",
      "Epoch 1341/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.9433 - val_loss: 28.9609\n",
      "Epoch 1342/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.7987 - val_loss: 28.7011\n",
      "Epoch 1343/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.2552 - val_loss: 28.5309\n",
      "Epoch 1344/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.1016 - val_loss: 28.7405\n",
      "Epoch 1345/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.3850 - val_loss: 29.4373\n",
      "Epoch 1346/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.5454 - val_loss: 28.0800\n",
      "Epoch 1347/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.6703 - val_loss: 29.1756\n",
      "Epoch 1348/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.7429 - val_loss: 29.3862\n",
      "Epoch 1349/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.3490 - val_loss: 28.6257\n",
      "Epoch 1350/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.3811 - val_loss: 28.6232\n",
      "Epoch 1351/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.0604 - val_loss: 28.0672\n",
      "Epoch 1352/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.8410 - val_loss: 28.8274\n",
      "Epoch 1353/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.6747 - val_loss: 27.8438\n",
      "Epoch 1354/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.5171 - val_loss: 27.9815\n",
      "Epoch 1355/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.9274 - val_loss: 28.0412\n",
      "Epoch 1356/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.7079 - val_loss: 28.1600\n",
      "Epoch 1357/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.7109 - val_loss: 28.4602\n",
      "Epoch 1358/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.8444 - val_loss: 27.8792\n",
      "Epoch 1359/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.9596 - val_loss: 28.5298\n",
      "Epoch 1360/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.0827 - val_loss: 27.2897\n",
      "Epoch 1361/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.9295 - val_loss: 27.6817\n",
      "Epoch 1362/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.6801 - val_loss: 27.9513\n",
      "Epoch 1363/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.7133 - val_loss: 27.8897\n",
      "Epoch 1364/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.5375 - val_loss: 28.0251\n",
      "Epoch 1365/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.6621 - val_loss: 27.9024\n",
      "Epoch 1366/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.7903 - val_loss: 28.1890\n",
      "Epoch 1367/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.7158 - val_loss: 27.5560\n",
      "Epoch 1368/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.5426 - val_loss: 27.7995\n",
      "Epoch 1369/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 25.4573 - val_loss: 27.4750\n",
      "Epoch 1370/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.6790 - val_loss: 28.1797\n",
      "Epoch 1371/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.0040 - val_loss: 28.6498\n",
      "Epoch 1372/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.9576 - val_loss: 28.7486\n",
      "Epoch 1373/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.0066 - val_loss: 28.5539\n",
      "Epoch 1374/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.9481 - val_loss: 28.6272\n",
      "Epoch 1375/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.0191 - val_loss: 28.6346\n",
      "Epoch 1376/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.3072 - val_loss: 28.1458\n",
      "Epoch 1377/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.9873 - val_loss: 28.3961\n",
      "Epoch 1378/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.0393 - val_loss: 27.9919\n",
      "Epoch 1379/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.8294 - val_loss: 28.7213\n",
      "Epoch 1380/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.9399 - val_loss: 28.3501\n",
      "Epoch 1381/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.9246 - val_loss: 27.6147\n",
      "Epoch 1382/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.7238 - val_loss: 28.3304\n",
      "Epoch 1383/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.9248 - val_loss: 28.1009\n",
      "Epoch 1384/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.8413 - val_loss: 28.6148\n",
      "Epoch 1385/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.5708 - val_loss: 28.2708\n",
      "Epoch 1386/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.7155 - val_loss: 27.6306\n",
      "Epoch 1387/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.8762 - val_loss: 28.2547\n",
      "Epoch 1388/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.7607 - val_loss: 28.3732\n",
      "Epoch 1389/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.8285 - val_loss: 28.2619\n",
      "Epoch 1390/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.9733 - val_loss: 28.1713\n",
      "Epoch 1391/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.9348 - val_loss: 28.9140\n",
      "Epoch 1392/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.7635 - val_loss: 28.4969\n",
      "Epoch 1393/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.6464 - val_loss: 27.9562\n",
      "Epoch 1394/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.8673 - val_loss: 28.3100\n",
      "Epoch 1395/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.0023 - val_loss: 29.0454\n",
      "Epoch 1396/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.2560 - val_loss: 28.4912\n",
      "Epoch 1397/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.2302 - val_loss: 28.0141\n",
      "Epoch 1398/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.8556 - val_loss: 28.7613\n",
      "Epoch 1399/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.0429 - val_loss: 28.0289\n",
      "Epoch 1400/2000\n",
      "6/6 [==============================] - ETA: 0s - loss: 25.8913\n",
      "Epoch 01400: saving model to saved_models/latent128/cp-1400.h5\n",
      "6/6 [==============================] - 1s 160ms/step - loss: 25.8913 - val_loss: 27.5111\n",
      "Epoch 1401/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.7351 - val_loss: 28.3206\n",
      "Epoch 1402/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.0450 - val_loss: 28.3424\n",
      "Epoch 1403/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.9974 - val_loss: 28.3232\n",
      "Epoch 1404/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.8951 - val_loss: 28.6452\n",
      "Epoch 1405/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.8476 - val_loss: 28.0570\n",
      "Epoch 1406/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.1558 - val_loss: 28.4998\n",
      "Epoch 1407/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.9324 - val_loss: 28.0306\n",
      "Epoch 1408/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.7292 - val_loss: 28.5431\n",
      "Epoch 1409/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.7049 - val_loss: 27.9459\n",
      "Epoch 1410/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.7474 - val_loss: 27.3716\n",
      "Epoch 1411/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.3862 - val_loss: 28.5763\n",
      "Epoch 1412/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.8024 - val_loss: 27.5726\n",
      "Epoch 1413/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.6458 - val_loss: 27.4638\n",
      "Epoch 1414/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.6090 - val_loss: 27.8664\n",
      "Epoch 1415/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.4529 - val_loss: 28.0273\n",
      "Epoch 1416/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.7095 - val_loss: 28.2557\n",
      "Epoch 1417/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.4208 - val_loss: 28.3110\n",
      "Epoch 1418/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.8783 - val_loss: 27.8150\n",
      "Epoch 1419/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.9169 - val_loss: 28.0069\n",
      "Epoch 1420/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.8665 - val_loss: 28.6680\n",
      "Epoch 1421/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.6300 - val_loss: 28.7671\n",
      "Epoch 1422/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.5627 - val_loss: 28.1308\n",
      "Epoch 1423/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.6546 - val_loss: 28.8974\n",
      "Epoch 1424/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.6124 - val_loss: 29.4041\n",
      "Epoch 1425/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.1124 - val_loss: 28.3298\n",
      "Epoch 1426/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.6896 - val_loss: 29.2294\n",
      "Epoch 1427/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.3553 - val_loss: 28.1340\n",
      "Epoch 1428/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.8544 - val_loss: 28.4013\n",
      "Epoch 1429/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.9834 - val_loss: 28.6514\n",
      "Epoch 1430/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.9578 - val_loss: 28.8973\n",
      "Epoch 1431/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.9764 - val_loss: 28.0437\n",
      "Epoch 1432/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.1513 - val_loss: 27.5736\n",
      "Epoch 1433/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.5711 - val_loss: 27.6650\n",
      "Epoch 1434/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.4948 - val_loss: 29.4837\n",
      "Epoch 1435/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 26.0124 - val_loss: 28.2810\n",
      "Epoch 1436/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.8215 - val_loss: 28.7992\n",
      "Epoch 1437/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.6775 - val_loss: 28.1993\n",
      "Epoch 1438/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.7087 - val_loss: 28.4095\n",
      "Epoch 1439/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.8024 - val_loss: 27.3317\n",
      "Epoch 1440/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.6856 - val_loss: 28.6412\n",
      "Epoch 1441/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.6640 - val_loss: 28.6869\n",
      "Epoch 1442/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.7059 - val_loss: 28.1730\n",
      "Epoch 1443/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.6145 - val_loss: 28.3626\n",
      "Epoch 1444/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.6430 - val_loss: 28.0305\n",
      "Epoch 1445/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.3847 - val_loss: 27.8023\n",
      "Epoch 1446/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.6919 - val_loss: 28.6848\n",
      "Epoch 1447/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.6788 - val_loss: 27.7872\n",
      "Epoch 1448/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.4599 - val_loss: 27.5228\n",
      "Epoch 1449/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.4086 - val_loss: 27.4998\n",
      "Epoch 1450/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.5634 - val_loss: 28.1101\n",
      "Epoch 1451/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.6802 - val_loss: 27.5843\n",
      "Epoch 1452/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.5429 - val_loss: 27.5285\n",
      "Epoch 1453/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.3610 - val_loss: 28.2997\n",
      "Epoch 1454/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.6980 - val_loss: 28.0784\n",
      "Epoch 1455/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.9150 - val_loss: 27.0987\n",
      "Epoch 1456/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.7439 - val_loss: 27.4989\n",
      "Epoch 1457/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.5659 - val_loss: 27.2909\n",
      "Epoch 1458/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.5735 - val_loss: 28.1220\n",
      "Epoch 1459/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.5611 - val_loss: 28.2241\n",
      "Epoch 1460/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.6605 - val_loss: 28.0362\n",
      "Epoch 1461/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.4998 - val_loss: 28.2967\n",
      "Epoch 1462/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.5855 - val_loss: 27.9215\n",
      "Epoch 1463/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.5153 - val_loss: 27.4397\n",
      "Epoch 1464/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.5793 - val_loss: 27.8583\n",
      "Epoch 1465/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.3207 - val_loss: 28.1220\n",
      "Epoch 1466/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.6770 - val_loss: 28.3556\n",
      "Epoch 1467/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.6827 - val_loss: 27.7068\n",
      "Epoch 1468/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.5255 - val_loss: 28.2543\n",
      "Epoch 1469/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.8972 - val_loss: 27.6245\n",
      "Epoch 1470/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.6900 - val_loss: 27.0262\n",
      "Epoch 1471/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.4420 - val_loss: 27.8978\n",
      "Epoch 1472/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.5028 - val_loss: 27.5969\n",
      "Epoch 1473/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.4792 - val_loss: 28.0725\n",
      "Epoch 1474/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.4562 - val_loss: 27.6963\n",
      "Epoch 1475/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.3663 - val_loss: 27.7679\n",
      "Epoch 1476/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.3830 - val_loss: 27.6010\n",
      "Epoch 1477/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.3105 - val_loss: 27.3166\n",
      "Epoch 1478/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.2190 - val_loss: 28.2672\n",
      "Epoch 1479/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.6272 - val_loss: 27.7264\n",
      "Epoch 1480/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.5598 - val_loss: 28.4694\n",
      "Epoch 1481/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.5113 - val_loss: 28.0176\n",
      "Epoch 1482/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.6232 - val_loss: 27.9945\n",
      "Epoch 1483/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.4392 - val_loss: 27.2563\n",
      "Epoch 1484/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.4196 - val_loss: 27.7105\n",
      "Epoch 1485/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.3707 - val_loss: 28.8168\n",
      "Epoch 1486/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.4430 - val_loss: 28.2679\n",
      "Epoch 1487/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.5866 - val_loss: 27.5313\n",
      "Epoch 1488/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.9867 - val_loss: 28.2066\n",
      "Epoch 1489/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.6336 - val_loss: 28.7000\n",
      "Epoch 1490/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.6164 - val_loss: 27.9412\n",
      "Epoch 1491/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.3533 - val_loss: 28.4481\n",
      "Epoch 1492/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.3830 - val_loss: 28.0336\n",
      "Epoch 1493/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.1926 - val_loss: 27.9545\n",
      "Epoch 1494/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.3484 - val_loss: 28.0660\n",
      "Epoch 1495/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.3970 - val_loss: 27.6209\n",
      "Epoch 1496/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.2359 - val_loss: 27.7184\n",
      "Epoch 1497/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.4956 - val_loss: 27.9380\n",
      "Epoch 1498/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.4794 - val_loss: 28.1820\n",
      "Epoch 1499/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.5652 - val_loss: 27.4168\n",
      "Epoch 1500/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.5500 - val_loss: 28.0526\n",
      "Epoch 1501/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.4197 - val_loss: 27.2189\n",
      "Epoch 1502/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.5331 - val_loss: 28.3199\n",
      "Epoch 1503/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.8662 - val_loss: 27.7763\n",
      "Epoch 1504/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.5785 - val_loss: 28.0193\n",
      "Epoch 1505/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.4617 - val_loss: 27.4877\n",
      "Epoch 1506/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.2929 - val_loss: 28.0240\n",
      "Epoch 1507/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.3719 - val_loss: 27.8652\n",
      "Epoch 1508/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.2784 - val_loss: 27.7075\n",
      "Epoch 1509/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.2588 - val_loss: 27.5892\n",
      "Epoch 1510/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.4591 - val_loss: 28.3580\n",
      "Epoch 1511/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.6252 - val_loss: 27.3845\n",
      "Epoch 1512/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.4122 - val_loss: 28.2445\n",
      "Epoch 1513/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.6572 - val_loss: 26.8602\n",
      "Epoch 1514/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.3503 - val_loss: 28.0900\n",
      "Epoch 1515/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.3334 - val_loss: 27.9700\n",
      "Epoch 1516/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.4807 - val_loss: 27.8474\n",
      "Epoch 1517/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.1912 - val_loss: 27.9468\n",
      "Epoch 1518/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.3725 - val_loss: 27.6189\n",
      "Epoch 1519/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.6156 - val_loss: 27.2738\n",
      "Epoch 1520/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.4854 - val_loss: 27.5417\n",
      "Epoch 1521/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.3832 - val_loss: 28.3644\n",
      "Epoch 1522/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.1956 - val_loss: 27.9021\n",
      "Epoch 1523/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.3615 - val_loss: 28.1927\n",
      "Epoch 1524/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.6364 - val_loss: 28.7766\n",
      "Epoch 1525/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.5195 - val_loss: 27.9458\n",
      "Epoch 1526/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.4807 - val_loss: 27.5811\n",
      "Epoch 1527/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.5135 - val_loss: 27.6287\n",
      "Epoch 1528/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.0756 - val_loss: 29.0520\n",
      "Epoch 1529/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.4379 - val_loss: 27.2034\n",
      "Epoch 1530/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.0505 - val_loss: 27.9696\n",
      "Epoch 1531/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.1982 - val_loss: 27.6885\n",
      "Epoch 1532/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.3542 - val_loss: 27.5787\n",
      "Epoch 1533/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.1246 - val_loss: 27.2829\n",
      "Epoch 1534/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.1471 - val_loss: 28.2878\n",
      "Epoch 1535/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.3151 - val_loss: 28.2336\n",
      "Epoch 1536/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.2757 - val_loss: 27.1575\n",
      "Epoch 1537/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.1338 - val_loss: 26.7833\n",
      "Epoch 1538/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.1121 - val_loss: 27.7708\n",
      "Epoch 1539/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 25.0430 - val_loss: 27.5546\n",
      "Epoch 1540/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.0823 - val_loss: 27.2958\n",
      "Epoch 1541/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.3773 - val_loss: 27.6719\n",
      "Epoch 1542/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.5628 - val_loss: 27.5156\n",
      "Epoch 1543/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.2107 - val_loss: 28.2148\n",
      "Epoch 1544/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.1806 - val_loss: 27.2876\n",
      "Epoch 1545/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.3292 - val_loss: 26.8511\n",
      "Epoch 1546/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.2717 - val_loss: 28.3902\n",
      "Epoch 1547/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.2512 - val_loss: 27.4147\n",
      "Epoch 1548/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.1924 - val_loss: 26.9620\n",
      "Epoch 1549/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.3304 - val_loss: 27.6659\n",
      "Epoch 1550/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.1905 - val_loss: 26.9331\n",
      "Epoch 1551/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.1831 - val_loss: 27.8111\n",
      "Epoch 1552/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.4454 - val_loss: 27.4331\n",
      "Epoch 1553/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.2165 - val_loss: 28.1229\n",
      "Epoch 1554/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.1497 - val_loss: 28.2318\n",
      "Epoch 1555/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.1876 - val_loss: 27.9986\n",
      "Epoch 1556/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.2076 - val_loss: 27.8519\n",
      "Epoch 1557/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.4522 - val_loss: 28.2389\n",
      "Epoch 1558/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.2186 - val_loss: 27.5836\n",
      "Epoch 1559/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.2971 - val_loss: 28.3727\n",
      "Epoch 1560/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.4202 - val_loss: 27.8763\n",
      "Epoch 1561/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.3147 - val_loss: 27.5474\n",
      "Epoch 1562/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.1467 - val_loss: 27.5717\n",
      "Epoch 1563/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.2731 - val_loss: 27.7891\n",
      "Epoch 1564/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.1854 - val_loss: 27.6735\n",
      "Epoch 1565/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.2893 - val_loss: 27.1774\n",
      "Epoch 1566/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.3431 - val_loss: 27.3709\n",
      "Epoch 1567/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.1284 - val_loss: 27.5912\n",
      "Epoch 1568/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.2653 - val_loss: 27.1111\n",
      "Epoch 1569/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.2365 - val_loss: 26.5829\n",
      "Epoch 1570/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.9615 - val_loss: 27.4935\n",
      "Epoch 1571/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.3438 - val_loss: 27.3952\n",
      "Epoch 1572/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.1791 - val_loss: 26.9609\n",
      "Epoch 1573/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.1783 - val_loss: 27.4174\n",
      "Epoch 1574/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.3695 - val_loss: 26.8745\n",
      "Epoch 1575/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.2886 - val_loss: 27.4316\n",
      "Epoch 1576/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.2317 - val_loss: 27.7404\n",
      "Epoch 1577/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.2763 - val_loss: 28.3594\n",
      "Epoch 1578/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.2041 - val_loss: 27.3942\n",
      "Epoch 1579/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 25.2585 - val_loss: 27.6252\n",
      "Epoch 1580/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.2069 - val_loss: 27.2088\n",
      "Epoch 1581/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.1996 - val_loss: 27.8314\n",
      "Epoch 1582/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.3487 - val_loss: 28.4054\n",
      "Epoch 1583/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.4407 - val_loss: 27.7129\n",
      "Epoch 1584/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.6333 - val_loss: 28.0786\n",
      "Epoch 1585/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.3939 - val_loss: 28.1412\n",
      "Epoch 1586/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.3991 - val_loss: 27.2645\n",
      "Epoch 1587/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.1657 - val_loss: 27.9699\n",
      "Epoch 1588/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.1229 - val_loss: 27.6355\n",
      "Epoch 1589/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.9499 - val_loss: 26.9803\n",
      "Epoch 1590/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.0554 - val_loss: 26.9993\n",
      "Epoch 1591/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.1011 - val_loss: 27.1030\n",
      "Epoch 1592/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 24.9026 - val_loss: 27.6567\n",
      "Epoch 1593/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.1299 - val_loss: 27.4706\n",
      "Epoch 1594/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.1671 - val_loss: 27.5559\n",
      "Epoch 1595/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.2668 - val_loss: 27.6081\n",
      "Epoch 1596/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.0980 - val_loss: 27.6797\n",
      "Epoch 1597/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.2480 - val_loss: 27.7507\n",
      "Epoch 1598/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.2576 - val_loss: 27.3466\n",
      "Epoch 1599/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.2466 - val_loss: 27.3286\n",
      "Epoch 1600/2000\n",
      "6/6 [==============================] - ETA: 0s - loss: 25.3421\n",
      "Epoch 01600: saving model to saved_models/latent128/cp-1600.h5\n",
      "6/6 [==============================] - 1s 156ms/step - loss: 25.3421 - val_loss: 27.8230\n",
      "Epoch 1601/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.2258 - val_loss: 27.5619\n",
      "Epoch 1602/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.2846 - val_loss: 28.4926\n",
      "Epoch 1603/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.6414 - val_loss: 28.1000\n",
      "Epoch 1604/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.3962 - val_loss: 27.3664\n",
      "Epoch 1605/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.4597 - val_loss: 28.5183\n",
      "Epoch 1606/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.3223 - val_loss: 27.9478\n",
      "Epoch 1607/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.3900 - val_loss: 28.3817\n",
      "Epoch 1608/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.4459 - val_loss: 27.8919\n",
      "Epoch 1609/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.2338 - val_loss: 27.5063\n",
      "Epoch 1610/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.2769 - val_loss: 28.4100\n",
      "Epoch 1611/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.3377 - val_loss: 27.2481\n",
      "Epoch 1612/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.3768 - val_loss: 27.5948\n",
      "Epoch 1613/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.1621 - val_loss: 27.7457\n",
      "Epoch 1614/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.2833 - val_loss: 27.8842\n",
      "Epoch 1615/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.2771 - val_loss: 27.6385\n",
      "Epoch 1616/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.2207 - val_loss: 27.4792\n",
      "Epoch 1617/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.3920 - val_loss: 27.6037\n",
      "Epoch 1618/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.0625 - val_loss: 27.7740\n",
      "Epoch 1619/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.4556 - val_loss: 27.9071\n",
      "Epoch 1620/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.2870 - val_loss: 27.6641\n",
      "Epoch 1621/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.9865 - val_loss: 28.7429\n",
      "Epoch 1622/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.0796 - val_loss: 27.9197\n",
      "Epoch 1623/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.2200 - val_loss: 28.2064\n",
      "Epoch 1624/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.2134 - val_loss: 28.1009\n",
      "Epoch 1625/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.2913 - val_loss: 27.8272\n",
      "Epoch 1626/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.4083 - val_loss: 27.5291\n",
      "Epoch 1627/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.5974 - val_loss: 28.1112\n",
      "Epoch 1628/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.3534 - val_loss: 27.9436\n",
      "Epoch 1629/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.0536 - val_loss: 27.4575\n",
      "Epoch 1630/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.2662 - val_loss: 27.4919\n",
      "Epoch 1631/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.2061 - val_loss: 27.3193\n",
      "Epoch 1632/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.9464 - val_loss: 26.8235\n",
      "Epoch 1633/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.1988 - val_loss: 27.5793\n",
      "Epoch 1634/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.4053 - val_loss: 26.9218\n",
      "Epoch 1635/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.1765 - val_loss: 27.1419\n",
      "Epoch 1636/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.0946 - val_loss: 27.9422\n",
      "Epoch 1637/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.2177 - val_loss: 28.3249\n",
      "Epoch 1638/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.1451 - val_loss: 28.1690\n",
      "Epoch 1639/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.4109 - val_loss: 27.2941\n",
      "Epoch 1640/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.2108 - val_loss: 27.7047\n",
      "Epoch 1641/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.3160 - val_loss: 27.5266\n",
      "Epoch 1642/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.1584 - val_loss: 27.1227\n",
      "Epoch 1643/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.2107 - val_loss: 27.6471\n",
      "Epoch 1644/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.3508 - val_loss: 28.1059\n",
      "Epoch 1645/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.2720 - val_loss: 27.7233\n",
      "Epoch 1646/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.1980 - val_loss: 27.0970\n",
      "Epoch 1647/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.0717 - val_loss: 27.1576\n",
      "Epoch 1648/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.3260 - val_loss: 27.9668\n",
      "Epoch 1649/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.0612 - val_loss: 27.8606\n",
      "Epoch 1650/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.2789 - val_loss: 27.4904\n",
      "Epoch 1651/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.1554 - val_loss: 27.7895\n",
      "Epoch 1652/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.0526 - val_loss: 28.0322\n",
      "Epoch 1653/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.1040 - val_loss: 27.5297\n",
      "Epoch 1654/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.0243 - val_loss: 27.3913\n",
      "Epoch 1655/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.9489 - val_loss: 27.5617\n",
      "Epoch 1656/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.0962 - val_loss: 27.1060\n",
      "Epoch 1657/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.9667 - val_loss: 28.2928\n",
      "Epoch 1658/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.1998 - val_loss: 28.4675\n",
      "Epoch 1659/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.1561 - val_loss: 27.4167\n",
      "Epoch 1660/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.3102 - val_loss: 27.9716\n",
      "Epoch 1661/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.2825 - val_loss: 27.1898\n",
      "Epoch 1662/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.1860 - val_loss: 27.9690\n",
      "Epoch 1663/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.3119 - val_loss: 27.9148\n",
      "Epoch 1664/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.5464 - val_loss: 28.6086\n",
      "Epoch 1665/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.4013 - val_loss: 27.8587\n",
      "Epoch 1666/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.1553 - val_loss: 27.1630\n",
      "Epoch 1667/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.9900 - val_loss: 27.2970\n",
      "Epoch 1668/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.9332 - val_loss: 27.4709\n",
      "Epoch 1669/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.0384 - val_loss: 27.1833\n",
      "Epoch 1670/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.9718 - val_loss: 27.3771\n",
      "Epoch 1671/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.9900 - val_loss: 27.8675\n",
      "Epoch 1672/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.0535 - val_loss: 27.1882\n",
      "Epoch 1673/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.8849 - val_loss: 27.3319\n",
      "Epoch 1674/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.1139 - val_loss: 28.4108\n",
      "Epoch 1675/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.2283 - val_loss: 27.2004\n",
      "Epoch 1676/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.9746 - val_loss: 27.9129\n",
      "Epoch 1677/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.9410 - val_loss: 27.7367\n",
      "Epoch 1678/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.1560 - val_loss: 27.4804\n",
      "Epoch 1679/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.1417 - val_loss: 28.2789\n",
      "Epoch 1680/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.2705 - val_loss: 27.6290\n",
      "Epoch 1681/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.9997 - val_loss: 27.6562\n",
      "Epoch 1682/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.9450 - val_loss: 27.6805\n",
      "Epoch 1683/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.0346 - val_loss: 27.2136\n",
      "Epoch 1684/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.2870 - val_loss: 27.1658\n",
      "Epoch 1685/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 24.8156 - val_loss: 27.3419\n",
      "Epoch 1686/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.9222 - val_loss: 26.9650\n",
      "Epoch 1687/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.0765 - val_loss: 27.9063\n",
      "Epoch 1688/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.2650 - val_loss: 27.4826\n",
      "Epoch 1689/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.0129 - val_loss: 28.2960\n",
      "Epoch 1690/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.9939 - val_loss: 27.7475\n",
      "Epoch 1691/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.0373 - val_loss: 27.5868\n",
      "Epoch 1692/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 25.2412 - val_loss: 28.0398\n",
      "Epoch 1693/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.3848 - val_loss: 27.8109\n",
      "Epoch 1694/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.2695 - val_loss: 27.3906\n",
      "Epoch 1695/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.1864 - val_loss: 27.5010\n",
      "Epoch 1696/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.2982 - val_loss: 27.4063\n",
      "Epoch 1697/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.3411 - val_loss: 27.5286\n",
      "Epoch 1698/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.2002 - val_loss: 27.4005\n",
      "Epoch 1699/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.1894 - val_loss: 27.3273\n",
      "Epoch 1700/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 25.0176 - val_loss: 27.7720\n",
      "Epoch 1701/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.8335 - val_loss: 27.3635\n",
      "Epoch 1702/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.9368 - val_loss: 27.1887\n",
      "Epoch 1703/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.9053 - val_loss: 27.6129\n",
      "Epoch 1704/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.9392 - val_loss: 27.2917\n",
      "Epoch 1705/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.9776 - val_loss: 29.3519\n",
      "Epoch 1706/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.2932 - val_loss: 28.4195\n",
      "Epoch 1707/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.3864 - val_loss: 27.8576\n",
      "Epoch 1708/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.2508 - val_loss: 27.6250\n",
      "Epoch 1709/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.0799 - val_loss: 27.9630\n",
      "Epoch 1710/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.2643 - val_loss: 27.5439\n",
      "Epoch 1711/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.2441 - val_loss: 27.3400\n",
      "Epoch 1712/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.8708 - val_loss: 27.3180\n",
      "Epoch 1713/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.8298 - val_loss: 28.3604\n",
      "Epoch 1714/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.2368 - val_loss: 27.3554\n",
      "Epoch 1715/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.1223 - val_loss: 27.9550\n",
      "Epoch 1716/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.4094 - val_loss: 27.3796\n",
      "Epoch 1717/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.8983 - val_loss: 27.4639\n",
      "Epoch 1718/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.0710 - val_loss: 27.3632\n",
      "Epoch 1719/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.4361 - val_loss: 27.8068\n",
      "Epoch 1720/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.1957 - val_loss: 28.1750\n",
      "Epoch 1721/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.3562 - val_loss: 27.6506\n",
      "Epoch 1722/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.3163 - val_loss: 27.9376\n",
      "Epoch 1723/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.0781 - val_loss: 27.9700\n",
      "Epoch 1724/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.2404 - val_loss: 27.3376\n",
      "Epoch 1725/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.2085 - val_loss: 27.2144\n",
      "Epoch 1726/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.1533 - val_loss: 27.9966\n",
      "Epoch 1727/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.2606 - val_loss: 27.7007\n",
      "Epoch 1728/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.1128 - val_loss: 27.0060\n",
      "Epoch 1729/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.0865 - val_loss: 27.3720\n",
      "Epoch 1730/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.1344 - val_loss: 27.5845\n",
      "Epoch 1731/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.7745 - val_loss: 28.2301\n",
      "Epoch 1732/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.8886 - val_loss: 27.2537\n",
      "Epoch 1733/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.9590 - val_loss: 28.0823\n",
      "Epoch 1734/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.9284 - val_loss: 26.9253\n",
      "Epoch 1735/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.0734 - val_loss: 27.5565\n",
      "Epoch 1736/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.1163 - val_loss: 27.6380\n",
      "Epoch 1737/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.1598 - val_loss: 27.4267\n",
      "Epoch 1738/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.9331 - val_loss: 27.2343\n",
      "Epoch 1739/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.1272 - val_loss: 27.3084\n",
      "Epoch 1740/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.0614 - val_loss: 27.7292\n",
      "Epoch 1741/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.2413 - val_loss: 27.5442\n",
      "Epoch 1742/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.9461 - val_loss: 27.2072\n",
      "Epoch 1743/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.1629 - val_loss: 27.2173\n",
      "Epoch 1744/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.9775 - val_loss: 27.5421\n",
      "Epoch 1745/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.1531 - val_loss: 27.7232\n",
      "Epoch 1746/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.9473 - val_loss: 27.1694\n",
      "Epoch 1747/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.8245 - val_loss: 27.7861\n",
      "Epoch 1748/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.9496 - val_loss: 27.3049\n",
      "Epoch 1749/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.8545 - val_loss: 27.3393\n",
      "Epoch 1750/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 24.6409 - val_loss: 27.1939\n",
      "Epoch 1751/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.0998 - val_loss: 27.4201\n",
      "Epoch 1752/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.0803 - val_loss: 27.1126\n",
      "Epoch 1753/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.0166 - val_loss: 27.6631\n",
      "Epoch 1754/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.0695 - val_loss: 27.2541\n",
      "Epoch 1755/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.9011 - val_loss: 27.6353\n",
      "Epoch 1756/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.9256 - val_loss: 27.0953\n",
      "Epoch 1757/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.8745 - val_loss: 27.4073\n",
      "Epoch 1758/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.8691 - val_loss: 27.2030\n",
      "Epoch 1759/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.8908 - val_loss: 26.7950\n",
      "Epoch 1760/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.7999 - val_loss: 27.3557\n",
      "Epoch 1761/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.9762 - val_loss: 26.9401\n",
      "Epoch 1762/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.8472 - val_loss: 27.3810\n",
      "Epoch 1763/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.7223 - val_loss: 27.2720\n",
      "Epoch 1764/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.9263 - val_loss: 26.7079\n",
      "Epoch 1765/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.7311 - val_loss: 27.1618\n",
      "Epoch 1766/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.8508 - val_loss: 26.8960\n",
      "Epoch 1767/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.7311 - val_loss: 27.3853\n",
      "Epoch 1768/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.7611 - val_loss: 27.6002\n",
      "Epoch 1769/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.7916 - val_loss: 27.2512\n",
      "Epoch 1770/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.7480 - val_loss: 26.9039\n",
      "Epoch 1771/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.0103 - val_loss: 28.0876\n",
      "Epoch 1772/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.8010 - val_loss: 27.2518\n",
      "Epoch 1773/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.9421 - val_loss: 27.6899\n",
      "Epoch 1774/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.7279 - val_loss: 27.5337\n",
      "Epoch 1775/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.9521 - val_loss: 27.0059\n",
      "Epoch 1776/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.0806 - val_loss: 27.9544\n",
      "Epoch 1777/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.0558 - val_loss: 27.4218\n",
      "Epoch 1778/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.8055 - val_loss: 27.8148\n",
      "Epoch 1779/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.8342 - val_loss: 26.9203\n",
      "Epoch 1780/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.8982 - val_loss: 27.8751\n",
      "Epoch 1781/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.0666 - val_loss: 27.4575\n",
      "Epoch 1782/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.0441 - val_loss: 27.8369\n",
      "Epoch 1783/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.1695 - val_loss: 26.9840\n",
      "Epoch 1784/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.9552 - val_loss: 28.0763\n",
      "Epoch 1785/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.8565 - val_loss: 27.7661\n",
      "Epoch 1786/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.8313 - val_loss: 28.5321\n",
      "Epoch 1787/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.9022 - val_loss: 26.9250\n",
      "Epoch 1788/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.6609 - val_loss: 27.6680\n",
      "Epoch 1789/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.7204 - val_loss: 26.8810\n",
      "Epoch 1790/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.8237 - val_loss: 27.7173\n",
      "Epoch 1791/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.8114 - val_loss: 28.1726\n",
      "Epoch 1792/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.8396 - val_loss: 26.6115\n",
      "Epoch 1793/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.8052 - val_loss: 27.4814\n",
      "Epoch 1794/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.8578 - val_loss: 27.0698\n",
      "Epoch 1795/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.6955 - val_loss: 26.9895\n",
      "Epoch 1796/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.8362 - val_loss: 28.0638\n",
      "Epoch 1797/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.8988 - val_loss: 26.8884\n",
      "Epoch 1798/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.7091 - val_loss: 27.3305\n",
      "Epoch 1799/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.8309 - val_loss: 27.5374\n",
      "Epoch 1800/2000\n",
      "6/6 [==============================] - ETA: 0s - loss: 24.7262\n",
      "Epoch 01800: saving model to saved_models/latent128/cp-1800.h5\n",
      "6/6 [==============================] - 1s 149ms/step - loss: 24.7262 - val_loss: 26.8346\n",
      "Epoch 1801/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.7081 - val_loss: 27.2455\n",
      "Epoch 1802/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.8673 - val_loss: 27.0461\n",
      "Epoch 1803/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 24.6232 - val_loss: 26.9097\n",
      "Epoch 1804/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.7166 - val_loss: 26.9290\n",
      "Epoch 1805/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.5958 - val_loss: 26.9457\n",
      "Epoch 1806/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.6471 - val_loss: 27.5302\n",
      "Epoch 1807/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.5284 - val_loss: 27.6014\n",
      "Epoch 1808/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.9237 - val_loss: 27.3962\n",
      "Epoch 1809/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.7540 - val_loss: 26.6943\n",
      "Epoch 1810/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.7912 - val_loss: 27.5274\n",
      "Epoch 1811/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.9002 - val_loss: 27.6086\n",
      "Epoch 1812/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.8095 - val_loss: 27.5265\n",
      "Epoch 1813/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.6972 - val_loss: 26.9798\n",
      "Epoch 1814/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.8562 - val_loss: 27.7718\n",
      "Epoch 1815/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.8396 - val_loss: 26.4428\n",
      "Epoch 1816/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.9139 - val_loss: 27.0338\n",
      "Epoch 1817/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.9191 - val_loss: 27.4228\n",
      "Epoch 1818/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.9689 - val_loss: 26.3638\n",
      "Epoch 1819/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.6759 - val_loss: 27.3155\n",
      "Epoch 1820/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.7588 - val_loss: 26.7510\n",
      "Epoch 1821/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.9479 - val_loss: 27.9590\n",
      "Epoch 1822/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.7851 - val_loss: 27.2988\n",
      "Epoch 1823/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.1186 - val_loss: 28.1669\n",
      "Epoch 1824/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.0355 - val_loss: 26.5485\n",
      "Epoch 1825/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.0868 - val_loss: 27.3495\n",
      "Epoch 1826/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.0554 - val_loss: 27.6764\n",
      "Epoch 1827/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.7705 - val_loss: 27.4933\n",
      "Epoch 1828/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.7998 - val_loss: 27.5347\n",
      "Epoch 1829/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.7265 - val_loss: 26.8325\n",
      "Epoch 1830/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.7027 - val_loss: 26.6229\n",
      "Epoch 1831/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.5355 - val_loss: 26.7779\n",
      "Epoch 1832/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.7946 - val_loss: 26.6141\n",
      "Epoch 1833/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.7450 - val_loss: 26.9130\n",
      "Epoch 1834/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.8656 - val_loss: 26.6133\n",
      "Epoch 1835/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.6569 - val_loss: 26.5876\n",
      "Epoch 1836/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.8504 - val_loss: 26.9361\n",
      "Epoch 1837/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.8848 - val_loss: 26.9531\n",
      "Epoch 1838/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.8210 - val_loss: 27.0745\n",
      "Epoch 1839/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.1502 - val_loss: 27.5504\n",
      "Epoch 1840/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.9373 - val_loss: 27.2212\n",
      "Epoch 1841/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.8351 - val_loss: 28.0874\n",
      "Epoch 1842/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.5713 - val_loss: 26.8648\n",
      "Epoch 1843/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.8214 - val_loss: 27.5956\n",
      "Epoch 1844/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.7295 - val_loss: 27.6146\n",
      "Epoch 1845/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.7628 - val_loss: 27.9743\n",
      "Epoch 1846/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.7497 - val_loss: 26.8467\n",
      "Epoch 1847/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.7310 - val_loss: 27.0417\n",
      "Epoch 1848/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.8893 - val_loss: 27.9262\n",
      "Epoch 1849/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.9291 - val_loss: 27.5033\n",
      "Epoch 1850/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.7449 - val_loss: 27.2919\n",
      "Epoch 1851/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.8961 - val_loss: 27.9297\n",
      "Epoch 1852/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.8064 - val_loss: 27.0709\n",
      "Epoch 1853/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.6994 - val_loss: 27.9220\n",
      "Epoch 1854/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.7492 - val_loss: 26.9834\n",
      "Epoch 1855/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.4652 - val_loss: 26.8670\n",
      "Epoch 1856/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.5409 - val_loss: 26.5990\n",
      "Epoch 1857/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.3530 - val_loss: 26.8052\n",
      "Epoch 1858/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.4653 - val_loss: 26.4024\n",
      "Epoch 1859/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.5572 - val_loss: 27.1879\n",
      "Epoch 1860/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.5136 - val_loss: 27.2772\n",
      "Epoch 1861/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.6991 - val_loss: 26.8262\n",
      "Epoch 1862/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.6434 - val_loss: 27.0301\n",
      "Epoch 1863/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.5017 - val_loss: 27.2753\n",
      "Epoch 1864/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.5334 - val_loss: 26.7328\n",
      "Epoch 1865/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.6954 - val_loss: 27.0942\n",
      "Epoch 1866/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.5256 - val_loss: 27.3367\n",
      "Epoch 1867/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.7026 - val_loss: 27.7065\n",
      "Epoch 1868/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.6181 - val_loss: 27.1126\n",
      "Epoch 1869/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.6653 - val_loss: 27.0991\n",
      "Epoch 1870/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.5524 - val_loss: 27.4853\n",
      "Epoch 1871/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.8689 - val_loss: 27.0135\n",
      "Epoch 1872/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.6911 - val_loss: 27.2827\n",
      "Epoch 1873/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.8084 - val_loss: 27.6906\n",
      "Epoch 1874/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.7392 - val_loss: 27.9610\n",
      "Epoch 1875/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.8303 - val_loss: 26.9077\n",
      "Epoch 1876/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.8946 - val_loss: 26.5932\n",
      "Epoch 1877/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.6619 - val_loss: 27.2308\n",
      "Epoch 1878/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.7399 - val_loss: 27.6961\n",
      "Epoch 1879/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.8309 - val_loss: 27.3121\n",
      "Epoch 1880/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.8873 - val_loss: 26.8495\n",
      "Epoch 1881/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.6839 - val_loss: 28.0938\n",
      "Epoch 1882/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.8915 - val_loss: 27.4508\n",
      "Epoch 1883/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.6596 - val_loss: 27.4901\n",
      "Epoch 1884/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.7719 - val_loss: 27.0720\n",
      "Epoch 1885/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.7749 - val_loss: 27.2885\n",
      "Epoch 1886/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.9423 - val_loss: 27.0959\n",
      "Epoch 1887/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.9558 - val_loss: 27.8017\n",
      "Epoch 1888/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.8352 - val_loss: 28.9228\n",
      "Epoch 1889/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.7350 - val_loss: 27.5986\n",
      "Epoch 1890/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.9528 - val_loss: 27.0489\n",
      "Epoch 1891/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.6793 - val_loss: 27.3503\n",
      "Epoch 1892/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.8034 - val_loss: 26.9011\n",
      "Epoch 1893/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.8306 - val_loss: 27.2497\n",
      "Epoch 1894/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.7277 - val_loss: 27.3480\n",
      "Epoch 1895/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.8101 - val_loss: 26.7963\n",
      "Epoch 1896/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.7836 - val_loss: 27.4953\n",
      "Epoch 1897/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 25.1459 - val_loss: 27.6036\n",
      "Epoch 1898/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.0620 - val_loss: 28.1534\n",
      "Epoch 1899/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 25.1193 - val_loss: 27.0237\n",
      "Epoch 1900/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.9297 - val_loss: 28.1315\n",
      "Epoch 1901/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.8893 - val_loss: 26.9130\n",
      "Epoch 1902/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.9443 - val_loss: 27.1826\n",
      "Epoch 1903/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.6117 - val_loss: 27.4406\n",
      "Epoch 1904/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.7999 - val_loss: 26.8793\n",
      "Epoch 1905/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.4896 - val_loss: 26.9710\n",
      "Epoch 1906/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.8070 - val_loss: 27.5858\n",
      "Epoch 1907/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.6483 - val_loss: 27.8199\n",
      "Epoch 1908/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.5687 - val_loss: 26.6472\n",
      "Epoch 1909/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.7059 - val_loss: 27.4171\n",
      "Epoch 1910/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.8711 - val_loss: 28.0233\n",
      "Epoch 1911/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.6376 - val_loss: 27.3184\n",
      "Epoch 1912/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.7811 - val_loss: 27.2427\n",
      "Epoch 1913/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.8087 - val_loss: 27.5157\n",
      "Epoch 1914/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.7119 - val_loss: 27.1355\n",
      "Epoch 1915/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.5375 - val_loss: 26.9451\n",
      "Epoch 1916/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.4295 - val_loss: 27.4922\n",
      "Epoch 1917/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.7543 - val_loss: 27.3606\n",
      "Epoch 1918/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.6498 - val_loss: 27.1514\n",
      "Epoch 1919/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.6354 - val_loss: 26.9110\n",
      "Epoch 1920/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.6524 - val_loss: 27.0937\n",
      "Epoch 1921/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.7882 - val_loss: 27.4361\n",
      "Epoch 1922/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.6994 - val_loss: 26.7216\n",
      "Epoch 1923/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.5772 - val_loss: 26.9624\n",
      "Epoch 1924/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.7146 - val_loss: 27.3686\n",
      "Epoch 1925/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.6598 - val_loss: 27.1822\n",
      "Epoch 1926/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.5577 - val_loss: 26.4669\n",
      "Epoch 1927/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.5858 - val_loss: 27.5889\n",
      "Epoch 1928/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.4537 - val_loss: 27.0258\n",
      "Epoch 1929/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.4367 - val_loss: 26.7065\n",
      "Epoch 1930/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.3590 - val_loss: 26.3962\n",
      "Epoch 1931/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.4718 - val_loss: 27.3466\n",
      "Epoch 1932/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.6220 - val_loss: 27.0272\n",
      "Epoch 1933/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.5405 - val_loss: 26.7787\n",
      "Epoch 1934/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.5010 - val_loss: 26.8073\n",
      "Epoch 1935/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.6485 - val_loss: 26.9989\n",
      "Epoch 1936/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.5998 - val_loss: 27.9827\n",
      "Epoch 1937/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.5888 - val_loss: 27.1126\n",
      "Epoch 1938/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.4917 - val_loss: 27.4352\n",
      "Epoch 1939/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.5293 - val_loss: 27.2663\n",
      "Epoch 1940/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.6024 - val_loss: 27.2484\n",
      "Epoch 1941/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.6958 - val_loss: 26.7740\n",
      "Epoch 1942/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.6827 - val_loss: 27.0117\n",
      "Epoch 1943/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.7299 - val_loss: 27.8430\n",
      "Epoch 1944/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.7847 - val_loss: 27.1738\n",
      "Epoch 1945/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.8469 - val_loss: 27.4146\n",
      "Epoch 1946/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.6685 - val_loss: 27.6206\n",
      "Epoch 1947/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.6015 - val_loss: 26.9401\n",
      "Epoch 1948/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.5030 - val_loss: 26.5895\n",
      "Epoch 1949/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.5623 - val_loss: 27.6973\n",
      "Epoch 1950/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.6334 - val_loss: 27.2197\n",
      "Epoch 1951/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.6652 - val_loss: 26.6926\n",
      "Epoch 1952/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.3382 - val_loss: 27.1389\n",
      "Epoch 1953/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.6816 - val_loss: 27.2034\n",
      "Epoch 1954/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.6378 - val_loss: 27.5903\n",
      "Epoch 1955/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.5821 - val_loss: 27.5568\n",
      "Epoch 1956/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.5216 - val_loss: 27.1175\n",
      "Epoch 1957/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.5988 - val_loss: 27.3405\n",
      "Epoch 1958/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.6559 - val_loss: 27.2442\n",
      "Epoch 1959/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.7346 - val_loss: 27.1655\n",
      "Epoch 1960/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.5150 - val_loss: 27.2830\n",
      "Epoch 1961/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.7845 - val_loss: 27.8359\n",
      "Epoch 1962/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.7702 - val_loss: 27.2022\n",
      "Epoch 1963/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.8441 - val_loss: 26.9904\n",
      "Epoch 1964/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.5727 - val_loss: 28.0285\n",
      "Epoch 1965/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.7721 - val_loss: 26.7362\n",
      "Epoch 1966/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.9275 - val_loss: 27.4386\n",
      "Epoch 1967/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.6373 - val_loss: 27.8744\n",
      "Epoch 1968/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.6797 - val_loss: 27.6683\n",
      "Epoch 1969/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.5767 - val_loss: 26.6272\n",
      "Epoch 1970/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 24.2742 - val_loss: 26.7637\n",
      "Epoch 1971/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.3971 - val_loss: 27.3074\n",
      "Epoch 1972/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.1704 - val_loss: 27.0989\n",
      "Epoch 1973/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.3902 - val_loss: 26.8962\n",
      "Epoch 1974/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.5909 - val_loss: 26.9931\n",
      "Epoch 1975/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.6410 - val_loss: 27.1919\n",
      "Epoch 1976/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.6964 - val_loss: 26.6976\n",
      "Epoch 1977/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.4002 - val_loss: 27.0908\n",
      "Epoch 1978/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.4814 - val_loss: 26.7418\n",
      "Epoch 1979/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.5135 - val_loss: 26.8924\n",
      "Epoch 1980/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.6370 - val_loss: 27.3595\n",
      "Epoch 1981/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.4790 - val_loss: 26.9798\n",
      "Epoch 1982/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.6062 - val_loss: 27.0476\n",
      "Epoch 1983/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.5175 - val_loss: 27.4483\n",
      "Epoch 1984/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.4743 - val_loss: 26.4032\n",
      "Epoch 1985/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 24.2331 - val_loss: 27.2601\n",
      "Epoch 1986/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.3200 - val_loss: 26.7739\n",
      "Epoch 1987/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.4626 - val_loss: 27.1577\n",
      "Epoch 1988/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.5483 - val_loss: 27.1728\n",
      "Epoch 1989/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.4636 - val_loss: 27.0798\n",
      "Epoch 1990/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.4792 - val_loss: 26.7303\n",
      "Epoch 1991/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.5537 - val_loss: 27.1230\n",
      "Epoch 1992/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.5534 - val_loss: 26.7630\n",
      "Epoch 1993/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.3720 - val_loss: 27.3535\n",
      "Epoch 1994/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.7074 - val_loss: 26.9362\n",
      "Epoch 1995/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.6013 - val_loss: 27.0063\n",
      "Epoch 1996/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 24.8677 - val_loss: 27.7687\n",
      "Epoch 1997/2000\n",
      "6/6 [==============================] - 1s 100ms/step - loss: 24.7516 - val_loss: 26.8693\n",
      "Epoch 1998/2000\n",
      "6/6 [==============================] - 1s 100ms/step - loss: 24.6962 - val_loss: 27.4529\n",
      "Epoch 1999/2000\n",
      "6/6 [==============================] - 1s 101ms/step - loss: 24.9012 - val_loss: 27.5113\n",
      "Epoch 2000/2000\n",
      "6/6 [==============================] - ETA: 0s - loss: 24.6087\n",
      "Epoch 02000: saving model to saved_models/latent128/cp-2000.h5\n",
      "6/6 [==============================] - 1s 147ms/step - loss: 24.6087 - val_loss: 27.7174\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Train VAE model with callbacks \"\"\"\n",
    "history = vae.fit(trainX, trainX, \n",
    "                  batch_size = batch_size, \n",
    "                  epochs = epochs, \n",
    "                  callbacks=[initial_checkpoint, checkpoint, EarlyStoppingAtMinLoss()],\n",
    "                  #callbacks=[initial_checkpoint, checkpoint],\n",
    "                  shuffle=True,\n",
    "                  validation_data=(valX, valX)\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step size: 6.0\n"
     ]
    }
   ],
   "source": [
    "# In this GPU implementation, it shows the number of batches/iterations for one epoch (and not the training samples), which is:\n",
    "print (\"Step size:\", np.ceil(trainX.shape[0]/batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Save the entire model \"\"\"\n",
    "# Save notes about hyperparameters used:\n",
    "with open(f'{SAVE_FOLDER}/notes.txt', 'w') as f:\n",
    "    f.write(f'Epochs: {epochs} \\n')\n",
    "    f.write(f'Batch size: {batch_size} \\n')\n",
    "    f.write(f'Latent dimension: {latent_dim} \\n')\n",
    "    f.write(f'Filter sizes: {filters} \\n')\n",
    "    f.write(f'img_width: {img_width} \\n')\n",
    "    f.write(f'img_height: {img_height} \\n')\n",
    "    f.write(f'num_channels: {num_channels}')\n",
    "    f.close()\n",
    "\n",
    "# Save weights for encoder model:\n",
    "encoder.save_weights(f'{SAVE_FOLDER}/ENCODER_WEIGHTS.h5')\n",
    "\n",
    "# Save weights for decoder model:\n",
    "decoder.save_weights(f'{SAVE_FOLDER}/DECODER_WEIGHTS.h5')\n",
    "\n",
    "# Save weights for total VAE:\n",
    "vae.save_weights(f'{SAVE_FOLDER}/VAE_WEIGHTS.h5')\n",
    "\n",
    "# Save the history at each epochs (cost of train and validation sets):\n",
    "np.save(f'{SAVE_FOLDER}/HISTORY.npy', history.history)\n",
    "\n",
    "# Save exact training, validation and testing data set (as numpy arrays):\n",
    "np.save(f'{SAVE_FOLDER}/trainX.npy', trainX)\n",
    "np.save(f'{SAVE_FOLDER}/valX.npy', valX)\n",
    "np.save(f'{SAVE_FOLDER}/testAllX.npy', testAllX)\n",
    "np.save(f'{SAVE_FOLDER}/testAllY.npy', testAllY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate training process\n",
    "\n",
    "Now that the training process is complete, lets evaluate the average loss of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 936x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set_style(\"darkgrid\")\n",
    "plt.figure(figsize=(13, 6))\n",
    "\n",
    "# summarize history for loss:\n",
    "plt.plot(history.history['loss'], color='blue', label='train')\n",
    "plt.plot(history.history['val_loss'], color='blue', alpha=0.4, label='validation')\n",
    "plt.xlabel('Epoch', fontsize=16)\n",
    "plt.ylabel('Cost', fontsize=16)\n",
    "plt.tick_params(axis='both', which='major', labelsize=14)\n",
    "plt.title('Average Loss During Training', fontsize=18)\n",
    "#plt.xticks(range(0,epochs))\n",
    "plt.legend(loc='upper right', fontsize=16)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
