{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Autoencoder: Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import axis, colorbar, imshow, show, figure, subplot\n",
    "from matplotlib import cm\n",
    "import matplotlib as mpl\n",
    "mpl.rc('axes', labelsize=18)\n",
    "mpl.rc('xtick', labelsize=16)\n",
    "mpl.rc('ytick', labelsize=16)\n",
    "%matplotlib inline\n",
    "\n",
    "import glob\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "import cv2\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "## ----- GPU ------------------------------------\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'    # use of GPU 0 (ERDA) (use before importing torch or tensorflow/keeas)\n",
    "## ----------------------------------------------\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.layers import Conv2D, Conv2DTranspose, Input, Flatten, Dense, Lambda, Reshape\n",
    "from keras.layers import BatchNormalization, ReLU, LeakyReLU\n",
    "from keras.models import Model\n",
    "from keras.losses import binary_crossentropy, mse\n",
    "from keras.activations import relu\n",
    "from keras.callbacks import Callback, ModelCheckpoint\n",
    "from keras import backend as K                         #contains calls for tensor manipulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n",
      "2.4.3\n"
     ]
    }
   ],
   "source": [
    "print (tf.__version__)\n",
    "print (keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NORMAL_TRAIN_AUG:       /home/jovyan/work/Speciale/FoodSorting/generated_dataset/KitKatChunky//DataAugmentation/NormalTrainAugmentations\n",
      "NORMAL_VALIDATION_AUG:  /home/jovyan/work/Speciale/FoodSorting/generated_dataset/KitKatChunky//DataAugmentation/NormalValidationAugmentations\n",
      "NORMAL_TEST_AUG:        /home/jovyan/work/Speciale/FoodSorting/generated_dataset/KitKatChunky//DataAugmentation/NormalTestAugmentations\n",
      "ANOMALY_AUG:            /home/jovyan/work/Speciale/FoodSorting/generated_dataset/KitKatChunky//DataAugmentation/AnomalyAugmentations\n"
     ]
    }
   ],
   "source": [
    "from utils_vae_kitkat_paths import *\n",
    "\n",
    "# Get work directions for augmentated data set:\n",
    "print (\"NORMAL_TRAIN_AUG:      \", NORMAL_TRAIN_AUG)\n",
    "print (\"NORMAL_VALIDATION_AUG: \", NORMAL_VAL_AUG)\n",
    "print (\"NORMAL_TEST_AUG:       \", NORMAL_TEST_AUG)\n",
    "print (\"ANOMALY_AUG:           \", ANOMALY_AUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Which Folder The Models Are Saved In: saved_models/latent2\n"
     ]
    }
   ],
   "source": [
    "# What latent dimension and filter size are we using?:\n",
    "latent_dim = 2\n",
    "filters    = 32\n",
    "\n",
    "# Get work direction for saving files:\n",
    "SAVE_FOLDER = f'saved_models/latent{latent_dim}'\n",
    "print (\"Which Folder The Models Are Saved In:\", SAVE_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] deleting existing files in folder...\n",
      "         done in 0.036 minutes\n"
     ]
    }
   ],
   "source": [
    "# Delete all existing files in folder where models are saved:\n",
    "print(\"[INFO] deleting existing files in folder...\")\n",
    "t0 = time()\n",
    "\n",
    "files = glob.glob(f'{SAVE_FOLDER}/*')\n",
    "for f in files:\n",
    "    os.remove(f)\n",
    "\n",
    "print (\"         done in %0.3f minutes\" % ((time() - t0)/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_VAE_AE import get_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Investigation of Ground Truths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of \"normal\" training images is:     1400\n",
      "Number of \"normal\" validation images is:   134\n",
      "Number of \"normal\" test images is:         266\n",
      "Number of \"anomaly\" images is:             600\n"
     ]
    }
   ],
   "source": [
    "# Glob the directories and get the lists of good and bad images\n",
    "good_train_fns = [f for f in os.listdir(NORMAL_TRAIN_AUG) if not f.startswith('.')]\n",
    "good_val_fns = [f for f in os.listdir(NORMAL_VAL_AUG) if not f.startswith('.')]\n",
    "good_test_fns = [f for f in os.listdir(NORMAL_TEST_AUG) if not f.startswith('.')]\n",
    "bad_fns = [f for f in os.listdir(ANOMALY_AUG) if not f.startswith('.')]\n",
    "\n",
    "print('Number of \"normal\" training images is:     {}'.format(len(good_train_fns)))\n",
    "print('Number of \"normal\" validation images is:   {}'.format(len(good_val_fns)))\n",
    "print('Number of \"normal\" test images is:         {}'.format(len(good_test_fns)))\n",
    "print('Number of \"anomaly\" images is:             {}'.format(len(bad_fns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Procentage of normal samples (ground truth):   75.00 %\n",
      "> Procentage of anomaly samples (ground truth):  25.00 %\n"
     ]
    }
   ],
   "source": [
    "# compute procentage of bad images vs good images:\n",
    "normal_fns = len(good_train_fns) + len(good_val_fns) + len(good_test_fns)\n",
    "anomaly_fns = len(bad_fns)\n",
    "print(f\"> Procentage of normal samples (ground truth):   {(normal_fns / (normal_fns + anomaly_fns)) * 100:.2f} %\")\n",
    "print(f\"> Procentage of anomaly samples (ground truth):  {(anomaly_fns / (normal_fns + anomaly_fns)) * 100:.2f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Normal Data\n",
    "\n",
    "Load normal samples in pre-splitted data sets: train, valdiation and test.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading normal training images...\n",
      "          done\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] loading normal training images...\")\n",
    "trainX = [cv2.imread(file, 0) for file in glob.glob(f\"{NORMAL_TRAIN_AUG}/*.png\")]  # read as grayscale\n",
    "trainX = np.array(trainX)\n",
    "print(\"          done\")\n",
    "\n",
    "# create a corresponding list of labels:\n",
    "trainY = get_labels(trainX, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading normal validation images...\n",
      "          done\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] loading normal validation images...\")\n",
    "x_normal_val = [cv2.imread(file, 0) for file in glob.glob(f\"{NORMAL_VAL_AUG}/*.png\")]  # read as grayscale\n",
    "x_normal_val = np.array(x_normal_val)\n",
    "print(\"          done\")\n",
    "\n",
    "# create a corresponding list of labels:\n",
    "y_normal_val = get_labels(x_normal_val, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading normal testing images...\n",
      "          done\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] loading normal testing images...\")\n",
    "x_normal_test = [cv2.imread(file, 0) for file in glob.glob(f\"{NORMAL_TEST_AUG}/*.png\")]  # read as grayscale\n",
    "x_normal_test = np.array(x_normal_test)\n",
    "print(\"          done\")\n",
    "\n",
    "# create a corresponding list of labels:\n",
    "y_normal_test = get_labels(x_normal_test, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Anomaly Data\n",
    "\n",
    "\n",
    "Load anomaly samples in its singular training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading anomaly images...\n",
      "          done\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] loading anomaly images...\")\n",
    "x_anomaly = [cv2.imread(file, 0) for file in glob.glob(f\"{ANOMALY_AUG}/*.png\")]  # read as grayscale\n",
    "x_anomaly = np.array(x_anomaly)\n",
    "print(\"          done\")\n",
    "\n",
    "# create a corresponding list of labels:\n",
    "y_anomaly = get_labels(x_anomaly, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine normal samples and anomaly samples and split them into training, validation and test sets\n",
    "\n",
    "`label 0` == Normal Samples\n",
    "\n",
    "`label 1` == Anomaly Samples\n",
    "\n",
    "Train and Validation sets only consists of `Normal Data`, aka. `label 0`.\n",
    "\n",
    "Testing sets consists of both  `Normal Data` (aka. `label 0`) and `Anomaly Data` (aka. `label 1`)\n",
    "\n",
    "________________\n",
    "\n",
    "Due to the very sparse original (preprossed) KitKat Chunky data, the validation set will be a mix of normal testing and validation data. The normal testing set will consists of all validation and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine pre-loaded validation and testing set into a new testing set:\n",
    "testvalX = np.vstack((x_normal_val, x_normal_test))\n",
    "testvalY = get_labels(testvalX, 0)\n",
    "\n",
    "# randomly split in order to make new validation set:\n",
    "NoUsageX, valX, NoUsageY, valY = train_test_split(testvalX, testvalY, test_size=0.25, random_state=4345672)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine pre-loaded validation and testing set into a new testing set:\n",
    "testNormalX = np.vstack((x_normal_val, x_normal_test))\n",
    "testNormalY = get_labels(testNormalX, 0)\n",
    "\n",
    "# anomaly class (only used for testing):\n",
    "testAnomalyX, testAnomalyY = x_anomaly, y_anomaly\n",
    "\n",
    "# Combine all testing data in one array:\n",
    "testAllX = np.vstack((testNormalX, testAnomalyX))\n",
    "testAllY = np.hstack((testNormalY, testAnomalyY))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Data Dimensions and Distributions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect data shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:\n",
      " > images: (1400, 128, 128)\n",
      " > labels: (1400,)\n",
      "\n",
      "Validation set:\n",
      " > images: (100, 128, 128)\n",
      " > labels: (100,)\n",
      "\n",
      "Test set:\n",
      " > images: (1000, 128, 128)\n",
      " > labels: (1000,)\n",
      "\t'Normal' test set:\n",
      "\t  > images: (400, 128, 128)\n",
      "\t  > labels: (400,)\n",
      "\t'Anomaly' test set:\n",
      "\t  > images: (600, 128, 128)\n",
      "\t  > labels: (600,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set:\")\n",
    "print(\" > images:\", trainX.shape)\n",
    "print(\" > labels:\", trainY.shape)\n",
    "print (\"\")\n",
    "\n",
    "print(\"Validation set:\")\n",
    "print(\" > images:\", valX.shape)\n",
    "print(\" > labels:\", valY.shape)\n",
    "print (\"\")\n",
    "\n",
    "print(\"Test set:\")\n",
    "print(\" > images:\", testAllX.shape)\n",
    "print(\" > labels:\", testAllY.shape)\n",
    "print(\"\\t'Normal' test set:\")\n",
    "print(\"\\t  > images:\", testNormalX.shape)\n",
    "print(\"\\t  > labels:\", testNormalY.shape)\n",
    "print(\"\\t'Anomaly' test set:\")\n",
    "print(\"\\t  > images:\", testAnomalyX.shape)\n",
    "print(\"\\t  > labels:\", testAnomalyY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procentage of training samples: 73.68 %\n",
      "Procentage of validation samples: 5.26 %\n",
      "Procentage of (only normal) testing samples: 21.05 %\n"
     ]
    }
   ],
   "source": [
    "# compute procentage between training, validation and test data sets (only normal data):\n",
    "print(f\"Procentage of training samples: {(len(trainX) / (len(trainX) + len(valX) + len(testNormalX))) * 100:.2f} %\")\n",
    "print(f\"Procentage of validation samples: {(len(valX) / (len(trainX) + len(valX) + len(testNormalX))) * 100:.2f} %\")\n",
    "print(f\"Procentage of (only normal) testing samples: {(len(testNormalX) / (len(trainX) + len(valX) + len(testNormalX))) * 100:.2f} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procentage of training samples: 56.00 %\n",
      "Procentage of validation samples: 4.00 %\n",
      "Procentage of (total) testing samples: 40.00 %\n"
     ]
    }
   ],
   "source": [
    "# compute procentage between training, validation and test data sets:\n",
    "print(f\"Procentage of training samples: {(len(trainX) / (len(trainX) + len(valX) + len(testAllX))) * 100:.2f} %\")\n",
    "print(f\"Procentage of validation samples: {(len(valX) / (len(trainX) + len(valX) + len(testAllX))) * 100:.2f} %\")\n",
    "print(f\"Procentage of (total) testing samples: {(len(testAllX) / (len(trainX) + len(valX) + len(testAllX))) * 100:.2f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for (un)balanced data\n",
    "\n",
    "In an ideal world the data should be balanced. However in the real world we expect there being around ~$99 \\%$ good samples and only ~$1 \\%$ bad samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels = ['0: normal', '1: anomaly']\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(16,4))\n",
    "ax[0].hist(trainY, align=\"left\"); ax[0].set_title('train', fontsize=18); ax[0].set_xticks([0,1]); ax[0].set_xticklabels(labels); ax[0].tick_params(axis='both', which='major', labelsize=16); ax[0].set_xlim(-0.5, 1.5);\n",
    "ax[1].hist(valY, align=\"left\"); ax[1].set_title('validation', fontsize=18); ax[1].set_xticks([0,1]); ax[1].set_xticklabels(labels); ax[1].tick_params(axis='both', which='major', labelsize=16); ax[1].set_xlim(-0.5, 1.5);\n",
    "ax[2].hist(testAllY, align=\"left\"); ax[2].set_title('test', fontsize=18); ax[2].set_xticks([0,1]); ax[2].set_xticklabels(labels); ax[2].tick_params(axis='both', which='major', labelsize=16); ax[2].set_xlim(-0.5, 1.5);\n",
    "\n",
    "ax[0].set_ylabel(\"Number of images\", fontsize=18)\n",
    "ax[1].set_xlabel(\"Class Labels\", fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procentage of normal test samples: 40.00 %\n",
      "Procentage of total anomaly test samples: 60.00 %\n"
     ]
    }
   ],
   "source": [
    "# compute procentage of normal test images vs total anomaly test images:\n",
    "print(f\"Procentage of normal test samples: {(len(testNormalX) / (len(testNormalX) + len(testAnomalyX))) * 100:.2f} %\")\n",
    "print(f\"Procentage of total anomaly test samples: {(len(testAnomalyX) / (len(testNormalX) + len(testAnomalyX))) * 100:.2f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling\n",
    "\n",
    "Only normalization has been used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data configuration\n",
    "img_width, img_height = trainX.shape[1], trainX.shape[2]      # input image dimensions\n",
    "num_channels = 1                                              # gray-channel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization\n",
    "\n",
    "Neural Networks learns faster with normalized data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize to be in  the [0, 1] range:\n",
    "trainX = trainX.astype('float32') / 255.\n",
    "valX = valX.astype('float32') / 255.\n",
    "testAllX = testAllX.astype('float32') / 255.\n",
    "\n",
    "# reshape data to keras inputs --> (n_samples, img_rows, img_cols, n_channels):\n",
    "trainX = trainX.reshape(trainX.shape[0], img_height, img_width, num_channels)\n",
    "valX = valX.reshape(valX.shape[0], img_height, img_width, num_channels)\n",
    "testAllX = testAllX.reshape(testAllX.shape[0], img_height, img_width, num_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture and Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import architecture of VAE model and its hyperparameters:\n",
    "from J2_VAE_model import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Encoder Part*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_input (InputLayer)      [(None, 128, 128, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 64, 64, 32)   320         encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)         (None, 64, 64, 32)   0           conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 32, 32, 32)   9248        leaky_re_lu[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)       (None, 32, 32, 32)   0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 16, 16, 32)   9248        leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)       (None, 16, 16, 32)   0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 8, 8, 32)     9248        leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)       (None, 8, 8, 32)     0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 4, 4, 32)     9248        leaky_re_lu_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)       (None, 4, 4, 32)     0           conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 512)          0           leaky_re_lu_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "latent (Dense)                  (None, 200)          102600      flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)       (None, 200)          0           latent[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "z_mean (Dense)                  (None, 2)            402         leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "z_log_mean (Dense)              (None, 2)            402         leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "z (Lambda)                      (None, 2)            0           z_mean[0][0]                     \n",
      "                                                                 z_log_mean[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 140,716\n",
      "Trainable params: 140,716\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Decoder Part*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "decoder_input (InputLayer)   [(None, 2)]               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               1536      \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 4, 4, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose (Conv2DTran (None, 8, 8, 32)          9248      \n",
      "_________________________________________________________________\n",
      "re_lu (ReLU)                 (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 16, 16, 32)        9248      \n",
      "_________________________________________________________________\n",
      "re_lu_1 (ReLU)               (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTr (None, 32, 32, 32)        9248      \n",
      "_________________________________________________________________\n",
      "re_lu_2 (ReLU)               (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTr (None, 64, 64, 32)        9248      \n",
      "_________________________________________________________________\n",
      "re_lu_3 (ReLU)               (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_4 (Conv2DTr (None, 128, 128, 32)      9248      \n",
      "_________________________________________________________________\n",
      "re_lu_4 (ReLU)               (None, 128, 128, 32)      0         \n",
      "_________________________________________________________________\n",
      "decoder_output (Conv2DTransp (None, 128, 128, 1)       289       \n",
      "=================================================================\n",
      "Total params: 48,065\n",
      "Trainable params: 48,065\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Total VAE Model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vae\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder_input (InputLayer)   [(None, 128, 128, 1)]     0         \n",
      "_________________________________________________________________\n",
      "encoder (Functional)         [(None, 2), (None, 2), (N 140716    \n",
      "_________________________________________________________________\n",
      "decoder (Functional)         (None, 128, 128, 1)       48065     \n",
      "=================================================================\n",
      "Total params: 188,781\n",
      "Trainable params: 188,781\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vae.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile and train model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Hyperparameters \"\"\"\n",
    "batch_size  = 256\n",
    "epochs      = 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Checkpoint callback options: save model at 1'st epoch (inital model)\n",
    "\n",
    "https://stackoverflow.com/questions/54323960/save-keras-model-at-specific-epochs\n",
    "\"\"\"\n",
    "\n",
    "class CustomSaver(keras.callbacks.Callback):\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        if epoch == 1:\n",
    "            self.model.save_weights(f\"{SAVE_FOLDER}/cp-0001.h5\")\n",
    "            \n",
    "# create and use callback:\n",
    "initial_checkpoint = CustomSaver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Checkpoint callback options: save model at every k'te epochs\n",
    "\"\"\"\n",
    "# Include the epoch in the file name (uses `str.format`):\n",
    "checkpoint_path = \"saved_models/latent2/cp-{epoch:04d}.h5\"\n",
    "\n",
    "# Create a callback that saves the model's weights every k'te epochs:\n",
    "checkpoint = ModelCheckpoint(filepath = checkpoint_path,\n",
    "                             monitor='loss',           # name of the metrics to monitor\n",
    "                             verbose=1, \n",
    "                             #save_best_only=False,     # if False, the best model will not be overridden.\n",
    "                             save_weights_only=True,   # if True, only the weights of the models will be saved.\n",
    "                                                        # if False, the whole models will be saved.\n",
    "                             #mode='auto', \n",
    "                             period=200                  # save after every k'te epochs\n",
    "                            )\n",
    "\n",
    "# Save the weights using the `checkpoint_path` format\n",
    "vae.save_weights(checkpoint_path.format(epoch=0))          # save for zero epoch (inital)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Checkpoint callback options: Early stopping\n",
    "https://www.tensorflow.org/guide/keras/custom_callback\n",
    "\"\"\"\n",
    "\n",
    "class EarlyStoppingAtMinLoss(keras.callbacks.Callback):\n",
    "    \"\"\"Stop training when the loss is at its min, i.e. the loss stops decreasing.\n",
    "\n",
    "  Arguments:\n",
    "      patience: Number of epochs to wait after min has been hit. After this\n",
    "      number of no improvement, training stops.\n",
    "  \"\"\"\n",
    "\n",
    "    def __init__(self, patience=100):\n",
    "        super(EarlyStoppingAtMinLoss, self).__init__()\n",
    "        self.patience = patience\n",
    "        # best_weights to store the weights at which the minimum loss occurs.\n",
    "        self.best_weights = None\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        # The number of epoch it has waited when loss is no longer minimum.\n",
    "        self.wait = 0\n",
    "        # The epoch the training stops at.\n",
    "        self.stopped_epoch = 0\n",
    "        # Initialize the best as infinity.\n",
    "        self.best = np.Inf\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        current = logs.get(\"loss\")\n",
    "        if np.less(current, self.best):\n",
    "            self.best = current\n",
    "            self.wait = 0\n",
    "            # Record the best weights if current results is better (less).\n",
    "            self.best_weights = self.model.get_weights()\n",
    "        else:\n",
    "            self.wait += 1\n",
    "            if self.wait >= self.patience:\n",
    "                self.stopped_epoch = epoch\n",
    "                self.model.stop_training = True\n",
    "                print(\"Restoring model weights from the end of the best epoch.\")\n",
    "                self.model.set_weights(self.best_weights)\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        if self.stopped_epoch > 0:\n",
    "            print(\"Epoch %05d: early stopping\" % (self.stopped_epoch + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Create and Compile Model\"\"\"\n",
    "# import architecture of VAE model and its hyperparameters. learning rate choosen from graph above.\n",
    "vae = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      "6/6 [==============================] - 1s 208ms/step - loss: 3529.5713 - val_loss: 3448.3372\n",
      "Epoch 2/2000\n",
      "6/6 [==============================] - 1s 105ms/step - loss: 3288.2820 - val_loss: 2970.1001\n",
      "Epoch 3/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 2405.3811 - val_loss: 1575.9888\n",
      "Epoch 4/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 1155.5129 - val_loss: 959.1853\n",
      "Epoch 5/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 859.2031 - val_loss: 906.3766\n",
      "Epoch 6/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 828.5538 - val_loss: 898.9529\n",
      "Epoch 7/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 818.6783 - val_loss: 876.1149\n",
      "Epoch 8/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 791.7833 - val_loss: 831.0687\n",
      "Epoch 9/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 733.5161 - val_loss: 764.2202\n",
      "Epoch 10/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 667.0248 - val_loss: 693.2255\n",
      "Epoch 11/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 612.9496 - val_loss: 645.2781\n",
      "Epoch 12/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 582.2083 - val_loss: 627.7177\n",
      "Epoch 13/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 573.8572 - val_loss: 620.4606\n",
      "Epoch 14/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 565.7864 - val_loss: 610.9565\n",
      "Epoch 15/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 556.4178 - val_loss: 603.5206\n",
      "Epoch 16/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 549.8777 - val_loss: 599.3296\n",
      "Epoch 17/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 546.7515 - val_loss: 595.4003\n",
      "Epoch 18/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 541.5275 - val_loss: 584.9836\n",
      "Epoch 19/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 530.7519 - val_loss: 568.4020\n",
      "Epoch 20/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 508.0038 - val_loss: 540.2921\n",
      "Epoch 21/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 480.5194 - val_loss: 507.6401\n",
      "Epoch 22/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 452.5013 - val_loss: 486.0593\n",
      "Epoch 23/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 434.3861 - val_loss: 467.4501\n",
      "Epoch 24/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 420.4263 - val_loss: 454.2140\n",
      "Epoch 25/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 408.8460 - val_loss: 443.2244\n",
      "Epoch 26/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 400.7427 - val_loss: 434.4237\n",
      "Epoch 27/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 391.6857 - val_loss: 423.1421\n",
      "Epoch 28/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 380.3196 - val_loss: 415.8459\n",
      "Epoch 29/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 372.9449 - val_loss: 404.1550\n",
      "Epoch 30/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 365.9011 - val_loss: 398.0268\n",
      "Epoch 31/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 358.1501 - val_loss: 388.9086\n",
      "Epoch 32/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 351.0665 - val_loss: 382.4480\n",
      "Epoch 33/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 340.9162 - val_loss: 372.7062\n",
      "Epoch 34/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 334.3441 - val_loss: 364.5260\n",
      "Epoch 35/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 327.7757 - val_loss: 350.9408\n",
      "Epoch 36/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 313.4784 - val_loss: 336.5623\n",
      "Epoch 37/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 299.2966 - val_loss: 324.4757\n",
      "Epoch 38/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 283.4389 - val_loss: 305.3244\n",
      "Epoch 39/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 269.4132 - val_loss: 297.9368\n",
      "Epoch 40/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 259.8239 - val_loss: 284.3210\n",
      "Epoch 41/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 251.0851 - val_loss: 272.1998\n",
      "Epoch 42/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 245.7476 - val_loss: 264.7449\n",
      "Epoch 43/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 237.9847 - val_loss: 255.2115\n",
      "Epoch 44/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 232.6728 - val_loss: 253.0204\n",
      "Epoch 45/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 229.7435 - val_loss: 245.5584\n",
      "Epoch 46/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 225.4240 - val_loss: 244.6907\n",
      "Epoch 47/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 223.2393 - val_loss: 239.8593\n",
      "Epoch 48/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 219.2128 - val_loss: 234.2056\n",
      "Epoch 49/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 215.9383 - val_loss: 232.7342\n",
      "Epoch 50/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 213.7954 - val_loss: 231.1683\n",
      "Epoch 51/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 209.3884 - val_loss: 228.9160\n",
      "Epoch 52/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 208.8832 - val_loss: 225.5257\n",
      "Epoch 53/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 205.9304 - val_loss: 225.2277\n",
      "Epoch 54/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 203.2698 - val_loss: 220.5547\n",
      "Epoch 55/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 201.4351 - val_loss: 220.4966\n",
      "Epoch 56/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 198.5162 - val_loss: 214.7912\n",
      "Epoch 57/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 197.3216 - val_loss: 215.5645\n",
      "Epoch 58/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 194.7206 - val_loss: 211.7318\n",
      "Epoch 59/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 193.0408 - val_loss: 209.7934\n",
      "Epoch 60/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 189.5493 - val_loss: 210.2755\n",
      "Epoch 61/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 191.1142 - val_loss: 209.4026\n",
      "Epoch 62/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 189.0975 - val_loss: 208.8740\n",
      "Epoch 63/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 188.2397 - val_loss: 202.5463\n",
      "Epoch 64/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 184.2173 - val_loss: 203.7562\n",
      "Epoch 65/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 182.8038 - val_loss: 203.3790\n",
      "Epoch 66/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 181.8622 - val_loss: 203.2580\n",
      "Epoch 67/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 179.3357 - val_loss: 198.3414\n",
      "Epoch 68/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 176.1567 - val_loss: 194.2260\n",
      "Epoch 69/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 174.6768 - val_loss: 193.3731\n",
      "Epoch 70/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 173.0006 - val_loss: 192.1682\n",
      "Epoch 71/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 170.8819 - val_loss: 189.0360\n",
      "Epoch 72/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 169.1852 - val_loss: 189.4583\n",
      "Epoch 73/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 170.4705 - val_loss: 189.6368\n",
      "Epoch 74/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 174.6431 - val_loss: 200.4690\n",
      "Epoch 75/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 175.2462 - val_loss: 192.5048\n",
      "Epoch 76/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 171.5250 - val_loss: 189.7021\n",
      "Epoch 77/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 167.7615 - val_loss: 185.8650\n",
      "Epoch 78/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 164.2238 - val_loss: 182.3538\n",
      "Epoch 79/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 162.4602 - val_loss: 181.9587\n",
      "Epoch 80/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 161.0017 - val_loss: 178.8454\n",
      "Epoch 81/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 157.8142 - val_loss: 178.8613\n",
      "Epoch 82/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 158.4603 - val_loss: 177.4533\n",
      "Epoch 83/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 156.9428 - val_loss: 181.0621\n",
      "Epoch 84/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 154.9376 - val_loss: 178.2873\n",
      "Epoch 85/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 156.1729 - val_loss: 175.4643\n",
      "Epoch 86/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 152.8153 - val_loss: 173.5336\n",
      "Epoch 87/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 152.3077 - val_loss: 170.6588\n",
      "Epoch 88/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 149.3130 - val_loss: 171.2442\n",
      "Epoch 89/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 148.8003 - val_loss: 169.4021\n",
      "Epoch 90/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 146.3171 - val_loss: 167.1495\n",
      "Epoch 91/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 146.8862 - val_loss: 168.4266\n",
      "Epoch 92/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 146.5143 - val_loss: 167.2168\n",
      "Epoch 93/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 143.0601 - val_loss: 165.9516\n",
      "Epoch 94/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 144.5434 - val_loss: 168.8893\n",
      "Epoch 95/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 142.3573 - val_loss: 165.3720\n",
      "Epoch 96/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 143.2472 - val_loss: 164.8391\n",
      "Epoch 97/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 141.2475 - val_loss: 163.6802\n",
      "Epoch 98/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 140.3451 - val_loss: 161.6894\n",
      "Epoch 99/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 137.3110 - val_loss: 159.6111\n",
      "Epoch 100/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 137.1434 - val_loss: 162.7399\n",
      "Epoch 101/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 137.8527 - val_loss: 157.8346\n",
      "Epoch 102/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 139.9326 - val_loss: 159.8408\n",
      "Epoch 103/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 141.3482 - val_loss: 162.9077\n",
      "Epoch 104/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 138.9961 - val_loss: 157.2316\n",
      "Epoch 105/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 137.1272 - val_loss: 160.0051\n",
      "Epoch 106/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 135.8183 - val_loss: 155.8366\n",
      "Epoch 107/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 132.5984 - val_loss: 153.4023\n",
      "Epoch 108/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 131.1917 - val_loss: 153.2230\n",
      "Epoch 109/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 129.2041 - val_loss: 152.0991\n",
      "Epoch 110/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 128.3084 - val_loss: 149.0681\n",
      "Epoch 111/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 128.4048 - val_loss: 150.0704\n",
      "Epoch 112/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 127.6186 - val_loss: 150.2341\n",
      "Epoch 113/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 126.5264 - val_loss: 149.1821\n",
      "Epoch 114/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 125.6504 - val_loss: 149.9332\n",
      "Epoch 115/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 125.4214 - val_loss: 146.5401\n",
      "Epoch 116/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 124.6004 - val_loss: 147.8073\n",
      "Epoch 117/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 123.6450 - val_loss: 149.8508\n",
      "Epoch 118/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 124.2251 - val_loss: 146.7697\n",
      "Epoch 119/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 122.2661 - val_loss: 147.9081\n",
      "Epoch 120/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 121.2748 - val_loss: 146.6389\n",
      "Epoch 121/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 122.3928 - val_loss: 146.9477\n",
      "Epoch 122/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 121.0662 - val_loss: 145.2046\n",
      "Epoch 123/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 120.7925 - val_loss: 145.4990\n",
      "Epoch 124/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 120.9085 - val_loss: 148.8125\n",
      "Epoch 125/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 121.8327 - val_loss: 144.5819\n",
      "Epoch 126/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 122.1184 - val_loss: 145.9253\n",
      "Epoch 127/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 121.2034 - val_loss: 147.1278\n",
      "Epoch 128/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 122.3932 - val_loss: 147.4806\n",
      "Epoch 129/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 120.6358 - val_loss: 143.1604\n",
      "Epoch 130/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 117.2134 - val_loss: 149.4876\n",
      "Epoch 131/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 118.0225 - val_loss: 140.8887\n",
      "Epoch 132/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 115.2691 - val_loss: 140.1610\n",
      "Epoch 133/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 115.4464 - val_loss: 141.2866\n",
      "Epoch 134/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 116.4765 - val_loss: 140.8086\n",
      "Epoch 135/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 114.9804 - val_loss: 139.2168\n",
      "Epoch 136/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 114.7528 - val_loss: 139.4936\n",
      "Epoch 137/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 116.2280 - val_loss: 139.4774\n",
      "Epoch 138/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 114.2790 - val_loss: 137.2364\n",
      "Epoch 139/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 111.9990 - val_loss: 137.7979\n",
      "Epoch 140/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 111.9196 - val_loss: 139.6434\n",
      "Epoch 141/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 111.6777 - val_loss: 136.5553\n",
      "Epoch 142/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 110.9092 - val_loss: 137.0473\n",
      "Epoch 143/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 110.6067 - val_loss: 137.4935\n",
      "Epoch 144/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 110.6904 - val_loss: 137.9638\n",
      "Epoch 145/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 110.1637 - val_loss: 135.9705\n",
      "Epoch 146/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 110.3503 - val_loss: 136.5515\n",
      "Epoch 147/2000\n",
      "6/6 [==============================] - 1s 105ms/step - loss: 109.7878 - val_loss: 139.3924\n",
      "Epoch 148/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 109.7186 - val_loss: 135.1832\n",
      "Epoch 149/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 109.6340 - val_loss: 135.1577\n",
      "Epoch 150/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 108.4071 - val_loss: 137.4612\n",
      "Epoch 151/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 109.1511 - val_loss: 137.8535\n",
      "Epoch 152/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 108.9578 - val_loss: 133.5387\n",
      "Epoch 153/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 107.8372 - val_loss: 141.0866\n",
      "Epoch 154/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 108.0551 - val_loss: 134.4583\n",
      "Epoch 155/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 106.5751 - val_loss: 135.1422\n",
      "Epoch 156/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 106.2105 - val_loss: 131.6197\n",
      "Epoch 157/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 105.0013 - val_loss: 137.2157\n",
      "Epoch 158/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 106.0156 - val_loss: 134.3731\n",
      "Epoch 159/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 105.6746 - val_loss: 130.1555\n",
      "Epoch 160/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 104.6668 - val_loss: 132.0007\n",
      "Epoch 161/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 103.9344 - val_loss: 130.7548\n",
      "Epoch 162/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 102.7661 - val_loss: 129.7577\n",
      "Epoch 163/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 102.8864 - val_loss: 129.7444\n",
      "Epoch 164/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 102.4357 - val_loss: 130.1304\n",
      "Epoch 165/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 101.3272 - val_loss: 131.5468\n",
      "Epoch 166/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 101.4018 - val_loss: 131.9252\n",
      "Epoch 167/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 102.0549 - val_loss: 132.8945\n",
      "Epoch 168/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 103.4895 - val_loss: 133.2065\n",
      "Epoch 169/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 103.2337 - val_loss: 131.6980\n",
      "Epoch 170/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 101.9886 - val_loss: 127.6589\n",
      "Epoch 171/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 100.8588 - val_loss: 130.3712\n",
      "Epoch 172/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 100.3288 - val_loss: 129.9392\n",
      "Epoch 173/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 100.7644 - val_loss: 126.9446\n",
      "Epoch 174/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 99.9134 - val_loss: 128.2388\n",
      "Epoch 175/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 99.3393 - val_loss: 128.1375\n",
      "Epoch 176/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 98.6787 - val_loss: 130.6973\n",
      "Epoch 177/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 99.6441 - val_loss: 130.4565\n",
      "Epoch 178/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 100.3064 - val_loss: 129.9535\n",
      "Epoch 179/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 99.1968 - val_loss: 127.2207\n",
      "Epoch 180/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 98.4404 - val_loss: 127.0621\n",
      "Epoch 181/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 100.3497 - val_loss: 126.6575\n",
      "Epoch 182/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 98.8477 - val_loss: 125.8936\n",
      "Epoch 183/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 97.8904 - val_loss: 128.0788\n",
      "Epoch 184/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 97.7534 - val_loss: 125.9807\n",
      "Epoch 185/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 96.6257 - val_loss: 126.0116\n",
      "Epoch 186/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 97.6224 - val_loss: 126.2310\n",
      "Epoch 187/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 97.2312 - val_loss: 125.8133\n",
      "Epoch 188/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 97.6080 - val_loss: 126.1762\n",
      "Epoch 189/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 97.0379 - val_loss: 127.9666\n",
      "Epoch 190/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 95.7972 - val_loss: 124.8790\n",
      "Epoch 191/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 94.4096 - val_loss: 122.0739\n",
      "Epoch 192/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 93.6181 - val_loss: 126.3465\n",
      "Epoch 193/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 95.2188 - val_loss: 122.4100\n",
      "Epoch 194/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 93.9769 - val_loss: 124.6164\n",
      "Epoch 195/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 94.1829 - val_loss: 124.3384\n",
      "Epoch 196/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 93.8143 - val_loss: 121.6003\n",
      "Epoch 197/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 93.4712 - val_loss: 123.5114\n",
      "Epoch 198/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 93.1143 - val_loss: 122.4382\n",
      "Epoch 199/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 92.7988 - val_loss: 122.7849\n",
      "Epoch 200/2000\n",
      "6/6 [==============================] - ETA: 0s - loss: 94.2246\n",
      "Epoch 00200: saving model to saved_models/latent2/cp-0200.h5\n",
      "6/6 [==============================] - 1s 143ms/step - loss: 94.2246 - val_loss: 123.9962\n",
      "Epoch 201/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 92.4135 - val_loss: 119.8286\n",
      "Epoch 202/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 92.4785 - val_loss: 121.7815\n",
      "Epoch 203/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 91.8663 - val_loss: 121.3965\n",
      "Epoch 204/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 92.1556 - val_loss: 122.8068\n",
      "Epoch 205/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 92.8426 - val_loss: 124.2625\n",
      "Epoch 206/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 92.9781 - val_loss: 122.1735\n",
      "Epoch 207/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 92.7867 - val_loss: 122.0116\n",
      "Epoch 208/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 91.5861 - val_loss: 118.7952\n",
      "Epoch 209/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 90.9537 - val_loss: 118.7231\n",
      "Epoch 210/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 90.7869 - val_loss: 117.2577\n",
      "Epoch 211/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 91.3435 - val_loss: 119.6998\n",
      "Epoch 212/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 91.3934 - val_loss: 120.6452\n",
      "Epoch 213/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 91.0708 - val_loss: 121.0934\n",
      "Epoch 214/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 90.2568 - val_loss: 119.4064\n",
      "Epoch 215/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 90.4420 - val_loss: 122.1153\n",
      "Epoch 216/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 90.6762 - val_loss: 121.9325\n",
      "Epoch 217/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 90.5489 - val_loss: 118.5498\n",
      "Epoch 218/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 91.2757 - val_loss: 120.1720\n",
      "Epoch 219/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 91.6835 - val_loss: 122.2525\n",
      "Epoch 220/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 90.1265 - val_loss: 118.2304\n",
      "Epoch 221/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 89.2222 - val_loss: 119.4339\n",
      "Epoch 222/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 89.6832 - val_loss: 119.1601\n",
      "Epoch 223/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 88.8015 - val_loss: 119.2744\n",
      "Epoch 224/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 87.6281 - val_loss: 115.1667\n",
      "Epoch 225/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 87.6828 - val_loss: 117.7561\n",
      "Epoch 226/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 87.3248 - val_loss: 118.8363\n",
      "Epoch 227/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 87.2023 - val_loss: 115.3929\n",
      "Epoch 228/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 86.7906 - val_loss: 118.8987\n",
      "Epoch 229/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 87.6054 - val_loss: 119.9436\n",
      "Epoch 230/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 88.3462 - val_loss: 117.9910\n",
      "Epoch 231/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 87.2743 - val_loss: 118.3277\n",
      "Epoch 232/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 86.3062 - val_loss: 115.8752\n",
      "Epoch 233/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 86.8745 - val_loss: 116.9070\n",
      "Epoch 234/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 85.8343 - val_loss: 117.4929\n",
      "Epoch 235/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 86.1914 - val_loss: 114.9477\n",
      "Epoch 236/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 87.1343 - val_loss: 116.2792\n",
      "Epoch 237/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 86.4000 - val_loss: 116.7945\n",
      "Epoch 238/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 85.7981 - val_loss: 113.8429\n",
      "Epoch 239/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 84.7500 - val_loss: 115.2643\n",
      "Epoch 240/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 84.9612 - val_loss: 113.7112\n",
      "Epoch 241/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 84.4339 - val_loss: 113.8009\n",
      "Epoch 242/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 83.9374 - val_loss: 115.0208\n",
      "Epoch 243/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 84.4332 - val_loss: 112.2245\n",
      "Epoch 244/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 84.0095 - val_loss: 114.1159\n",
      "Epoch 245/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 83.7067 - val_loss: 112.8002\n",
      "Epoch 246/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 84.1542 - val_loss: 112.9344\n",
      "Epoch 247/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 83.9312 - val_loss: 118.7992\n",
      "Epoch 248/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 85.2455 - val_loss: 112.2550\n",
      "Epoch 249/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 84.2173 - val_loss: 116.4579\n",
      "Epoch 250/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 85.5931 - val_loss: 118.8580\n",
      "Epoch 251/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 84.8331 - val_loss: 113.0145\n",
      "Epoch 252/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 82.9536 - val_loss: 113.4840\n",
      "Epoch 253/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 82.3235 - val_loss: 110.4336\n",
      "Epoch 254/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 82.6252 - val_loss: 113.1554\n",
      "Epoch 255/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 81.6264 - val_loss: 110.7764\n",
      "Epoch 256/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 81.2547 - val_loss: 111.8322\n",
      "Epoch 257/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 81.0542 - val_loss: 110.0474\n",
      "Epoch 258/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 81.0564 - val_loss: 113.4354\n",
      "Epoch 259/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 82.7318 - val_loss: 113.7147\n",
      "Epoch 260/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 83.2549 - val_loss: 113.5306\n",
      "Epoch 261/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 82.7574 - val_loss: 112.6732\n",
      "Epoch 262/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 82.1124 - val_loss: 110.6665\n",
      "Epoch 263/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 81.2825 - val_loss: 110.0343\n",
      "Epoch 264/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 80.0821 - val_loss: 110.6250\n",
      "Epoch 265/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 80.5673 - val_loss: 109.2726\n",
      "Epoch 266/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 80.5519 - val_loss: 111.1788\n",
      "Epoch 267/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 79.5809 - val_loss: 109.0221\n",
      "Epoch 268/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 80.7638 - val_loss: 111.5821\n",
      "Epoch 269/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 80.4311 - val_loss: 109.6264\n",
      "Epoch 270/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 80.2795 - val_loss: 110.7317\n",
      "Epoch 271/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 80.7677 - val_loss: 108.5415\n",
      "Epoch 272/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 80.3885 - val_loss: 108.5695\n",
      "Epoch 273/2000\n",
      "6/6 [==============================] - 1s 105ms/step - loss: 79.1489 - val_loss: 108.4391\n",
      "Epoch 274/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 79.4095 - val_loss: 109.2631\n",
      "Epoch 275/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 79.0016 - val_loss: 107.7484\n",
      "Epoch 276/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 78.8718 - val_loss: 110.6821\n",
      "Epoch 277/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 79.0152 - val_loss: 108.2055\n",
      "Epoch 278/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 79.0380 - val_loss: 108.6401\n",
      "Epoch 279/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 79.0506 - val_loss: 107.5453\n",
      "Epoch 280/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 78.3803 - val_loss: 110.9169\n",
      "Epoch 281/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 79.4461 - val_loss: 107.9791\n",
      "Epoch 282/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 79.8734 - val_loss: 108.6703\n",
      "Epoch 283/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 78.9122 - val_loss: 105.6652\n",
      "Epoch 284/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 77.0945 - val_loss: 106.9641\n",
      "Epoch 285/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 77.6125 - val_loss: 110.0192\n",
      "Epoch 286/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 77.2633 - val_loss: 109.0033\n",
      "Epoch 287/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 78.0178 - val_loss: 108.2201\n",
      "Epoch 288/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 77.6425 - val_loss: 106.8052\n",
      "Epoch 289/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 77.2678 - val_loss: 108.4944\n",
      "Epoch 290/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 77.4824 - val_loss: 106.0754\n",
      "Epoch 291/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 76.9372 - val_loss: 107.0581\n",
      "Epoch 292/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 77.2920 - val_loss: 107.6556\n",
      "Epoch 293/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 76.8438 - val_loss: 104.1178\n",
      "Epoch 294/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 78.3521 - val_loss: 106.1837\n",
      "Epoch 295/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 77.4578 - val_loss: 106.6980\n",
      "Epoch 296/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 78.3831 - val_loss: 104.4715\n",
      "Epoch 297/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 77.8747 - val_loss: 110.1515\n",
      "Epoch 298/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 78.1126 - val_loss: 106.3439\n",
      "Epoch 299/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 76.4289 - val_loss: 105.0401\n",
      "Epoch 300/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 75.6494 - val_loss: 104.5219\n",
      "Epoch 301/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 75.8808 - val_loss: 104.6040\n",
      "Epoch 302/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 76.3532 - val_loss: 105.9551\n",
      "Epoch 303/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 76.1726 - val_loss: 107.1590\n",
      "Epoch 304/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 75.9702 - val_loss: 106.6055\n",
      "Epoch 305/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 77.1673 - val_loss: 105.0596\n",
      "Epoch 306/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 77.2570 - val_loss: 108.8448\n",
      "Epoch 307/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 77.9566 - val_loss: 105.9899\n",
      "Epoch 308/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 77.2753 - val_loss: 106.9003\n",
      "Epoch 309/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 77.3852 - val_loss: 105.2099\n",
      "Epoch 310/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 75.5717 - val_loss: 105.6307\n",
      "Epoch 311/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 75.1314 - val_loss: 104.9828\n",
      "Epoch 312/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 75.4759 - val_loss: 104.7695\n",
      "Epoch 313/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 75.1603 - val_loss: 105.7859\n",
      "Epoch 314/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 74.5119 - val_loss: 102.9199\n",
      "Epoch 315/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 74.1578 - val_loss: 104.0522\n",
      "Epoch 316/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 74.1032 - val_loss: 104.6035\n",
      "Epoch 317/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 74.1759 - val_loss: 102.7343\n",
      "Epoch 318/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 74.2338 - val_loss: 105.3188\n",
      "Epoch 319/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 74.4554 - val_loss: 104.4357\n",
      "Epoch 320/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 75.6183 - val_loss: 103.5311\n",
      "Epoch 321/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 74.6438 - val_loss: 107.4180\n",
      "Epoch 322/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 76.6501 - val_loss: 103.4812\n",
      "Epoch 323/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 74.4173 - val_loss: 103.7040\n",
      "Epoch 324/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 73.8717 - val_loss: 101.4909\n",
      "Epoch 325/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 74.5153 - val_loss: 105.5859\n",
      "Epoch 326/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 74.1599 - val_loss: 100.3177\n",
      "Epoch 327/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 75.0587 - val_loss: 103.9417\n",
      "Epoch 328/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 75.6435 - val_loss: 104.9728\n",
      "Epoch 329/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 75.5826 - val_loss: 103.5112\n",
      "Epoch 330/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 73.8502 - val_loss: 103.0640\n",
      "Epoch 331/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 72.9732 - val_loss: 99.7295\n",
      "Epoch 332/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 72.5769 - val_loss: 102.4965\n",
      "Epoch 333/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 72.6510 - val_loss: 100.3855\n",
      "Epoch 334/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 72.5041 - val_loss: 100.1005\n",
      "Epoch 335/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 73.3480 - val_loss: 102.4005\n",
      "Epoch 336/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 72.7669 - val_loss: 101.6193\n",
      "Epoch 337/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 73.0642 - val_loss: 105.6385\n",
      "Epoch 338/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 73.4970 - val_loss: 101.8535\n",
      "Epoch 339/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 71.6062 - val_loss: 99.4812\n",
      "Epoch 340/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 71.7005 - val_loss: 100.8498\n",
      "Epoch 341/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 71.9759 - val_loss: 101.2015\n",
      "Epoch 342/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 72.5251 - val_loss: 101.1359\n",
      "Epoch 343/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 72.5074 - val_loss: 103.5116\n",
      "Epoch 344/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 71.6700 - val_loss: 101.9503\n",
      "Epoch 345/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 71.9377 - val_loss: 101.4738\n",
      "Epoch 346/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 71.7191 - val_loss: 100.3960\n",
      "Epoch 347/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 71.0480 - val_loss: 101.6951\n",
      "Epoch 348/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 72.0753 - val_loss: 100.6347\n",
      "Epoch 349/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 71.6236 - val_loss: 100.4444\n",
      "Epoch 350/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 71.8876 - val_loss: 100.7481\n",
      "Epoch 351/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 71.7766 - val_loss: 104.1744\n",
      "Epoch 352/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 72.1581 - val_loss: 99.3983\n",
      "Epoch 353/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 70.6638 - val_loss: 99.1123\n",
      "Epoch 354/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 70.3396 - val_loss: 99.1551\n",
      "Epoch 355/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 69.7525 - val_loss: 98.5694\n",
      "Epoch 356/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 69.6734 - val_loss: 100.9051\n",
      "Epoch 357/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 69.5831 - val_loss: 98.2098\n",
      "Epoch 358/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 69.9492 - val_loss: 100.3603\n",
      "Epoch 359/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 70.3962 - val_loss: 100.3790\n",
      "Epoch 360/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 71.1598 - val_loss: 101.0730\n",
      "Epoch 361/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 70.1183 - val_loss: 97.7854\n",
      "Epoch 362/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 70.6725 - val_loss: 101.2175\n",
      "Epoch 363/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 70.3932 - val_loss: 100.9873\n",
      "Epoch 364/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 70.6851 - val_loss: 99.7680\n",
      "Epoch 365/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 69.8451 - val_loss: 99.8238\n",
      "Epoch 366/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 70.2871 - val_loss: 98.2466\n",
      "Epoch 367/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 69.6082 - val_loss: 99.9888\n",
      "Epoch 368/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 69.3963 - val_loss: 98.9026\n",
      "Epoch 369/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 70.0252 - val_loss: 98.9904\n",
      "Epoch 370/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 70.3751 - val_loss: 101.6416\n",
      "Epoch 371/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 69.7360 - val_loss: 99.5810\n",
      "Epoch 372/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 70.2318 - val_loss: 102.2328\n",
      "Epoch 373/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 69.7917 - val_loss: 99.7925\n",
      "Epoch 374/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 69.7296 - val_loss: 97.5483\n",
      "Epoch 375/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 70.1795 - val_loss: 97.9182\n",
      "Epoch 376/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 70.4486 - val_loss: 101.9796\n",
      "Epoch 377/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 71.5123 - val_loss: 100.3396\n",
      "Epoch 378/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 71.1671 - val_loss: 97.8472\n",
      "Epoch 379/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 69.6904 - val_loss: 100.9680\n",
      "Epoch 380/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 70.3481 - val_loss: 98.9103\n",
      "Epoch 381/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 70.8386 - val_loss: 105.3099\n",
      "Epoch 382/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 72.3971 - val_loss: 104.4105\n",
      "Epoch 383/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 72.2855 - val_loss: 103.2360\n",
      "Epoch 384/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 70.9993 - val_loss: 97.6553\n",
      "Epoch 385/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 70.2574 - val_loss: 103.7452\n",
      "Epoch 386/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 72.7343 - val_loss: 98.2665\n",
      "Epoch 387/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 71.5097 - val_loss: 99.3556\n",
      "Epoch 388/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 70.4744 - val_loss: 99.0777\n",
      "Epoch 389/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 69.2032 - val_loss: 101.0416\n",
      "Epoch 390/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 68.7346 - val_loss: 96.3798\n",
      "Epoch 391/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 68.1389 - val_loss: 96.0998\n",
      "Epoch 392/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 67.8245 - val_loss: 97.3146\n",
      "Epoch 393/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 67.8790 - val_loss: 98.5606\n",
      "Epoch 394/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 68.0763 - val_loss: 101.1291\n",
      "Epoch 395/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 68.5539 - val_loss: 98.2063\n",
      "Epoch 396/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 67.8617 - val_loss: 96.7794\n",
      "Epoch 397/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 67.4330 - val_loss: 101.2445\n",
      "Epoch 398/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 67.5200 - val_loss: 96.1893\n",
      "Epoch 399/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 68.1880 - val_loss: 97.5515\n",
      "Epoch 400/2000\n",
      "6/6 [==============================] - ETA: 0s - loss: 67.6892\n",
      "Epoch 00400: saving model to saved_models/latent2/cp-0400.h5\n",
      "6/6 [==============================] - 1s 157ms/step - loss: 67.6892 - val_loss: 97.6053\n",
      "Epoch 401/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 67.7280 - val_loss: 97.3492\n",
      "Epoch 402/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 67.3522 - val_loss: 97.3350\n",
      "Epoch 403/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 67.8833 - val_loss: 98.0246\n",
      "Epoch 404/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 67.1511 - val_loss: 95.5173\n",
      "Epoch 405/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 67.9935 - val_loss: 95.5834\n",
      "Epoch 406/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 67.3037 - val_loss: 98.1102\n",
      "Epoch 407/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 68.3044 - val_loss: 96.2337\n",
      "Epoch 408/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 67.1510 - val_loss: 95.6302\n",
      "Epoch 409/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 66.4740 - val_loss: 94.3022\n",
      "Epoch 410/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 66.5286 - val_loss: 94.8658\n",
      "Epoch 411/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 66.1873 - val_loss: 98.3979\n",
      "Epoch 412/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 66.0753 - val_loss: 94.6809\n",
      "Epoch 413/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 66.6193 - val_loss: 95.4292\n",
      "Epoch 414/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 66.8097 - val_loss: 97.6997\n",
      "Epoch 415/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 67.7817 - val_loss: 100.2150\n",
      "Epoch 416/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 67.4927 - val_loss: 95.7508\n",
      "Epoch 417/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 68.0138 - val_loss: 95.1648\n",
      "Epoch 418/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 67.3533 - val_loss: 96.2600\n",
      "Epoch 419/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 66.3680 - val_loss: 96.4453\n",
      "Epoch 420/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 67.1539 - val_loss: 94.1086\n",
      "Epoch 421/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 66.7467 - val_loss: 98.3070\n",
      "Epoch 422/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 68.1688 - val_loss: 96.6928\n",
      "Epoch 423/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 66.4640 - val_loss: 93.5780\n",
      "Epoch 424/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 66.6215 - val_loss: 96.1502\n",
      "Epoch 425/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 67.2296 - val_loss: 97.6911\n",
      "Epoch 426/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 66.9170 - val_loss: 94.6101\n",
      "Epoch 427/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 66.4458 - val_loss: 95.9508\n",
      "Epoch 428/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 66.0076 - val_loss: 98.1042\n",
      "Epoch 429/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 66.3919 - val_loss: 97.4044\n",
      "Epoch 430/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 66.3514 - val_loss: 95.8277\n",
      "Epoch 431/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 65.4547 - val_loss: 96.9041\n",
      "Epoch 432/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 65.3613 - val_loss: 98.2719\n",
      "Epoch 433/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 66.2127 - val_loss: 95.6003\n",
      "Epoch 434/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 65.1229 - val_loss: 95.4659\n",
      "Epoch 435/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 65.0104 - val_loss: 92.9205\n",
      "Epoch 436/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 65.4943 - val_loss: 93.3842\n",
      "Epoch 437/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 64.7774 - val_loss: 97.3851\n",
      "Epoch 438/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 65.2861 - val_loss: 94.8765\n",
      "Epoch 439/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 65.4332 - val_loss: 97.8456\n",
      "Epoch 440/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 66.0053 - val_loss: 95.6888\n",
      "Epoch 441/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 66.1173 - val_loss: 95.0779\n",
      "Epoch 442/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 65.4905 - val_loss: 97.8145\n",
      "Epoch 443/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 65.0636 - val_loss: 93.8525\n",
      "Epoch 444/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 64.2531 - val_loss: 93.7936\n",
      "Epoch 445/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 65.0777 - val_loss: 95.1873\n",
      "Epoch 446/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 64.9311 - val_loss: 94.8686\n",
      "Epoch 447/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 64.8526 - val_loss: 95.8972\n",
      "Epoch 448/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 65.4239 - val_loss: 96.3200\n",
      "Epoch 449/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 65.0916 - val_loss: 97.5641\n",
      "Epoch 450/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 64.5468 - val_loss: 91.7808\n",
      "Epoch 451/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 64.8655 - val_loss: 94.8119\n",
      "Epoch 452/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 64.8105 - val_loss: 93.9252\n",
      "Epoch 453/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 65.0064 - val_loss: 94.6568\n",
      "Epoch 454/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 65.3981 - val_loss: 95.7070\n",
      "Epoch 455/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 64.1032 - val_loss: 99.0932\n",
      "Epoch 456/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 65.1019 - val_loss: 92.3722\n",
      "Epoch 457/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 64.3820 - val_loss: 94.3871\n",
      "Epoch 458/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 64.0838 - val_loss: 93.3843\n",
      "Epoch 459/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 64.6721 - val_loss: 96.8257\n",
      "Epoch 460/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 64.2756 - val_loss: 95.3901\n",
      "Epoch 461/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 64.6022 - val_loss: 96.9834\n",
      "Epoch 462/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 63.8832 - val_loss: 92.9554\n",
      "Epoch 463/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 64.0582 - val_loss: 96.9682\n",
      "Epoch 464/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 64.3183 - val_loss: 93.4370\n",
      "Epoch 465/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 63.7186 - val_loss: 92.9951\n",
      "Epoch 466/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 64.5029 - val_loss: 94.4049\n",
      "Epoch 467/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 64.1960 - val_loss: 92.9096\n",
      "Epoch 468/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 64.1395 - val_loss: 93.5247\n",
      "Epoch 469/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 64.4526 - val_loss: 94.6677\n",
      "Epoch 470/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 63.9905 - val_loss: 91.8535\n",
      "Epoch 471/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 63.4858 - val_loss: 92.4813\n",
      "Epoch 472/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 63.8348 - val_loss: 94.1304\n",
      "Epoch 473/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 63.7180 - val_loss: 97.8274\n",
      "Epoch 474/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 65.1722 - val_loss: 97.3309\n",
      "Epoch 475/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 65.3278 - val_loss: 95.4737\n",
      "Epoch 476/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 63.9617 - val_loss: 91.9756\n",
      "Epoch 477/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 63.5238 - val_loss: 94.4414\n",
      "Epoch 478/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 64.6362 - val_loss: 92.9667\n",
      "Epoch 479/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 64.7223 - val_loss: 94.2921\n",
      "Epoch 480/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 63.6382 - val_loss: 96.9816\n",
      "Epoch 481/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 63.6987 - val_loss: 96.4812\n",
      "Epoch 482/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 63.8729 - val_loss: 96.5379\n",
      "Epoch 483/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 63.1792 - val_loss: 97.6792\n",
      "Epoch 484/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 65.2211 - val_loss: 98.3086\n",
      "Epoch 485/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 66.6840 - val_loss: 97.3286\n",
      "Epoch 486/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 64.9888 - val_loss: 99.6473\n",
      "Epoch 487/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 64.4818 - val_loss: 90.4171\n",
      "Epoch 488/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 64.0785 - val_loss: 95.4171\n",
      "Epoch 489/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 63.6162 - val_loss: 92.9659\n",
      "Epoch 490/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 63.2602 - val_loss: 96.4121\n",
      "Epoch 491/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 64.0090 - val_loss: 91.4475\n",
      "Epoch 492/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 63.7817 - val_loss: 92.6378\n",
      "Epoch 493/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 63.2525 - val_loss: 92.4305\n",
      "Epoch 494/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 63.4389 - val_loss: 95.5121\n",
      "Epoch 495/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 62.8035 - val_loss: 93.4719\n",
      "Epoch 496/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 63.4110 - val_loss: 94.7159\n",
      "Epoch 497/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 63.4228 - val_loss: 93.2054\n",
      "Epoch 498/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 63.4956 - val_loss: 96.9697\n",
      "Epoch 499/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 63.3779 - val_loss: 93.2162\n",
      "Epoch 500/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 63.0493 - val_loss: 94.9059\n",
      "Epoch 501/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 62.6904 - val_loss: 93.4558\n",
      "Epoch 502/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 62.6990 - val_loss: 95.6521\n",
      "Epoch 503/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 62.5317 - val_loss: 99.8988\n",
      "Epoch 504/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 62.9765 - val_loss: 94.7203\n",
      "Epoch 505/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 62.4358 - val_loss: 93.7688\n",
      "Epoch 506/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 62.4897 - val_loss: 94.2354\n",
      "Epoch 507/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 62.6020 - val_loss: 92.1415\n",
      "Epoch 508/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 62.8178 - val_loss: 98.9932\n",
      "Epoch 509/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 64.4798 - val_loss: 97.4200\n",
      "Epoch 510/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 64.4019 - val_loss: 94.5837\n",
      "Epoch 511/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 64.2474 - val_loss: 97.0433\n",
      "Epoch 512/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 63.8074 - val_loss: 95.5866\n",
      "Epoch 513/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 63.5275 - val_loss: 95.9499\n",
      "Epoch 514/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 62.9444 - val_loss: 96.7326\n",
      "Epoch 515/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 62.5840 - val_loss: 91.1244\n",
      "Epoch 516/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 62.6660 - val_loss: 98.3535\n",
      "Epoch 517/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 63.7457 - val_loss: 93.1009\n",
      "Epoch 518/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 62.2425 - val_loss: 92.6857\n",
      "Epoch 519/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 61.9927 - val_loss: 92.8962\n",
      "Epoch 520/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 62.3468 - val_loss: 94.3890\n",
      "Epoch 521/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 61.9465 - val_loss: 93.4268\n",
      "Epoch 522/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 61.7072 - val_loss: 92.9350\n",
      "Epoch 523/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 61.6925 - val_loss: 93.4882\n",
      "Epoch 524/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 61.8035 - val_loss: 91.8793\n",
      "Epoch 525/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 62.1763 - val_loss: 95.1831\n",
      "Epoch 526/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 62.9012 - val_loss: 96.5544\n",
      "Epoch 527/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 63.0980 - val_loss: 96.2326\n",
      "Epoch 528/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 62.3903 - val_loss: 95.2199\n",
      "Epoch 529/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 63.1595 - val_loss: 94.3235\n",
      "Epoch 530/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 62.4231 - val_loss: 93.4571\n",
      "Epoch 531/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 62.3579 - val_loss: 95.5003\n",
      "Epoch 532/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 62.0849 - val_loss: 92.8759\n",
      "Epoch 533/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 61.7101 - val_loss: 92.7593\n",
      "Epoch 534/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 61.5444 - val_loss: 94.2242\n",
      "Epoch 535/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 62.8888 - val_loss: 95.4589\n",
      "Epoch 536/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 63.8294 - val_loss: 95.5324\n",
      "Epoch 537/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 62.7836 - val_loss: 93.8333\n",
      "Epoch 538/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 63.5639 - val_loss: 96.6271\n",
      "Epoch 539/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 64.9466 - val_loss: 101.5628\n",
      "Epoch 540/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 64.8256 - val_loss: 88.8623\n",
      "Epoch 541/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 63.6424 - val_loss: 94.0955\n",
      "Epoch 542/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 62.5921 - val_loss: 95.7935\n",
      "Epoch 543/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 62.2021 - val_loss: 92.7944\n",
      "Epoch 544/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 61.8744 - val_loss: 92.9355\n",
      "Epoch 545/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 61.5940 - val_loss: 93.8908\n",
      "Epoch 546/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 62.6816 - val_loss: 97.2768\n",
      "Epoch 547/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 62.7805 - val_loss: 93.8351\n",
      "Epoch 548/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 63.4711 - val_loss: 93.3436\n",
      "Epoch 549/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 62.7579 - val_loss: 94.4705\n",
      "Epoch 550/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 61.6813 - val_loss: 94.3527\n",
      "Epoch 551/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 61.6525 - val_loss: 92.3649\n",
      "Epoch 552/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 61.5845 - val_loss: 93.5952\n",
      "Epoch 553/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 61.8982 - val_loss: 94.9326\n",
      "Epoch 554/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 62.1454 - val_loss: 93.5376\n",
      "Epoch 555/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 62.3024 - val_loss: 91.4042\n",
      "Epoch 556/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 62.1737 - val_loss: 93.2407\n",
      "Epoch 557/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 61.4392 - val_loss: 95.8534\n",
      "Epoch 558/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 61.3580 - val_loss: 93.6068\n",
      "Epoch 559/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 61.7036 - val_loss: 95.9440\n",
      "Epoch 560/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 61.4780 - val_loss: 96.9328\n",
      "Epoch 561/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 60.8715 - val_loss: 93.0872\n",
      "Epoch 562/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 60.7398 - val_loss: 92.6063\n",
      "Epoch 563/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 61.3095 - val_loss: 95.6258\n",
      "Epoch 564/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 61.6545 - val_loss: 97.5073\n",
      "Epoch 565/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 60.9390 - val_loss: 92.5090\n",
      "Epoch 566/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 61.3631 - val_loss: 94.2048\n",
      "Epoch 567/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 61.9118 - val_loss: 92.1087\n",
      "Epoch 568/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 61.7215 - val_loss: 95.4639\n",
      "Epoch 569/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 61.7535 - val_loss: 92.4095\n",
      "Epoch 570/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 61.4261 - val_loss: 94.3775\n",
      "Epoch 571/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 61.0380 - val_loss: 93.8682\n",
      "Epoch 572/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 61.0415 - val_loss: 93.3526\n",
      "Epoch 573/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 61.4210 - val_loss: 95.1096\n",
      "Epoch 574/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 61.9684 - val_loss: 90.7545\n",
      "Epoch 575/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 62.0066 - val_loss: 92.0099\n",
      "Epoch 576/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 62.5459 - val_loss: 93.8218\n",
      "Epoch 577/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 61.8984 - val_loss: 94.3173\n",
      "Epoch 578/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 63.1472 - val_loss: 94.9011\n",
      "Epoch 579/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 63.5940 - val_loss: 93.0816\n",
      "Epoch 580/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 65.5742 - val_loss: 99.1727\n",
      "Epoch 581/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 65.0091 - val_loss: 94.6659\n",
      "Epoch 582/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 63.7255 - val_loss: 95.4608\n",
      "Epoch 583/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 61.8841 - val_loss: 90.0883\n",
      "Epoch 584/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 61.7419 - val_loss: 93.9709\n",
      "Epoch 585/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 61.4678 - val_loss: 91.9652\n",
      "Epoch 586/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 61.3685 - val_loss: 92.2125\n",
      "Epoch 587/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 62.5947 - val_loss: 94.7395\n",
      "Epoch 588/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 62.4919 - val_loss: 92.5173\n",
      "Epoch 589/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 61.8013 - val_loss: 95.6973\n",
      "Epoch 590/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 61.3057 - val_loss: 94.8112\n",
      "Epoch 591/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 61.1965 - val_loss: 92.9582\n",
      "Epoch 592/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 60.9723 - val_loss: 93.4349\n",
      "Epoch 593/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 60.7344 - val_loss: 94.2011\n",
      "Epoch 594/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 60.8735 - val_loss: 92.5443\n",
      "Epoch 595/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 60.6749 - val_loss: 91.9187\n",
      "Epoch 596/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 60.7575 - val_loss: 94.0114\n",
      "Epoch 597/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 60.7312 - val_loss: 93.3788\n",
      "Epoch 598/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 61.1783 - val_loss: 91.2630\n",
      "Epoch 599/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 61.3301 - val_loss: 95.2863\n",
      "Epoch 600/2000\n",
      "6/6 [==============================] - ETA: 0s - loss: 61.9030\n",
      "Epoch 00600: saving model to saved_models/latent2/cp-0600.h5\n",
      "6/6 [==============================] - 1s 154ms/step - loss: 61.9030 - val_loss: 95.6609\n",
      "Epoch 601/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 62.5616 - val_loss: 97.2101\n",
      "Epoch 602/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 62.0810 - val_loss: 94.9291\n",
      "Epoch 603/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 63.0165 - val_loss: 93.5064\n",
      "Epoch 604/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 61.9155 - val_loss: 94.3231\n",
      "Epoch 605/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 61.3185 - val_loss: 94.4047\n",
      "Epoch 606/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 60.7523 - val_loss: 90.8627\n",
      "Epoch 607/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 59.8526 - val_loss: 92.9100\n",
      "Epoch 608/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 60.6871 - val_loss: 94.6448\n",
      "Epoch 609/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 61.0730 - val_loss: 91.4187\n",
      "Epoch 610/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 61.0184 - val_loss: 93.5848\n",
      "Epoch 611/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 60.3856 - val_loss: 96.0289\n",
      "Epoch 612/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 60.8945 - val_loss: 92.1576\n",
      "Epoch 613/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 61.4837 - val_loss: 92.0943\n",
      "Epoch 614/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 60.7636 - val_loss: 92.5919\n",
      "Epoch 615/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 60.4520 - val_loss: 94.1936\n",
      "Epoch 616/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 59.9993 - val_loss: 94.2472\n",
      "Epoch 617/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 59.8837 - val_loss: 93.4890\n",
      "Epoch 618/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 59.7904 - val_loss: 91.0915\n",
      "Epoch 619/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 59.4642 - val_loss: 94.3525\n",
      "Epoch 620/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 59.6942 - val_loss: 94.7655\n",
      "Epoch 621/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 59.6571 - val_loss: 92.3769\n",
      "Epoch 622/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 59.8532 - val_loss: 92.9690\n",
      "Epoch 623/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 60.1576 - val_loss: 93.2953\n",
      "Epoch 624/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 60.5094 - val_loss: 93.8832\n",
      "Epoch 625/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 60.6265 - val_loss: 92.7086\n",
      "Epoch 626/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 60.8471 - val_loss: 95.7739\n",
      "Epoch 627/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 59.9278 - val_loss: 94.6417\n",
      "Epoch 628/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 59.5601 - val_loss: 90.2143\n",
      "Epoch 629/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 60.1215 - val_loss: 93.2142\n",
      "Epoch 630/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 59.9924 - val_loss: 94.3277\n",
      "Epoch 631/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 60.3073 - val_loss: 92.2771\n",
      "Epoch 632/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 59.6072 - val_loss: 90.2125\n",
      "Epoch 633/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 60.1051 - val_loss: 94.5170\n",
      "Epoch 634/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 60.2247 - val_loss: 93.8097\n",
      "Epoch 635/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 60.3907 - val_loss: 93.8687\n",
      "Epoch 636/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 60.9797 - val_loss: 94.0657\n",
      "Epoch 637/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 60.7568 - val_loss: 92.9377\n",
      "Epoch 638/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 60.2968 - val_loss: 93.2726\n",
      "Epoch 639/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 60.5090 - val_loss: 90.7362\n",
      "Epoch 640/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 60.1049 - val_loss: 93.3398\n",
      "Epoch 641/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 59.8985 - val_loss: 94.1613\n",
      "Epoch 642/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 59.9276 - val_loss: 92.7272\n",
      "Epoch 643/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 59.7673 - val_loss: 90.5622\n",
      "Epoch 644/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 59.4574 - val_loss: 92.1643\n",
      "Epoch 645/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 59.7100 - val_loss: 92.4099\n",
      "Epoch 646/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 59.7826 - val_loss: 92.2985\n",
      "Epoch 647/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 59.4217 - val_loss: 95.3857\n",
      "Epoch 648/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 59.7008 - val_loss: 91.6521\n",
      "Epoch 649/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 60.2748 - val_loss: 94.7953\n",
      "Epoch 650/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 59.7595 - val_loss: 95.5130\n",
      "Epoch 651/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 59.9187 - val_loss: 92.9196\n",
      "Epoch 652/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 60.5603 - val_loss: 94.6043\n",
      "Epoch 653/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 60.8226 - val_loss: 91.8719\n",
      "Epoch 654/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 59.6712 - val_loss: 91.3105\n",
      "Epoch 655/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 59.6310 - val_loss: 95.9625\n",
      "Epoch 656/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 60.0820 - val_loss: 92.7101\n",
      "Epoch 657/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 59.7144 - val_loss: 94.2022\n",
      "Epoch 658/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 59.5263 - val_loss: 95.8970\n",
      "Epoch 659/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 60.0956 - val_loss: 91.1656\n",
      "Epoch 660/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 59.5187 - val_loss: 93.2171\n",
      "Epoch 661/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 59.5765 - val_loss: 92.4027\n",
      "Epoch 662/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 59.6098 - val_loss: 93.5091\n",
      "Epoch 663/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 58.8525 - val_loss: 93.7914\n",
      "Epoch 664/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 58.9628 - val_loss: 91.7714\n",
      "Epoch 665/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 59.0411 - val_loss: 92.4696\n",
      "Epoch 666/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 58.7136 - val_loss: 92.6895\n",
      "Epoch 667/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 58.9872 - val_loss: 93.5728\n",
      "Epoch 668/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 59.3630 - val_loss: 92.2945\n",
      "Epoch 669/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 60.1759 - val_loss: 93.1857\n",
      "Epoch 670/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 59.5065 - val_loss: 91.7052\n",
      "Epoch 671/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 59.9424 - val_loss: 93.8531\n",
      "Epoch 672/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 59.3591 - val_loss: 92.5634\n",
      "Epoch 673/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 58.6705 - val_loss: 93.8221\n",
      "Epoch 674/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 59.3913 - val_loss: 92.1183\n",
      "Epoch 675/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 58.9162 - val_loss: 93.9903\n",
      "Epoch 676/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 59.0360 - val_loss: 91.3015\n",
      "Epoch 677/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 59.0800 - val_loss: 95.5968\n",
      "Epoch 678/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 59.2248 - val_loss: 93.1276\n",
      "Epoch 679/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 59.5443 - val_loss: 92.8913\n",
      "Epoch 680/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 58.8866 - val_loss: 91.7636\n",
      "Epoch 681/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 59.4569 - val_loss: 96.7349\n",
      "Epoch 682/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 60.1397 - val_loss: 93.3648\n",
      "Epoch 683/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 59.9734 - val_loss: 92.5667\n",
      "Epoch 684/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 59.5803 - val_loss: 93.9206\n",
      "Epoch 685/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 60.1929 - val_loss: 95.9631\n",
      "Epoch 686/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 60.9262 - val_loss: 96.5773\n",
      "Epoch 687/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 61.4595 - val_loss: 93.8402\n",
      "Epoch 688/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 60.7497 - val_loss: 92.4589\n",
      "Epoch 689/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 60.0710 - val_loss: 95.4284\n",
      "Epoch 690/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 60.8721 - val_loss: 97.6037\n",
      "Epoch 691/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 61.9868 - val_loss: 92.2178\n",
      "Epoch 692/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 59.9478 - val_loss: 91.4506\n",
      "Epoch 693/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 59.5909 - val_loss: 91.2008\n",
      "Epoch 694/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 58.6786 - val_loss: 89.1377\n",
      "Epoch 695/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 58.9239 - val_loss: 92.5513\n",
      "Epoch 696/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 58.9844 - val_loss: 92.2555\n",
      "Epoch 697/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 59.0169 - val_loss: 92.9162\n",
      "Epoch 698/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 59.6763 - val_loss: 92.7309\n",
      "Epoch 699/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 59.1497 - val_loss: 91.8173\n",
      "Epoch 700/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 58.7177 - val_loss: 92.4846\n",
      "Epoch 701/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 59.1010 - val_loss: 95.5255\n",
      "Epoch 702/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 59.7554 - val_loss: 93.1366\n",
      "Epoch 703/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 59.0758 - val_loss: 94.0439\n",
      "Epoch 704/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 58.7735 - val_loss: 94.3411\n",
      "Epoch 705/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 60.0796 - val_loss: 96.5027\n",
      "Epoch 706/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 59.4962 - val_loss: 93.3748\n",
      "Epoch 707/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 60.6669 - val_loss: 91.3520\n",
      "Epoch 708/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 61.2057 - val_loss: 93.7944\n",
      "Epoch 709/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 60.2829 - val_loss: 94.7356\n",
      "Epoch 710/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 60.5625 - val_loss: 95.0714\n",
      "Epoch 711/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 58.8143 - val_loss: 91.1084\n",
      "Epoch 712/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 59.1528 - val_loss: 93.0214\n",
      "Epoch 713/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 59.0861 - val_loss: 94.6500\n",
      "Epoch 714/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 58.8784 - val_loss: 95.8095\n",
      "Epoch 715/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 59.6563 - val_loss: 93.8497\n",
      "Epoch 716/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 58.8336 - val_loss: 93.4955\n",
      "Epoch 717/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 58.1403 - val_loss: 93.2689\n",
      "Epoch 718/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 58.0050 - val_loss: 95.0068\n",
      "Epoch 719/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 58.9800 - val_loss: 93.5614\n",
      "Epoch 720/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 58.6696 - val_loss: 92.7503\n",
      "Epoch 721/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 60.1105 - val_loss: 97.0032\n",
      "Epoch 722/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 59.6195 - val_loss: 93.1409\n",
      "Epoch 723/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 60.2556 - val_loss: 92.2026\n",
      "Epoch 724/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 58.7347 - val_loss: 95.2221\n",
      "Epoch 725/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 58.2777 - val_loss: 94.7592\n",
      "Epoch 726/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 59.4648 - val_loss: 92.7876\n",
      "Epoch 727/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 59.7777 - val_loss: 93.5949\n",
      "Epoch 728/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 57.9871 - val_loss: 92.7638\n",
      "Epoch 729/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 57.9130 - val_loss: 93.0192\n",
      "Epoch 730/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 57.7395 - val_loss: 90.7882\n",
      "Epoch 731/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 58.0973 - val_loss: 90.7358\n",
      "Epoch 732/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 57.9649 - val_loss: 92.2244\n",
      "Epoch 733/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 57.9339 - val_loss: 90.0415\n",
      "Epoch 734/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 58.3480 - val_loss: 93.2253\n",
      "Epoch 735/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 59.8543 - val_loss: 93.8003\n",
      "Epoch 736/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 57.9889 - val_loss: 93.6518\n",
      "Epoch 737/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 58.5886 - val_loss: 91.9970\n",
      "Epoch 738/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 57.3340 - val_loss: 92.7894\n",
      "Epoch 739/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 58.4470 - val_loss: 90.2660\n",
      "Epoch 740/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 58.2052 - val_loss: 90.1432\n",
      "Epoch 741/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 57.1156 - val_loss: 93.0656\n",
      "Epoch 742/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 57.2069 - val_loss: 92.0374\n",
      "Epoch 743/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 57.3090 - val_loss: 92.5153\n",
      "Epoch 744/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 57.9594 - val_loss: 93.7130\n",
      "Epoch 745/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 59.3971 - val_loss: 98.9582\n",
      "Epoch 746/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 61.6363 - val_loss: 90.3992\n",
      "Epoch 747/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 60.3117 - val_loss: 93.6188\n",
      "Epoch 748/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 59.5876 - val_loss: 90.2665\n",
      "Epoch 749/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 58.5476 - val_loss: 92.4828\n",
      "Epoch 750/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 59.4032 - val_loss: 92.8632\n",
      "Epoch 751/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 58.3551 - val_loss: 93.1366\n",
      "Epoch 752/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 58.1315 - val_loss: 91.2310\n",
      "Epoch 753/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 57.6127 - val_loss: 91.2903\n",
      "Epoch 754/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 57.0039 - val_loss: 92.3668\n",
      "Epoch 755/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 57.6580 - val_loss: 94.7357\n",
      "Epoch 756/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 58.5364 - val_loss: 91.2695\n",
      "Epoch 757/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 59.1067 - val_loss: 91.7483\n",
      "Epoch 758/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 59.1138 - val_loss: 92.9559\n",
      "Epoch 759/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 58.0497 - val_loss: 92.3510\n",
      "Epoch 760/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 57.1932 - val_loss: 89.5266\n",
      "Epoch 761/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 56.8303 - val_loss: 91.8154\n",
      "Epoch 762/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 57.0339 - val_loss: 89.6887\n",
      "Epoch 763/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 56.9250 - val_loss: 91.9263\n",
      "Epoch 764/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 56.6401 - val_loss: 91.4027\n",
      "Epoch 765/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 56.1902 - val_loss: 90.9883\n",
      "Epoch 766/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 56.4939 - val_loss: 91.7706\n",
      "Epoch 767/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 55.9906 - val_loss: 91.7010\n",
      "Epoch 768/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 55.9080 - val_loss: 91.2564\n",
      "Epoch 769/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 55.5237 - val_loss: 91.9136\n",
      "Epoch 770/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 56.0613 - val_loss: 93.0512\n",
      "Epoch 771/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 56.8762 - val_loss: 91.8516\n",
      "Epoch 772/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 56.4016 - val_loss: 91.7286\n",
      "Epoch 773/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 56.1585 - val_loss: 90.7425\n",
      "Epoch 774/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 56.1136 - val_loss: 93.2782\n",
      "Epoch 775/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 55.9768 - val_loss: 92.3326\n",
      "Epoch 776/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 56.5433 - val_loss: 93.7811\n",
      "Epoch 777/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 56.5197 - val_loss: 90.8455\n",
      "Epoch 778/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 57.1195 - val_loss: 93.1705\n",
      "Epoch 779/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 56.7769 - val_loss: 95.5508\n",
      "Epoch 780/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 56.6843 - val_loss: 91.5675\n",
      "Epoch 781/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 55.8511 - val_loss: 90.3554\n",
      "Epoch 782/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 55.8613 - val_loss: 91.7082\n",
      "Epoch 783/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 56.4200 - val_loss: 90.8542\n",
      "Epoch 784/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 57.4066 - val_loss: 92.8185\n",
      "Epoch 785/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 56.9321 - val_loss: 91.5495\n",
      "Epoch 786/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 56.0768 - val_loss: 92.3713\n",
      "Epoch 787/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 56.6849 - val_loss: 90.7121\n",
      "Epoch 788/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 57.3676 - val_loss: 92.0868\n",
      "Epoch 789/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 56.1941 - val_loss: 90.8401\n",
      "Epoch 790/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 56.0369 - val_loss: 92.4923\n",
      "Epoch 791/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 55.3174 - val_loss: 93.6482\n",
      "Epoch 792/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 55.4745 - val_loss: 90.5407\n",
      "Epoch 793/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 56.1935 - val_loss: 93.4881\n",
      "Epoch 794/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 56.4390 - val_loss: 95.0416\n",
      "Epoch 795/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 55.8730 - val_loss: 90.4661\n",
      "Epoch 796/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 55.7647 - val_loss: 93.8927\n",
      "Epoch 797/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 56.0300 - val_loss: 92.9397\n",
      "Epoch 798/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 55.9283 - val_loss: 92.8773\n",
      "Epoch 799/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 57.2457 - val_loss: 93.0496\n",
      "Epoch 800/2000\n",
      "6/6 [==============================] - ETA: 0s - loss: 58.0322\n",
      "Epoch 00800: saving model to saved_models/latent2/cp-0800.h5\n",
      "6/6 [==============================] - 1s 149ms/step - loss: 58.0322 - val_loss: 89.7755\n",
      "Epoch 801/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 57.9164 - val_loss: 89.2630\n",
      "Epoch 802/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 56.6938 - val_loss: 89.6061\n",
      "Epoch 803/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 55.8310 - val_loss: 92.3414\n",
      "Epoch 804/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 55.3155 - val_loss: 93.1846\n",
      "Epoch 805/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 55.1409 - val_loss: 92.7126\n",
      "Epoch 806/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 55.3217 - val_loss: 89.5287\n",
      "Epoch 807/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 55.6842 - val_loss: 91.1356\n",
      "Epoch 808/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 55.6061 - val_loss: 92.2643\n",
      "Epoch 809/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 55.4521 - val_loss: 90.4767\n",
      "Epoch 810/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 55.6413 - val_loss: 91.3772\n",
      "Epoch 811/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 55.6468 - val_loss: 92.7298\n",
      "Epoch 812/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 55.9199 - val_loss: 93.4800\n",
      "Epoch 813/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 55.7998 - val_loss: 92.4293\n",
      "Epoch 814/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 57.3413 - val_loss: 92.3946\n",
      "Epoch 815/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 56.5190 - val_loss: 92.6608\n",
      "Epoch 816/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 56.1733 - val_loss: 93.2904\n",
      "Epoch 817/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 56.3785 - val_loss: 92.0554\n",
      "Epoch 818/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 55.6279 - val_loss: 93.1297\n",
      "Epoch 819/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 56.0029 - val_loss: 90.5486\n",
      "Epoch 820/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 56.4312 - val_loss: 91.3683\n",
      "Epoch 821/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 57.0405 - val_loss: 90.3250\n",
      "Epoch 822/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 55.4050 - val_loss: 92.8270\n",
      "Epoch 823/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 54.8571 - val_loss: 94.0427\n",
      "Epoch 824/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 55.6630 - val_loss: 93.1300\n",
      "Epoch 825/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 56.4849 - val_loss: 95.6731\n",
      "Epoch 826/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 56.1907 - val_loss: 91.5616\n",
      "Epoch 827/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 55.9240 - val_loss: 94.4542\n",
      "Epoch 828/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 55.9444 - val_loss: 88.1158\n",
      "Epoch 829/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 55.8341 - val_loss: 91.6432\n",
      "Epoch 830/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 55.7755 - val_loss: 93.7086\n",
      "Epoch 831/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 55.9876 - val_loss: 91.5092\n",
      "Epoch 832/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 55.2382 - val_loss: 92.2929\n",
      "Epoch 833/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 56.0236 - val_loss: 88.4213\n",
      "Epoch 834/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 55.7531 - val_loss: 94.2622\n",
      "Epoch 835/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 56.2974 - val_loss: 90.0895\n",
      "Epoch 836/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 57.1283 - val_loss: 95.9406\n",
      "Epoch 837/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 56.9064 - val_loss: 91.7512\n",
      "Epoch 838/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 54.8475 - val_loss: 88.4497\n",
      "Epoch 839/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 54.7733 - val_loss: 92.3125\n",
      "Epoch 840/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 55.5977 - val_loss: 93.4547\n",
      "Epoch 841/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 55.3060 - val_loss: 91.1928\n",
      "Epoch 842/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 55.2349 - val_loss: 90.7475\n",
      "Epoch 843/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 53.8045 - val_loss: 91.2678\n",
      "Epoch 844/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 54.2501 - val_loss: 93.4615\n",
      "Epoch 845/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 54.9042 - val_loss: 91.4195\n",
      "Epoch 846/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 55.0339 - val_loss: 91.3534\n",
      "Epoch 847/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 54.7195 - val_loss: 92.1781\n",
      "Epoch 848/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 54.6266 - val_loss: 92.9292\n",
      "Epoch 849/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 54.5521 - val_loss: 92.2155\n",
      "Epoch 850/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 54.5163 - val_loss: 92.2696\n",
      "Epoch 851/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 54.5074 - val_loss: 90.8421\n",
      "Epoch 852/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 53.8915 - val_loss: 91.4366\n",
      "Epoch 853/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 54.0162 - val_loss: 91.5585\n",
      "Epoch 854/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 53.6235 - val_loss: 89.3602\n",
      "Epoch 855/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 53.5876 - val_loss: 92.6718\n",
      "Epoch 856/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 54.7616 - val_loss: 93.4262\n",
      "Epoch 857/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 54.7927 - val_loss: 95.2702\n",
      "Epoch 858/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 56.5137 - val_loss: 94.0683\n",
      "Epoch 859/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 56.2879 - val_loss: 90.3931\n",
      "Epoch 860/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 54.6644 - val_loss: 91.3558\n",
      "Epoch 861/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 54.1757 - val_loss: 91.5771\n",
      "Epoch 862/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 54.2180 - val_loss: 90.4545\n",
      "Epoch 863/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 53.5755 - val_loss: 90.8110\n",
      "Epoch 864/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 54.1184 - val_loss: 94.3678\n",
      "Epoch 865/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 55.6940 - val_loss: 90.3677\n",
      "Epoch 866/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 54.6393 - val_loss: 90.5356\n",
      "Epoch 867/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 54.6877 - val_loss: 89.9146\n",
      "Epoch 868/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 54.2451 - val_loss: 92.8849\n",
      "Epoch 869/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 54.1092 - val_loss: 89.9870\n",
      "Epoch 870/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 53.8292 - val_loss: 90.9842\n",
      "Epoch 871/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 53.7748 - val_loss: 89.6651\n",
      "Epoch 872/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 54.2501 - val_loss: 92.5368\n",
      "Epoch 873/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 54.7017 - val_loss: 92.1631\n",
      "Epoch 874/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 53.9428 - val_loss: 92.2909\n",
      "Epoch 875/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 54.0579 - val_loss: 90.6011\n",
      "Epoch 876/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 53.4628 - val_loss: 90.4875\n",
      "Epoch 877/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 53.3697 - val_loss: 93.1692\n",
      "Epoch 878/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 54.4065 - val_loss: 92.3093\n",
      "Epoch 879/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 54.7061 - val_loss: 92.9315\n",
      "Epoch 880/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 55.7007 - val_loss: 95.6516\n",
      "Epoch 881/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 55.7344 - val_loss: 96.9063\n",
      "Epoch 882/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 55.9896 - val_loss: 93.2008\n",
      "Epoch 883/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 54.0850 - val_loss: 91.4502\n",
      "Epoch 884/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 53.3211 - val_loss: 93.6167\n",
      "Epoch 885/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 54.1472 - val_loss: 90.1071\n",
      "Epoch 886/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 53.9288 - val_loss: 91.3814\n",
      "Epoch 887/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 53.3054 - val_loss: 91.6820\n",
      "Epoch 888/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 54.0707 - val_loss: 90.8146\n",
      "Epoch 889/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 53.5070 - val_loss: 92.9873\n",
      "Epoch 890/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 52.8354 - val_loss: 92.6326\n",
      "Epoch 891/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 53.6736 - val_loss: 91.4708\n",
      "Epoch 892/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 54.1479 - val_loss: 94.4597\n",
      "Epoch 893/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 54.9275 - val_loss: 97.9890\n",
      "Epoch 894/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 55.6100 - val_loss: 92.8479\n",
      "Epoch 895/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 53.8012 - val_loss: 93.2547\n",
      "Epoch 896/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 53.3320 - val_loss: 92.3868\n",
      "Epoch 897/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 54.2214 - val_loss: 93.4019\n",
      "Epoch 898/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 54.5830 - val_loss: 93.4418\n",
      "Epoch 899/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 53.9840 - val_loss: 92.0326\n",
      "Epoch 900/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 55.2662 - val_loss: 94.0043\n",
      "Epoch 901/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 54.5151 - val_loss: 90.0006\n",
      "Epoch 902/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 54.3293 - val_loss: 91.6789\n",
      "Epoch 903/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 53.6633 - val_loss: 90.5945\n",
      "Epoch 904/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 53.4698 - val_loss: 88.1104\n",
      "Epoch 905/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 53.5520 - val_loss: 93.0030\n",
      "Epoch 906/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 53.4514 - val_loss: 92.1180\n",
      "Epoch 907/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 54.0513 - val_loss: 91.9518\n",
      "Epoch 908/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 53.7124 - val_loss: 90.4330\n",
      "Epoch 909/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 53.7361 - val_loss: 90.8816\n",
      "Epoch 910/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 53.5354 - val_loss: 90.2828\n",
      "Epoch 911/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 53.5068 - val_loss: 93.9996\n",
      "Epoch 912/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 53.6181 - val_loss: 93.6146\n",
      "Epoch 913/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 53.3034 - val_loss: 89.2146\n",
      "Epoch 914/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 53.5636 - val_loss: 91.9004\n",
      "Epoch 915/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 54.3540 - val_loss: 98.5509\n",
      "Epoch 916/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 54.9823 - val_loss: 94.1017\n",
      "Epoch 917/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 54.3098 - val_loss: 90.2325\n",
      "Epoch 918/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 53.6834 - val_loss: 95.0217\n",
      "Epoch 919/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 54.2556 - val_loss: 90.9374\n",
      "Epoch 920/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 53.4886 - val_loss: 91.1146\n",
      "Epoch 921/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 53.2022 - val_loss: 94.0533\n",
      "Epoch 922/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 53.6796 - val_loss: 89.7421\n",
      "Epoch 923/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 53.5151 - val_loss: 93.5367\n",
      "Epoch 924/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 53.4405 - val_loss: 93.8120\n",
      "Epoch 925/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 53.8748 - val_loss: 93.5496\n",
      "Epoch 926/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 53.3863 - val_loss: 92.3387\n",
      "Epoch 927/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 53.7584 - val_loss: 92.0257\n",
      "Epoch 928/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 54.5056 - val_loss: 91.7586\n",
      "Epoch 929/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 52.9166 - val_loss: 91.2403\n",
      "Epoch 930/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 53.7129 - val_loss: 93.1359\n",
      "Epoch 931/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 53.7993 - val_loss: 90.6104\n",
      "Epoch 932/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 53.3160 - val_loss: 90.8879\n",
      "Epoch 933/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 53.5455 - val_loss: 94.9462\n",
      "Epoch 934/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 53.3140 - val_loss: 90.5423\n",
      "Epoch 935/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 52.5278 - val_loss: 89.1480\n",
      "Epoch 936/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 52.6927 - val_loss: 90.1495\n",
      "Epoch 937/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 53.8977 - val_loss: 90.6983\n",
      "Epoch 938/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 54.1845 - val_loss: 92.2231\n",
      "Epoch 939/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 54.0262 - val_loss: 93.5311\n",
      "Epoch 940/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 53.5415 - val_loss: 93.1379\n",
      "Epoch 941/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 53.8436 - val_loss: 90.6686\n",
      "Epoch 942/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 53.8774 - val_loss: 93.6405\n",
      "Epoch 943/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 52.9919 - val_loss: 89.8163\n",
      "Epoch 944/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 53.7904 - val_loss: 87.1711\n",
      "Epoch 945/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 52.8440 - val_loss: 92.7194\n",
      "Epoch 946/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 52.9443 - val_loss: 90.5505\n",
      "Epoch 947/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 52.1532 - val_loss: 90.1395\n",
      "Epoch 948/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 53.0161 - val_loss: 90.2513\n",
      "Epoch 949/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 53.1234 - val_loss: 89.3047\n",
      "Epoch 950/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 53.4670 - val_loss: 91.5881\n",
      "Epoch 951/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 52.7698 - val_loss: 92.0356\n",
      "Epoch 952/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 52.2367 - val_loss: 95.9181\n",
      "Epoch 953/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 54.2669 - val_loss: 97.5119\n",
      "Epoch 954/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 56.1589 - val_loss: 94.5025\n",
      "Epoch 955/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 55.5625 - val_loss: 93.8602\n",
      "Epoch 956/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 55.3310 - val_loss: 91.5788\n",
      "Epoch 957/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 53.7511 - val_loss: 90.7024\n",
      "Epoch 958/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 53.0947 - val_loss: 89.9442\n",
      "Epoch 959/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 52.9359 - val_loss: 91.2954\n",
      "Epoch 960/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 52.6666 - val_loss: 91.4561\n",
      "Epoch 961/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 52.8531 - val_loss: 92.1943\n",
      "Epoch 962/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 53.2178 - val_loss: 93.1782\n",
      "Epoch 963/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 53.2838 - val_loss: 92.5968\n",
      "Epoch 964/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 53.2813 - val_loss: 91.1835\n",
      "Epoch 965/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 53.4961 - val_loss: 92.5284\n",
      "Epoch 966/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 52.5152 - val_loss: 93.1383\n",
      "Epoch 967/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 53.5798 - val_loss: 91.3407\n",
      "Epoch 968/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 52.7972 - val_loss: 90.1954\n",
      "Epoch 969/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 52.7002 - val_loss: 90.1546\n",
      "Epoch 970/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 52.9188 - val_loss: 94.1126\n",
      "Epoch 971/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 52.3236 - val_loss: 91.7832\n",
      "Epoch 972/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 52.2758 - val_loss: 89.0105\n",
      "Epoch 973/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 52.9517 - val_loss: 91.1488\n",
      "Epoch 974/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 52.3315 - val_loss: 87.7313\n",
      "Epoch 975/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 52.4937 - val_loss: 89.0579\n",
      "Epoch 976/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 51.8459 - val_loss: 88.3473\n",
      "Epoch 977/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 52.3367 - val_loss: 90.0118\n",
      "Epoch 978/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 51.0821 - val_loss: 91.7534\n",
      "Epoch 979/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 51.5568 - val_loss: 94.3893\n",
      "Epoch 980/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 52.3740 - val_loss: 89.8176\n",
      "Epoch 981/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 51.8547 - val_loss: 89.8065\n",
      "Epoch 982/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 51.6023 - val_loss: 91.7710\n",
      "Epoch 983/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 51.7119 - val_loss: 91.1273\n",
      "Epoch 984/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 51.5730 - val_loss: 91.2045\n",
      "Epoch 985/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 51.8625 - val_loss: 93.7388\n",
      "Epoch 986/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 53.6078 - val_loss: 96.6611\n",
      "Epoch 987/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 53.7893 - val_loss: 95.0033\n",
      "Epoch 988/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 52.6924 - val_loss: 93.0119\n",
      "Epoch 989/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 53.2118 - val_loss: 91.6725\n",
      "Epoch 990/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 52.9224 - val_loss: 92.5279\n",
      "Epoch 991/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 53.1755 - val_loss: 91.0994\n",
      "Epoch 992/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 54.8190 - val_loss: 91.1075\n",
      "Epoch 993/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 53.1777 - val_loss: 92.1587\n",
      "Epoch 994/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 52.5533 - val_loss: 90.2482\n",
      "Epoch 995/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 51.6379 - val_loss: 89.9260\n",
      "Epoch 996/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 51.6712 - val_loss: 92.0355\n",
      "Epoch 997/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 52.5158 - val_loss: 92.2280\n",
      "Epoch 998/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 53.8199 - val_loss: 91.8486\n",
      "Epoch 999/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 53.2958 - val_loss: 91.3375\n",
      "Epoch 1000/2000\n",
      "6/6 [==============================] - ETA: 0s - loss: 52.7037\n",
      "Epoch 01000: saving model to saved_models/latent2/cp-1000.h5\n",
      "6/6 [==============================] - 1s 158ms/step - loss: 52.7037 - val_loss: 92.5960\n",
      "Epoch 1001/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 54.1342 - val_loss: 93.8438\n",
      "Epoch 1002/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 53.1151 - val_loss: 92.2031\n",
      "Epoch 1003/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 52.5083 - val_loss: 90.5692\n",
      "Epoch 1004/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 52.0904 - val_loss: 92.8966\n",
      "Epoch 1005/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 53.0785 - val_loss: 95.1661\n",
      "Epoch 1006/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 52.0767 - val_loss: 91.1158\n",
      "Epoch 1007/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 51.4492 - val_loss: 91.7865\n",
      "Epoch 1008/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 51.6923 - val_loss: 89.4606\n",
      "Epoch 1009/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 51.5514 - val_loss: 89.9810\n",
      "Epoch 1010/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 53.0987 - val_loss: 91.0862\n",
      "Epoch 1011/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 52.0664 - val_loss: 92.2343\n",
      "Epoch 1012/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 53.4885 - val_loss: 91.0372\n",
      "Epoch 1013/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 53.5107 - val_loss: 93.3527\n",
      "Epoch 1014/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 54.2224 - val_loss: 95.6613\n",
      "Epoch 1015/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 54.4519 - val_loss: 93.5873\n",
      "Epoch 1016/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 54.1434 - val_loss: 92.5239\n",
      "Epoch 1017/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 54.3481 - val_loss: 89.9331\n",
      "Epoch 1018/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 53.1284 - val_loss: 91.1036\n",
      "Epoch 1019/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 53.0417 - val_loss: 90.0015\n",
      "Epoch 1020/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 52.8024 - val_loss: 92.5885\n",
      "Epoch 1021/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 52.5476 - val_loss: 93.2165\n",
      "Epoch 1022/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 51.8926 - val_loss: 93.1529\n",
      "Epoch 1023/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 51.7013 - val_loss: 89.2419\n",
      "Epoch 1024/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 51.1832 - val_loss: 91.9540\n",
      "Epoch 1025/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 50.9825 - val_loss: 90.2019\n",
      "Epoch 1026/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 51.3945 - val_loss: 92.9436\n",
      "Epoch 1027/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 51.1438 - val_loss: 88.7958\n",
      "Epoch 1028/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 50.7948 - val_loss: 89.8774\n",
      "Epoch 1029/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 50.6082 - val_loss: 89.5875\n",
      "Epoch 1030/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 51.0349 - val_loss: 91.4283\n",
      "Epoch 1031/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 51.6254 - val_loss: 88.6304\n",
      "Epoch 1032/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 51.5688 - val_loss: 92.8078\n",
      "Epoch 1033/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 51.2424 - val_loss: 91.0344\n",
      "Epoch 1034/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 51.8177 - val_loss: 92.9036\n",
      "Epoch 1035/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 52.0252 - val_loss: 91.7032\n",
      "Epoch 1036/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 51.1775 - val_loss: 93.5671\n",
      "Epoch 1037/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 50.7726 - val_loss: 89.8522\n",
      "Epoch 1038/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 50.8279 - val_loss: 90.9831\n",
      "Epoch 1039/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 50.6379 - val_loss: 89.8405\n",
      "Epoch 1040/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 50.9050 - val_loss: 90.2344\n",
      "Epoch 1041/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 51.3299 - val_loss: 91.0957\n",
      "Epoch 1042/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 51.2923 - val_loss: 92.6396\n",
      "Epoch 1043/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 51.9394 - val_loss: 90.9475\n",
      "Epoch 1044/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 51.6729 - val_loss: 92.0243\n",
      "Epoch 1045/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 51.6970 - val_loss: 91.5057\n",
      "Epoch 1046/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 51.7474 - val_loss: 88.8127\n",
      "Epoch 1047/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 51.6953 - val_loss: 89.6329\n",
      "Epoch 1048/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 50.9820 - val_loss: 90.3631\n",
      "Epoch 1049/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 50.4254 - val_loss: 92.3616\n",
      "Epoch 1050/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 50.6030 - val_loss: 88.7862\n",
      "Epoch 1051/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 50.7785 - val_loss: 88.7199\n",
      "Epoch 1052/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 50.3398 - val_loss: 90.5304\n",
      "Epoch 1053/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 51.6781 - val_loss: 90.1629\n",
      "Epoch 1054/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 51.4788 - val_loss: 91.6829\n",
      "Epoch 1055/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 51.0501 - val_loss: 93.8768\n",
      "Epoch 1056/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 51.9411 - val_loss: 90.3117\n",
      "Epoch 1057/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 51.7039 - val_loss: 90.3935\n",
      "Epoch 1058/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 51.9698 - val_loss: 94.7246\n",
      "Epoch 1059/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 52.1703 - val_loss: 89.2324\n",
      "Epoch 1060/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 51.8030 - val_loss: 89.6263\n",
      "Epoch 1061/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 50.7527 - val_loss: 89.0553\n",
      "Epoch 1062/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 51.5349 - val_loss: 90.7839\n",
      "Epoch 1063/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 51.5997 - val_loss: 90.7042\n",
      "Epoch 1064/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 50.8732 - val_loss: 90.1835\n",
      "Epoch 1065/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 50.5845 - val_loss: 89.0513\n",
      "Epoch 1066/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 50.2597 - val_loss: 91.7927\n",
      "Epoch 1067/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 50.0590 - val_loss: 90.0047\n",
      "Epoch 1068/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 51.0803 - val_loss: 90.5953\n",
      "Epoch 1069/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 50.8092 - val_loss: 91.7749\n",
      "Epoch 1070/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 50.6663 - val_loss: 89.7331\n",
      "Epoch 1071/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 50.9314 - val_loss: 91.4568\n",
      "Epoch 1072/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 51.1039 - val_loss: 90.1441\n",
      "Epoch 1073/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 52.0132 - val_loss: 92.0122\n",
      "Epoch 1074/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 53.4476 - val_loss: 91.2198\n",
      "Epoch 1075/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 53.7119 - val_loss: 90.7365\n",
      "Epoch 1076/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 52.5586 - val_loss: 92.5322\n",
      "Epoch 1077/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 51.3051 - val_loss: 91.9769\n",
      "Epoch 1078/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 52.1766 - val_loss: 91.6646\n",
      "Epoch 1079/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 53.1908 - val_loss: 93.5283\n",
      "Epoch 1080/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 52.3766 - val_loss: 92.5141\n",
      "Epoch 1081/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 52.5557 - val_loss: 94.5706\n",
      "Epoch 1082/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 51.9295 - val_loss: 88.4044\n",
      "Epoch 1083/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 50.9168 - val_loss: 89.5677\n",
      "Epoch 1084/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 50.4308 - val_loss: 90.3149\n",
      "Epoch 1085/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 51.9235 - val_loss: 90.5604\n",
      "Epoch 1086/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 51.6482 - val_loss: 90.5211\n",
      "Epoch 1087/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 50.9188 - val_loss: 89.7994\n",
      "Epoch 1088/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 50.7155 - val_loss: 88.2047\n",
      "Epoch 1089/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 50.6771 - val_loss: 91.7292\n",
      "Epoch 1090/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 50.8797 - val_loss: 89.3338\n",
      "Epoch 1091/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 50.7384 - val_loss: 91.0212\n",
      "Epoch 1092/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 51.1399 - val_loss: 91.4601\n",
      "Epoch 1093/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 51.9071 - val_loss: 89.5432\n",
      "Epoch 1094/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 50.4810 - val_loss: 91.4273\n",
      "Epoch 1095/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 50.5070 - val_loss: 89.5756\n",
      "Epoch 1096/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 50.6549 - val_loss: 90.6629\n",
      "Epoch 1097/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 51.1148 - val_loss: 89.3555\n",
      "Epoch 1098/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 52.2169 - val_loss: 90.9531\n",
      "Epoch 1099/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 51.6670 - val_loss: 90.0275\n",
      "Epoch 1100/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 51.3161 - val_loss: 90.4272\n",
      "Epoch 1101/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 51.9377 - val_loss: 90.4551\n",
      "Epoch 1102/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 50.9100 - val_loss: 90.9408\n",
      "Epoch 1103/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 50.8672 - val_loss: 90.9024\n",
      "Epoch 1104/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 50.5564 - val_loss: 90.2057\n",
      "Epoch 1105/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 50.1205 - val_loss: 89.1124\n",
      "Epoch 1106/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 49.8292 - val_loss: 92.0814\n",
      "Epoch 1107/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 50.3621 - val_loss: 91.5602\n",
      "Epoch 1108/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 50.5033 - val_loss: 90.2784\n",
      "Epoch 1109/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 51.4698 - val_loss: 94.3529\n",
      "Epoch 1110/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 52.8959 - val_loss: 92.6119\n",
      "Epoch 1111/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 51.4975 - val_loss: 93.7207\n",
      "Epoch 1112/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 52.3693 - val_loss: 91.7184\n",
      "Epoch 1113/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 51.1870 - val_loss: 95.4307\n",
      "Epoch 1114/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 51.5163 - val_loss: 91.8648\n",
      "Epoch 1115/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 51.0238 - val_loss: 91.2894\n",
      "Epoch 1116/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 50.2222 - val_loss: 89.3538\n",
      "Epoch 1117/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 49.8956 - val_loss: 90.3874\n",
      "Epoch 1118/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 49.9609 - val_loss: 91.9570\n",
      "Epoch 1119/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 50.7553 - val_loss: 91.3087\n",
      "Epoch 1120/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 50.7763 - val_loss: 90.4101\n",
      "Epoch 1121/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 50.9798 - val_loss: 90.5183\n",
      "Epoch 1122/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 50.9541 - val_loss: 90.2691\n",
      "Epoch 1123/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 51.3462 - val_loss: 91.3275\n",
      "Epoch 1124/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 50.5068 - val_loss: 90.9120\n",
      "Epoch 1125/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 50.3591 - val_loss: 92.7207\n",
      "Epoch 1126/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 50.6000 - val_loss: 90.8800\n",
      "Epoch 1127/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 50.4904 - val_loss: 89.2135\n",
      "Epoch 1128/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 50.2918 - val_loss: 90.8434\n",
      "Epoch 1129/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 50.2038 - val_loss: 89.7141\n",
      "Epoch 1130/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 49.8863 - val_loss: 90.0242\n",
      "Epoch 1131/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 50.7342 - val_loss: 90.6185\n",
      "Epoch 1132/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 50.2635 - val_loss: 89.5152\n",
      "Epoch 1133/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 49.4651 - val_loss: 91.4171\n",
      "Epoch 1134/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 50.0436 - val_loss: 89.9246\n",
      "Epoch 1135/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 50.2384 - val_loss: 92.8020\n",
      "Epoch 1136/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 51.5565 - val_loss: 92.2083\n",
      "Epoch 1137/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 51.2894 - val_loss: 92.6672\n",
      "Epoch 1138/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 50.8603 - val_loss: 90.7440\n",
      "Epoch 1139/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 51.0048 - val_loss: 94.2588\n",
      "Epoch 1140/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 51.3923 - val_loss: 90.2461\n",
      "Epoch 1141/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 50.6707 - val_loss: 90.2992\n",
      "Epoch 1142/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 50.5587 - val_loss: 93.1359\n",
      "Epoch 1143/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 50.2437 - val_loss: 89.2272\n",
      "Epoch 1144/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 49.6297 - val_loss: 91.0511\n",
      "Epoch 1145/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 49.6569 - val_loss: 91.6526\n",
      "Epoch 1146/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 50.0538 - val_loss: 91.7362\n",
      "Epoch 1147/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 49.7299 - val_loss: 92.0607\n",
      "Epoch 1148/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 50.3257 - val_loss: 89.2969\n",
      "Epoch 1149/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 51.0416 - val_loss: 91.6220\n",
      "Epoch 1150/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 50.1351 - val_loss: 88.1499\n",
      "Epoch 1151/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 49.4874 - val_loss: 90.8565\n",
      "Epoch 1152/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 50.5619 - val_loss: 89.8813\n",
      "Epoch 1153/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 51.1856 - val_loss: 91.7795\n",
      "Epoch 1154/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 52.3816 - val_loss: 90.3148\n",
      "Epoch 1155/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 51.8794 - val_loss: 90.9478\n",
      "Epoch 1156/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 51.0742 - val_loss: 90.1099\n",
      "Epoch 1157/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 50.4375 - val_loss: 89.0940\n",
      "Epoch 1158/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 50.1267 - val_loss: 91.1380\n",
      "Epoch 1159/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 50.0745 - val_loss: 90.0087\n",
      "Epoch 1160/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 50.1913 - val_loss: 91.6554\n",
      "Epoch 1161/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 50.6424 - val_loss: 91.5956\n",
      "Epoch 1162/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 49.8955 - val_loss: 89.8997\n",
      "Epoch 1163/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 49.7966 - val_loss: 89.9963\n",
      "Epoch 1164/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 49.9783 - val_loss: 89.5247\n",
      "Epoch 1165/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 50.6117 - val_loss: 91.2360\n",
      "Epoch 1166/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 50.5356 - val_loss: 91.6100\n",
      "Epoch 1167/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 50.9583 - val_loss: 90.0115\n",
      "Epoch 1168/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 50.6814 - val_loss: 88.5729\n",
      "Epoch 1169/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 51.1885 - val_loss: 90.9017\n",
      "Epoch 1170/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 51.5234 - val_loss: 93.9705\n",
      "Epoch 1171/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 51.3400 - val_loss: 91.4943\n",
      "Epoch 1172/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 50.1852 - val_loss: 90.5030\n",
      "Epoch 1173/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 51.8496 - val_loss: 91.6170\n",
      "Epoch 1174/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 51.6917 - val_loss: 89.7926\n",
      "Epoch 1175/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 50.9721 - val_loss: 88.9112\n",
      "Epoch 1176/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 50.1032 - val_loss: 91.1033\n",
      "Epoch 1177/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 49.6133 - val_loss: 89.5561\n",
      "Epoch 1178/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 49.1748 - val_loss: 90.7307\n",
      "Epoch 1179/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 48.8979 - val_loss: 88.3065\n",
      "Epoch 1180/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 49.1690 - val_loss: 88.7415\n",
      "Epoch 1181/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 49.2195 - val_loss: 91.3667\n",
      "Epoch 1182/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 49.6908 - val_loss: 90.3504\n",
      "Epoch 1183/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 49.7794 - val_loss: 89.8803\n",
      "Epoch 1184/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 49.8502 - val_loss: 90.0006\n",
      "Epoch 1185/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 50.1123 - val_loss: 92.8712\n",
      "Epoch 1186/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 49.6213 - val_loss: 90.4368\n",
      "Epoch 1187/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 49.9399 - val_loss: 90.7714\n",
      "Epoch 1188/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 50.4542 - val_loss: 91.3378\n",
      "Epoch 1189/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 49.3563 - val_loss: 88.0028\n",
      "Epoch 1190/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 49.4655 - val_loss: 90.5413\n",
      "Epoch 1191/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 49.2510 - val_loss: 89.7544\n",
      "Epoch 1192/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 49.4778 - val_loss: 91.6171\n",
      "Epoch 1193/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 49.7538 - val_loss: 91.2569\n",
      "Epoch 1194/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 50.0025 - val_loss: 91.7457\n",
      "Epoch 1195/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 50.3166 - val_loss: 92.2944\n",
      "Epoch 1196/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 50.2976 - val_loss: 93.6760\n",
      "Epoch 1197/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 51.1006 - val_loss: 91.3252\n",
      "Epoch 1198/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 51.8519 - val_loss: 93.2516\n",
      "Epoch 1199/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 51.6126 - val_loss: 91.1817\n",
      "Epoch 1200/2000\n",
      "6/6 [==============================] - ETA: 0s - loss: 52.1984\n",
      "Epoch 01200: saving model to saved_models/latent2/cp-1200.h5\n",
      "6/6 [==============================] - 1s 187ms/step - loss: 52.1984 - val_loss: 89.6810\n",
      "Epoch 1201/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 50.7824 - val_loss: 94.3351\n",
      "Epoch 1202/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 50.2407 - val_loss: 93.4283\n",
      "Epoch 1203/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 50.2141 - val_loss: 92.5442\n",
      "Epoch 1204/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 51.3182 - val_loss: 91.1024\n",
      "Epoch 1205/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 49.6194 - val_loss: 89.9853\n",
      "Epoch 1206/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 49.7597 - val_loss: 89.7235\n",
      "Epoch 1207/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 49.5249 - val_loss: 92.8786\n",
      "Epoch 1208/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 50.0160 - val_loss: 90.9723\n",
      "Epoch 1209/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 49.6144 - val_loss: 91.6014\n",
      "Epoch 1210/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 49.7186 - val_loss: 93.1705\n",
      "Epoch 1211/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 50.4398 - val_loss: 92.8254\n",
      "Epoch 1212/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 50.2640 - val_loss: 91.1784\n",
      "Epoch 1213/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 49.9430 - val_loss: 89.3148\n",
      "Epoch 1214/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 49.7802 - val_loss: 90.1881\n",
      "Epoch 1215/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 49.2141 - val_loss: 90.7413\n",
      "Epoch 1216/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 49.2161 - val_loss: 89.7281\n",
      "Epoch 1217/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 49.5556 - val_loss: 89.5362\n",
      "Epoch 1218/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 49.5811 - val_loss: 89.4335\n",
      "Epoch 1219/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 49.4496 - val_loss: 94.3197\n",
      "Epoch 1220/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 49.6125 - val_loss: 89.6896\n",
      "Epoch 1221/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 49.7743 - val_loss: 91.3351\n",
      "Epoch 1222/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 49.9994 - val_loss: 91.6654\n",
      "Epoch 1223/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 49.7788 - val_loss: 94.4470\n",
      "Epoch 1224/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 51.4581 - val_loss: 95.4577\n",
      "Epoch 1225/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 52.6727 - val_loss: 94.8058\n",
      "Epoch 1226/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 51.5580 - val_loss: 97.3642\n",
      "Epoch 1227/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 51.7967 - val_loss: 92.5197\n",
      "Epoch 1228/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 51.4393 - val_loss: 95.7197\n",
      "Epoch 1229/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 52.0624 - val_loss: 94.4634\n",
      "Epoch 1230/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 51.5216 - val_loss: 90.8622\n",
      "Epoch 1231/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 49.8824 - val_loss: 90.4671\n",
      "Epoch 1232/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 49.6431 - val_loss: 89.2141\n",
      "Epoch 1233/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 49.6660 - val_loss: 90.1073\n",
      "Epoch 1234/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 49.9062 - val_loss: 93.5839\n",
      "Epoch 1235/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 50.4404 - val_loss: 90.8550\n",
      "Epoch 1236/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 49.7493 - val_loss: 88.8934\n",
      "Epoch 1237/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 49.3247 - val_loss: 89.0222\n",
      "Epoch 1238/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 48.9943 - val_loss: 92.8405\n",
      "Epoch 1239/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 49.3220 - val_loss: 90.9436\n",
      "Epoch 1240/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 49.9652 - val_loss: 88.8825\n",
      "Epoch 1241/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 49.9679 - val_loss: 89.9266\n",
      "Epoch 1242/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 50.1902 - val_loss: 90.5197\n",
      "Epoch 1243/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 50.9861 - val_loss: 91.6256\n",
      "Epoch 1244/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 49.0531 - val_loss: 88.8562\n",
      "Epoch 1245/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 48.6440 - val_loss: 89.4120\n",
      "Epoch 1246/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 48.5115 - val_loss: 89.4864\n",
      "Epoch 1247/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 48.2440 - val_loss: 89.9516\n",
      "Epoch 1248/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 48.6193 - val_loss: 90.1205\n",
      "Epoch 1249/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 48.3979 - val_loss: 90.5224\n",
      "Epoch 1250/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 48.6326 - val_loss: 89.9879\n",
      "Epoch 1251/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 48.9893 - val_loss: 89.9325\n",
      "Epoch 1252/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 49.0880 - val_loss: 89.3847\n",
      "Epoch 1253/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 48.3774 - val_loss: 90.7728\n",
      "Epoch 1254/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 48.2031 - val_loss: 90.1768\n",
      "Epoch 1255/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 49.0049 - val_loss: 89.3853\n",
      "Epoch 1256/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 49.0874 - val_loss: 92.0899\n",
      "Epoch 1257/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 49.6523 - val_loss: 91.8236\n",
      "Epoch 1258/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 49.3613 - val_loss: 91.1722\n",
      "Epoch 1259/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 49.5531 - val_loss: 94.8260\n",
      "Epoch 1260/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 50.9565 - val_loss: 91.5261\n",
      "Epoch 1261/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 50.1247 - val_loss: 88.9068\n",
      "Epoch 1262/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 49.4391 - val_loss: 89.6633\n",
      "Epoch 1263/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 48.9758 - val_loss: 91.3921\n",
      "Epoch 1264/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 48.8861 - val_loss: 88.7514\n",
      "Epoch 1265/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 49.6264 - val_loss: 92.3633\n",
      "Epoch 1266/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 49.9570 - val_loss: 90.9504\n",
      "Epoch 1267/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 48.6519 - val_loss: 90.5458\n",
      "Epoch 1268/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 49.4196 - val_loss: 89.3817\n",
      "Epoch 1269/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 48.7796 - val_loss: 88.7968\n",
      "Epoch 1270/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 49.1074 - val_loss: 92.3259\n",
      "Epoch 1271/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 49.6628 - val_loss: 88.7582\n",
      "Epoch 1272/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 49.7775 - val_loss: 92.6760\n",
      "Epoch 1273/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 49.1153 - val_loss: 91.3037\n",
      "Epoch 1274/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 49.7419 - val_loss: 91.5994\n",
      "Epoch 1275/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 50.0080 - val_loss: 91.6754\n",
      "Epoch 1276/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 49.7879 - val_loss: 90.3541\n",
      "Epoch 1277/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 48.7313 - val_loss: 90.4797\n",
      "Epoch 1278/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 49.0924 - val_loss: 91.8189\n",
      "Epoch 1279/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 48.8192 - val_loss: 89.2994\n",
      "Epoch 1280/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 48.8094 - val_loss: 90.2600\n",
      "Epoch 1281/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 48.4780 - val_loss: 92.8108\n",
      "Epoch 1282/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 48.8629 - val_loss: 89.3631\n",
      "Epoch 1283/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 49.3259 - val_loss: 91.8577\n",
      "Epoch 1284/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 48.8586 - val_loss: 89.2585\n",
      "Epoch 1285/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 50.3924 - val_loss: 88.3116\n",
      "Epoch 1286/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 51.1876 - val_loss: 92.5862\n",
      "Epoch 1287/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 49.9905 - val_loss: 90.6637\n",
      "Epoch 1288/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 50.1981 - val_loss: 91.3848\n",
      "Epoch 1289/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 49.1711 - val_loss: 92.3794\n",
      "Epoch 1290/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 49.1249 - val_loss: 93.1839\n",
      "Epoch 1291/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 49.2446 - val_loss: 92.8729\n",
      "Epoch 1292/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 49.1309 - val_loss: 91.7464\n",
      "Epoch 1293/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 49.3078 - val_loss: 91.0176\n",
      "Epoch 1294/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 48.5329 - val_loss: 91.1021\n",
      "Epoch 1295/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 48.7463 - val_loss: 89.1951\n",
      "Epoch 1296/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 48.3399 - val_loss: 90.7876\n",
      "Epoch 1297/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 48.1201 - val_loss: 90.5926\n",
      "Epoch 1298/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 48.3160 - val_loss: 91.3048\n",
      "Epoch 1299/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 48.3529 - val_loss: 88.8636\n",
      "Epoch 1300/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 48.2059 - val_loss: 91.0388\n",
      "Epoch 1301/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 48.8973 - val_loss: 92.7696\n",
      "Epoch 1302/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 48.3628 - val_loss: 91.4133\n",
      "Epoch 1303/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 48.5466 - val_loss: 88.7620\n",
      "Epoch 1304/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 48.8346 - val_loss: 91.5982\n",
      "Epoch 1305/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 48.5787 - val_loss: 91.6427\n",
      "Epoch 1306/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 49.0921 - val_loss: 93.4649\n",
      "Epoch 1307/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 50.1668 - val_loss: 90.3627\n",
      "Epoch 1308/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 48.7890 - val_loss: 88.6860\n",
      "Epoch 1309/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 48.6081 - val_loss: 93.2416\n",
      "Epoch 1310/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 49.1711 - val_loss: 91.6348\n",
      "Epoch 1311/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 49.9554 - val_loss: 93.6388\n",
      "Epoch 1312/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 51.2378 - val_loss: 95.4677\n",
      "Epoch 1313/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 52.8528 - val_loss: 94.8928\n",
      "Epoch 1314/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 52.1708 - val_loss: 92.2117\n",
      "Epoch 1315/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 50.3558 - val_loss: 89.4230\n",
      "Epoch 1316/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 50.6132 - val_loss: 90.7145\n",
      "Epoch 1317/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 50.4008 - val_loss: 91.9736\n",
      "Epoch 1318/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 49.2912 - val_loss: 89.5727\n",
      "Epoch 1319/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 48.7838 - val_loss: 88.3558\n",
      "Epoch 1320/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 48.4996 - val_loss: 90.9872\n",
      "Epoch 1321/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 48.4541 - val_loss: 89.2603\n",
      "Epoch 1322/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 48.3088 - val_loss: 89.3654\n",
      "Epoch 1323/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 47.9956 - val_loss: 87.3803\n",
      "Epoch 1324/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 48.3502 - val_loss: 91.3269\n",
      "Epoch 1325/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 47.4243 - val_loss: 91.5369\n",
      "Epoch 1326/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 48.7036 - val_loss: 89.3194\n",
      "Epoch 1327/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 48.2102 - val_loss: 90.5370\n",
      "Epoch 1328/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 48.3847 - val_loss: 90.4077\n",
      "Epoch 1329/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 47.9579 - val_loss: 91.5313\n",
      "Epoch 1330/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 48.8763 - val_loss: 89.2166\n",
      "Epoch 1331/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 48.0494 - val_loss: 89.7809\n",
      "Epoch 1332/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 48.3859 - val_loss: 89.9408\n",
      "Epoch 1333/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 49.3240 - val_loss: 91.4986\n",
      "Epoch 1334/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 49.1553 - val_loss: 88.7274\n",
      "Epoch 1335/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 48.4169 - val_loss: 90.7442\n",
      "Epoch 1336/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 48.1577 - val_loss: 89.3432\n",
      "Epoch 1337/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 47.9543 - val_loss: 89.0152\n",
      "Epoch 1338/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 48.8163 - val_loss: 90.5489\n",
      "Epoch 1339/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 49.0431 - val_loss: 90.5856\n",
      "Epoch 1340/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 49.0458 - val_loss: 90.4900\n",
      "Epoch 1341/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 48.8000 - val_loss: 90.1968\n",
      "Epoch 1342/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 48.2008 - val_loss: 90.4529\n",
      "Epoch 1343/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 48.5266 - val_loss: 88.7756\n",
      "Epoch 1344/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 48.4908 - val_loss: 91.2117\n",
      "Epoch 1345/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 47.9663 - val_loss: 88.2950\n",
      "Epoch 1346/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 47.2983 - val_loss: 89.9116\n",
      "Epoch 1347/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 47.9504 - val_loss: 91.7036\n",
      "Epoch 1348/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 47.7520 - val_loss: 90.1352\n",
      "Epoch 1349/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 48.0437 - val_loss: 90.7068\n",
      "Epoch 1350/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 47.6578 - val_loss: 90.6102\n",
      "Epoch 1351/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 48.9589 - val_loss: 89.4580\n",
      "Epoch 1352/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 48.3317 - val_loss: 90.4458\n",
      "Epoch 1353/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 47.8041 - val_loss: 90.0071\n",
      "Epoch 1354/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 48.0886 - val_loss: 89.3426\n",
      "Epoch 1355/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 48.0715 - val_loss: 89.2940\n",
      "Epoch 1356/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 48.4419 - val_loss: 89.1650\n",
      "Epoch 1357/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 48.7962 - val_loss: 90.6048\n",
      "Epoch 1358/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 48.4443 - val_loss: 91.9843\n",
      "Epoch 1359/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 48.6192 - val_loss: 90.9521\n",
      "Epoch 1360/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 48.0051 - val_loss: 90.9448\n",
      "Epoch 1361/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 48.6857 - val_loss: 89.4744\n",
      "Epoch 1362/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 49.2245 - val_loss: 90.6007\n",
      "Epoch 1363/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 49.8162 - val_loss: 92.2594\n",
      "Epoch 1364/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 49.3947 - val_loss: 94.7974\n",
      "Epoch 1365/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 48.7941 - val_loss: 90.3400\n",
      "Epoch 1366/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 49.6119 - val_loss: 88.9828\n",
      "Epoch 1367/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 49.7049 - val_loss: 95.6045\n",
      "Epoch 1368/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 51.8086 - val_loss: 90.6887\n",
      "Epoch 1369/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 50.5090 - val_loss: 91.0739\n",
      "Epoch 1370/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 50.0016 - val_loss: 93.4154\n",
      "Epoch 1371/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 49.6630 - val_loss: 89.0625\n",
      "Epoch 1372/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 49.7649 - val_loss: 90.1274\n",
      "Epoch 1373/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 49.0894 - val_loss: 91.8094\n",
      "Epoch 1374/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 48.9540 - val_loss: 90.5286\n",
      "Epoch 1375/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 48.6490 - val_loss: 89.3524\n",
      "Epoch 1376/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 48.1923 - val_loss: 90.6364\n",
      "Epoch 1377/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 48.8699 - val_loss: 92.5883\n",
      "Epoch 1378/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 49.4151 - val_loss: 89.1516\n",
      "Epoch 1379/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 49.5248 - val_loss: 89.7598\n",
      "Epoch 1380/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 48.1024 - val_loss: 90.7163\n",
      "Epoch 1381/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 47.7866 - val_loss: 90.2122\n",
      "Epoch 1382/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 47.4653 - val_loss: 91.3223\n",
      "Epoch 1383/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 48.4398 - val_loss: 90.8108\n",
      "Epoch 1384/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 47.7710 - val_loss: 93.0760\n",
      "Epoch 1385/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 47.6209 - val_loss: 91.4507\n",
      "Epoch 1386/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 49.0987 - val_loss: 90.3180\n",
      "Epoch 1387/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 49.1063 - val_loss: 91.3494\n",
      "Epoch 1388/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 48.3306 - val_loss: 90.4100\n",
      "Epoch 1389/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 47.9611 - val_loss: 90.0158\n",
      "Epoch 1390/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 47.8780 - val_loss: 90.6411\n",
      "Epoch 1391/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 47.8472 - val_loss: 88.0961\n",
      "Epoch 1392/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 47.3116 - val_loss: 91.8302\n",
      "Epoch 1393/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 47.8798 - val_loss: 90.1148\n",
      "Epoch 1394/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 47.3562 - val_loss: 91.2775\n",
      "Epoch 1395/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 46.6498 - val_loss: 89.4323\n",
      "Epoch 1396/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 47.6006 - val_loss: 91.5161\n",
      "Epoch 1397/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 48.0242 - val_loss: 88.9880\n",
      "Epoch 1398/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 48.4220 - val_loss: 91.0046\n",
      "Epoch 1399/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 48.3129 - val_loss: 91.2698\n",
      "Epoch 1400/2000\n",
      "6/6 [==============================] - ETA: 0s - loss: 48.9148\n",
      "Epoch 01400: saving model to saved_models/latent2/cp-1400.h5\n",
      "6/6 [==============================] - 1s 159ms/step - loss: 48.9148 - val_loss: 91.6103\n",
      "Epoch 1401/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 48.0262 - val_loss: 90.6614\n",
      "Epoch 1402/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 47.4544 - val_loss: 88.1830\n",
      "Epoch 1403/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 48.4357 - val_loss: 88.3576\n",
      "Epoch 1404/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 47.6817 - val_loss: 91.4368\n",
      "Epoch 1405/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 47.2563 - val_loss: 90.4630\n",
      "Epoch 1406/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 47.0745 - val_loss: 91.1493\n",
      "Epoch 1407/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 47.8259 - val_loss: 91.2476\n",
      "Epoch 1408/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 47.9869 - val_loss: 89.8885\n",
      "Epoch 1409/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 47.5898 - val_loss: 91.9067\n",
      "Epoch 1410/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 48.2254 - val_loss: 90.3128\n",
      "Epoch 1411/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 47.7666 - val_loss: 91.5479\n",
      "Epoch 1412/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 47.6734 - val_loss: 92.2456\n",
      "Epoch 1413/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 48.9131 - val_loss: 91.3419\n",
      "Epoch 1414/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 47.3978 - val_loss: 92.0001\n",
      "Epoch 1415/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 48.3064 - val_loss: 89.8065\n",
      "Epoch 1416/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 47.6933 - val_loss: 90.6152\n",
      "Epoch 1417/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 47.3243 - val_loss: 89.0175\n",
      "Epoch 1418/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 47.4645 - val_loss: 90.5699\n",
      "Epoch 1419/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 47.1979 - val_loss: 89.5592\n",
      "Epoch 1420/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 48.3480 - val_loss: 90.5104\n",
      "Epoch 1421/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 48.2214 - val_loss: 90.4174\n",
      "Epoch 1422/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 48.1655 - val_loss: 92.2799\n",
      "Epoch 1423/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 48.4748 - val_loss: 90.9292\n",
      "Epoch 1424/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 48.4432 - val_loss: 92.4412\n",
      "Epoch 1425/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 48.4170 - val_loss: 87.1661\n",
      "Epoch 1426/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.9120 - val_loss: 89.0039\n",
      "Epoch 1427/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 47.7374 - val_loss: 91.7744\n",
      "Epoch 1428/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 47.9508 - val_loss: 89.5374\n",
      "Epoch 1429/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 48.3827 - val_loss: 91.0990\n",
      "Epoch 1430/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 47.8344 - val_loss: 91.4506\n",
      "Epoch 1431/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 47.6334 - val_loss: 87.5269\n",
      "Epoch 1432/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.8546 - val_loss: 91.3575\n",
      "Epoch 1433/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 46.5951 - val_loss: 90.1961\n",
      "Epoch 1434/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 47.0340 - val_loss: 91.1407\n",
      "Epoch 1435/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 47.4973 - val_loss: 91.4203\n",
      "Epoch 1436/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 47.8339 - val_loss: 92.4901\n",
      "Epoch 1437/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 49.4513 - val_loss: 89.4157\n",
      "Epoch 1438/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 50.0089 - val_loss: 89.8990\n",
      "Epoch 1439/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 48.8038 - val_loss: 92.8290\n",
      "Epoch 1440/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 49.2933 - val_loss: 90.9358\n",
      "Epoch 1441/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 48.6182 - val_loss: 90.5311\n",
      "Epoch 1442/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 48.0553 - val_loss: 93.0749\n",
      "Epoch 1443/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 48.2536 - val_loss: 90.0351\n",
      "Epoch 1444/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 47.9261 - val_loss: 90.6434\n",
      "Epoch 1445/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 48.2487 - val_loss: 89.3237\n",
      "Epoch 1446/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 47.2134 - val_loss: 90.7256\n",
      "Epoch 1447/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 48.2025 - val_loss: 89.7630\n",
      "Epoch 1448/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 47.4892 - val_loss: 90.3832\n",
      "Epoch 1449/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 47.7889 - val_loss: 90.6637\n",
      "Epoch 1450/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 47.3996 - val_loss: 91.0836\n",
      "Epoch 1451/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 48.4973 - val_loss: 91.8630\n",
      "Epoch 1452/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 47.7607 - val_loss: 92.0306\n",
      "Epoch 1453/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 48.4296 - val_loss: 89.9928\n",
      "Epoch 1454/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 47.4899 - val_loss: 92.2339\n",
      "Epoch 1455/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 47.4706 - val_loss: 88.4371\n",
      "Epoch 1456/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 47.7371 - val_loss: 93.5391\n",
      "Epoch 1457/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 48.2082 - val_loss: 89.1838\n",
      "Epoch 1458/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 47.1865 - val_loss: 89.8199\n",
      "Epoch 1459/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 47.0429 - val_loss: 89.9514\n",
      "Epoch 1460/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 47.0489 - val_loss: 90.4763\n",
      "Epoch 1461/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 47.0948 - val_loss: 90.2921\n",
      "Epoch 1462/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 46.5107 - val_loss: 89.0484\n",
      "Epoch 1463/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.8861 - val_loss: 89.7290\n",
      "Epoch 1464/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 46.6342 - val_loss: 89.5152\n",
      "Epoch 1465/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 47.1967 - val_loss: 90.5628\n",
      "Epoch 1466/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 47.7652 - val_loss: 93.7810\n",
      "Epoch 1467/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 48.5689 - val_loss: 90.6928\n",
      "Epoch 1468/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 49.1387 - val_loss: 89.7038\n",
      "Epoch 1469/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 48.2690 - val_loss: 94.2926\n",
      "Epoch 1470/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 48.5419 - val_loss: 91.6695\n",
      "Epoch 1471/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 49.4390 - val_loss: 91.9635\n",
      "Epoch 1472/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 48.3852 - val_loss: 92.2758\n",
      "Epoch 1473/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 48.4767 - val_loss: 88.4936\n",
      "Epoch 1474/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 48.1127 - val_loss: 91.4610\n",
      "Epoch 1475/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 48.4692 - val_loss: 92.3583\n",
      "Epoch 1476/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 47.9803 - val_loss: 90.2908\n",
      "Epoch 1477/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 47.8269 - val_loss: 93.0902\n",
      "Epoch 1478/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 47.9984 - val_loss: 91.0775\n",
      "Epoch 1479/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 47.9061 - val_loss: 90.3397\n",
      "Epoch 1480/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 47.2764 - val_loss: 90.9026\n",
      "Epoch 1481/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 47.4931 - val_loss: 90.9885\n",
      "Epoch 1482/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 47.1171 - val_loss: 91.2324\n",
      "Epoch 1483/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 47.0349 - val_loss: 90.9050\n",
      "Epoch 1484/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.7251 - val_loss: 90.9655\n",
      "Epoch 1485/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 47.9361 - val_loss: 91.4543\n",
      "Epoch 1486/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 48.9215 - val_loss: 88.2372\n",
      "Epoch 1487/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 48.4666 - val_loss: 89.0814\n",
      "Epoch 1488/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 47.6677 - val_loss: 88.9907\n",
      "Epoch 1489/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 47.1237 - val_loss: 91.1256\n",
      "Epoch 1490/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 47.2251 - val_loss: 90.3050\n",
      "Epoch 1491/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.9691 - val_loss: 90.7858\n",
      "Epoch 1492/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 47.2349 - val_loss: 92.9083\n",
      "Epoch 1493/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 47.0344 - val_loss: 89.1081\n",
      "Epoch 1494/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.9454 - val_loss: 91.8431\n",
      "Epoch 1495/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 47.1463 - val_loss: 92.3330\n",
      "Epoch 1496/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 47.4941 - val_loss: 90.1997\n",
      "Epoch 1497/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 47.3948 - val_loss: 90.6410\n",
      "Epoch 1498/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 47.4878 - val_loss: 92.0426\n",
      "Epoch 1499/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 47.3050 - val_loss: 91.5267\n",
      "Epoch 1500/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 47.1233 - val_loss: 90.4385\n",
      "Epoch 1501/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 47.5598 - val_loss: 89.5338\n",
      "Epoch 1502/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 47.3815 - val_loss: 91.3254\n",
      "Epoch 1503/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 47.5880 - val_loss: 92.5210\n",
      "Epoch 1504/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 47.6830 - val_loss: 92.0389\n",
      "Epoch 1505/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 47.1687 - val_loss: 90.3415\n",
      "Epoch 1506/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 47.1623 - val_loss: 90.0817\n",
      "Epoch 1507/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 47.3943 - val_loss: 90.5312\n",
      "Epoch 1508/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 47.3401 - val_loss: 90.4922\n",
      "Epoch 1509/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 47.0497 - val_loss: 90.1094\n",
      "Epoch 1510/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 47.3557 - val_loss: 89.1740\n",
      "Epoch 1511/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 48.5164 - val_loss: 92.4229\n",
      "Epoch 1512/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 47.3794 - val_loss: 89.4273\n",
      "Epoch 1513/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 47.4601 - val_loss: 91.3495\n",
      "Epoch 1514/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.9722 - val_loss: 90.8305\n",
      "Epoch 1515/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 47.4474 - val_loss: 89.4563\n",
      "Epoch 1516/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.7367 - val_loss: 90.4311\n",
      "Epoch 1517/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.5230 - val_loss: 93.0699\n",
      "Epoch 1518/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 48.9310 - val_loss: 90.9778\n",
      "Epoch 1519/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 48.7815 - val_loss: 91.6567\n",
      "Epoch 1520/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 48.0546 - val_loss: 91.3034\n",
      "Epoch 1521/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 47.5973 - val_loss: 90.7698\n",
      "Epoch 1522/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.5730 - val_loss: 88.6648\n",
      "Epoch 1523/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 46.8038 - val_loss: 91.4234\n",
      "Epoch 1524/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 46.7875 - val_loss: 89.4980\n",
      "Epoch 1525/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 48.1562 - val_loss: 93.8956\n",
      "Epoch 1526/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 47.5306 - val_loss: 90.9443\n",
      "Epoch 1527/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.7708 - val_loss: 90.2800\n",
      "Epoch 1528/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 46.0860 - val_loss: 90.7649\n",
      "Epoch 1529/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 46.4638 - val_loss: 94.7005\n",
      "Epoch 1530/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 48.3389 - val_loss: 93.0939\n",
      "Epoch 1531/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 47.9205 - val_loss: 91.9427\n",
      "Epoch 1532/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 47.6636 - val_loss: 90.7914\n",
      "Epoch 1533/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 47.7137 - val_loss: 91.4161\n",
      "Epoch 1534/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 47.6212 - val_loss: 89.8301\n",
      "Epoch 1535/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.5749 - val_loss: 92.5518\n",
      "Epoch 1536/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 47.2601 - val_loss: 91.7106\n",
      "Epoch 1537/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 47.3537 - val_loss: 89.9348\n",
      "Epoch 1538/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 47.6984 - val_loss: 91.9928\n",
      "Epoch 1539/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 47.2981 - val_loss: 91.0274\n",
      "Epoch 1540/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 47.5170 - val_loss: 91.6853\n",
      "Epoch 1541/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.5705 - val_loss: 91.9565\n",
      "Epoch 1542/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 47.4488 - val_loss: 92.9194\n",
      "Epoch 1543/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 48.5845 - val_loss: 91.2272\n",
      "Epoch 1544/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 47.7541 - val_loss: 89.7264\n",
      "Epoch 1545/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.6885 - val_loss: 89.8190\n",
      "Epoch 1546/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.1520 - val_loss: 90.6150\n",
      "Epoch 1547/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 45.9393 - val_loss: 91.8922\n",
      "Epoch 1548/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 46.1346 - val_loss: 92.6757\n",
      "Epoch 1549/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.9016 - val_loss: 88.8684\n",
      "Epoch 1550/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.2991 - val_loss: 92.1312\n",
      "Epoch 1551/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 47.1365 - val_loss: 90.5951\n",
      "Epoch 1552/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.5127 - val_loss: 91.5297\n",
      "Epoch 1553/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 48.4085 - val_loss: 93.7893\n",
      "Epoch 1554/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 50.0692 - val_loss: 91.8030\n",
      "Epoch 1555/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 50.5954 - val_loss: 90.3110\n",
      "Epoch 1556/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 47.7841 - val_loss: 89.7895\n",
      "Epoch 1557/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 47.8394 - val_loss: 95.5216\n",
      "Epoch 1558/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 48.0427 - val_loss: 93.1369\n",
      "Epoch 1559/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 48.7506 - val_loss: 90.2717\n",
      "Epoch 1560/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 48.6490 - val_loss: 92.3256\n",
      "Epoch 1561/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 49.1351 - val_loss: 92.7656\n",
      "Epoch 1562/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 47.6086 - val_loss: 94.2440\n",
      "Epoch 1563/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 47.3860 - val_loss: 90.0912\n",
      "Epoch 1564/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.9659 - val_loss: 90.6182\n",
      "Epoch 1565/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 47.0642 - val_loss: 90.9571\n",
      "Epoch 1566/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.4737 - val_loss: 88.8226\n",
      "Epoch 1567/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.7905 - val_loss: 91.4594\n",
      "Epoch 1568/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.5187 - val_loss: 95.2968\n",
      "Epoch 1569/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 49.2628 - val_loss: 91.5078\n",
      "Epoch 1570/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 48.6033 - val_loss: 91.7907\n",
      "Epoch 1571/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 47.7901 - val_loss: 91.0742\n",
      "Epoch 1572/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.7030 - val_loss: 90.6493\n",
      "Epoch 1573/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.4323 - val_loss: 89.2156\n",
      "Epoch 1574/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.6797 - val_loss: 89.5369\n",
      "Epoch 1575/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 45.8776 - val_loss: 90.6415\n",
      "Epoch 1576/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 46.0439 - val_loss: 91.9843\n",
      "Epoch 1577/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.8840 - val_loss: 91.3617\n",
      "Epoch 1578/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 47.2255 - val_loss: 89.6181\n",
      "Epoch 1579/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.5599 - val_loss: 92.3444\n",
      "Epoch 1580/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.9584 - val_loss: 91.5498\n",
      "Epoch 1581/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.1544 - val_loss: 89.0816\n",
      "Epoch 1582/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.0124 - val_loss: 89.6802\n",
      "Epoch 1583/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 46.0528 - val_loss: 92.2434\n",
      "Epoch 1584/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.0244 - val_loss: 90.5534\n",
      "Epoch 1585/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.1817 - val_loss: 91.0952\n",
      "Epoch 1586/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.4147 - val_loss: 91.4970\n",
      "Epoch 1587/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 47.8213 - val_loss: 90.4908\n",
      "Epoch 1588/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 47.4918 - val_loss: 93.1468\n",
      "Epoch 1589/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 47.1026 - val_loss: 91.2790\n",
      "Epoch 1590/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 45.7393 - val_loss: 90.3753\n",
      "Epoch 1591/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.3571 - val_loss: 89.6008\n",
      "Epoch 1592/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 45.7607 - val_loss: 89.9537\n",
      "Epoch 1593/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.1696 - val_loss: 92.1565\n",
      "Epoch 1594/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.4934 - val_loss: 92.5015\n",
      "Epoch 1595/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 47.5435 - val_loss: 92.2643\n",
      "Epoch 1596/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.7154 - val_loss: 92.7607\n",
      "Epoch 1597/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 47.4933 - val_loss: 90.4944\n",
      "Epoch 1598/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 48.0222 - val_loss: 91.7339\n",
      "Epoch 1599/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.4541 - val_loss: 90.9534\n",
      "Epoch 1600/2000\n",
      "6/6 [==============================] - ETA: 0s - loss: 46.1531\n",
      "Epoch 01600: saving model to saved_models/latent2/cp-1600.h5\n",
      "6/6 [==============================] - 1s 153ms/step - loss: 46.1531 - val_loss: 93.3927\n",
      "Epoch 1601/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 47.3205 - val_loss: 91.8537\n",
      "Epoch 1602/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 45.9201 - val_loss: 90.9133\n",
      "Epoch 1603/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 45.6201 - val_loss: 90.7275\n",
      "Epoch 1604/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.3945 - val_loss: 90.6666\n",
      "Epoch 1605/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.1551 - val_loss: 94.7799\n",
      "Epoch 1606/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 46.3283 - val_loss: 91.3803\n",
      "Epoch 1607/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 45.9211 - val_loss: 90.1063\n",
      "Epoch 1608/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.0710 - val_loss: 91.5179\n",
      "Epoch 1609/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.7524 - val_loss: 91.4030\n",
      "Epoch 1610/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 47.3234 - val_loss: 90.2984\n",
      "Epoch 1611/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 47.3224 - val_loss: 92.0807\n",
      "Epoch 1612/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 47.0502 - val_loss: 92.5437\n",
      "Epoch 1613/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.4165 - val_loss: 91.1877\n",
      "Epoch 1614/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.4777 - val_loss: 90.6469\n",
      "Epoch 1615/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.2661 - val_loss: 90.7608\n",
      "Epoch 1616/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.0659 - val_loss: 92.0707\n",
      "Epoch 1617/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 46.3055 - val_loss: 89.8735\n",
      "Epoch 1618/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 45.9604 - val_loss: 89.8578\n",
      "Epoch 1619/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 45.9323 - val_loss: 91.0628\n",
      "Epoch 1620/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 45.8329 - val_loss: 90.7055\n",
      "Epoch 1621/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.5908 - val_loss: 91.1528\n",
      "Epoch 1622/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 45.6004 - val_loss: 90.7461\n",
      "Epoch 1623/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.1171 - val_loss: 91.2159\n",
      "Epoch 1624/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 45.1749 - val_loss: 90.3879\n",
      "Epoch 1625/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.8745 - val_loss: 93.5239\n",
      "Epoch 1626/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 48.3402 - val_loss: 91.6286\n",
      "Epoch 1627/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 47.0457 - val_loss: 90.2840\n",
      "Epoch 1628/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 45.6250 - val_loss: 91.0186\n",
      "Epoch 1629/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 45.7745 - val_loss: 92.3366\n",
      "Epoch 1630/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 46.3707 - val_loss: 91.5152\n",
      "Epoch 1631/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.9126 - val_loss: 91.4372\n",
      "Epoch 1632/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.3618 - val_loss: 92.7515\n",
      "Epoch 1633/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.4813 - val_loss: 91.1182\n",
      "Epoch 1634/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.5324 - val_loss: 91.5385\n",
      "Epoch 1635/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 45.8284 - val_loss: 89.2080\n",
      "Epoch 1636/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 45.9358 - val_loss: 91.4043\n",
      "Epoch 1637/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 45.6990 - val_loss: 91.6231\n",
      "Epoch 1638/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 45.7604 - val_loss: 92.3911\n",
      "Epoch 1639/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.1057 - val_loss: 93.4022\n",
      "Epoch 1640/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.6106 - val_loss: 92.0108\n",
      "Epoch 1641/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.2999 - val_loss: 92.1700\n",
      "Epoch 1642/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 45.4984 - val_loss: 91.0506\n",
      "Epoch 1643/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 45.6467 - val_loss: 90.2148\n",
      "Epoch 1644/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.0505 - val_loss: 89.9685\n",
      "Epoch 1645/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 45.8686 - val_loss: 92.0459\n",
      "Epoch 1646/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.2190 - val_loss: 93.9073\n",
      "Epoch 1647/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.3666 - val_loss: 93.1056\n",
      "Epoch 1648/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.9794 - val_loss: 93.1475\n",
      "Epoch 1649/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.4147 - val_loss: 92.0445\n",
      "Epoch 1650/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.0566 - val_loss: 90.6401\n",
      "Epoch 1651/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.2643 - val_loss: 91.1574\n",
      "Epoch 1652/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.3229 - val_loss: 93.7023\n",
      "Epoch 1653/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 45.7363 - val_loss: 92.5877\n",
      "Epoch 1654/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 45.7080 - val_loss: 90.3586\n",
      "Epoch 1655/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 45.7283 - val_loss: 90.5270\n",
      "Epoch 1656/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.3373 - val_loss: 92.5396\n",
      "Epoch 1657/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 46.4286 - val_loss: 93.7515\n",
      "Epoch 1658/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 47.4163 - val_loss: 93.9917\n",
      "Epoch 1659/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 48.0971 - val_loss: 91.9657\n",
      "Epoch 1660/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 47.7789 - val_loss: 90.7707\n",
      "Epoch 1661/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.3637 - val_loss: 93.2991\n",
      "Epoch 1662/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.4268 - val_loss: 88.9301\n",
      "Epoch 1663/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 45.7974 - val_loss: 91.4191\n",
      "Epoch 1664/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.0347 - val_loss: 90.3331\n",
      "Epoch 1665/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.2115 - val_loss: 90.5771\n",
      "Epoch 1666/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 45.0382 - val_loss: 90.6318\n",
      "Epoch 1667/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 45.0449 - val_loss: 89.0621\n",
      "Epoch 1668/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 45.4413 - val_loss: 92.2990\n",
      "Epoch 1669/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.2300 - val_loss: 90.7861\n",
      "Epoch 1670/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.8845 - val_loss: 93.0914\n",
      "Epoch 1671/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.1127 - val_loss: 91.8786\n",
      "Epoch 1672/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.0436 - val_loss: 93.2124\n",
      "Epoch 1673/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 47.3696 - val_loss: 89.7471\n",
      "Epoch 1674/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 47.4377 - val_loss: 95.8617\n",
      "Epoch 1675/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 47.0379 - val_loss: 95.2843\n",
      "Epoch 1676/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.9244 - val_loss: 90.0269\n",
      "Epoch 1677/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.5219 - val_loss: 92.5210\n",
      "Epoch 1678/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.6811 - val_loss: 90.0149\n",
      "Epoch 1679/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.0684 - val_loss: 95.7322\n",
      "Epoch 1680/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 47.6898 - val_loss: 93.3811\n",
      "Epoch 1681/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 48.5632 - val_loss: 91.8423\n",
      "Epoch 1682/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.3259 - val_loss: 91.4918\n",
      "Epoch 1683/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.0268 - val_loss: 92.2099\n",
      "Epoch 1684/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.2218 - val_loss: 91.9131\n",
      "Epoch 1685/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.0771 - val_loss: 90.1615\n",
      "Epoch 1686/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 45.7272 - val_loss: 90.8040\n",
      "Epoch 1687/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 45.7068 - val_loss: 91.0616\n",
      "Epoch 1688/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 45.0829 - val_loss: 92.5123\n",
      "Epoch 1689/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 45.0580 - val_loss: 91.2421\n",
      "Epoch 1690/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 45.0138 - val_loss: 91.0324\n",
      "Epoch 1691/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.4741 - val_loss: 92.9411\n",
      "Epoch 1692/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 45.7853 - val_loss: 94.4118\n",
      "Epoch 1693/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.6588 - val_loss: 93.2086\n",
      "Epoch 1694/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.6948 - val_loss: 93.6662\n",
      "Epoch 1695/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.2496 - val_loss: 92.4736\n",
      "Epoch 1696/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.2058 - val_loss: 92.6926\n",
      "Epoch 1697/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.7032 - val_loss: 91.1077\n",
      "Epoch 1698/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 47.8177 - val_loss: 94.0251\n",
      "Epoch 1699/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 47.6245 - val_loss: 96.2717\n",
      "Epoch 1700/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 47.0798 - val_loss: 91.3224\n",
      "Epoch 1701/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.1898 - val_loss: 92.2939\n",
      "Epoch 1702/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.3514 - val_loss: 91.9413\n",
      "Epoch 1703/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 47.1124 - val_loss: 95.5057\n",
      "Epoch 1704/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 48.0739 - val_loss: 92.2604\n",
      "Epoch 1705/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 48.2915 - val_loss: 92.4742\n",
      "Epoch 1706/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 47.8392 - val_loss: 95.3684\n",
      "Epoch 1707/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 47.9139 - val_loss: 92.1824\n",
      "Epoch 1708/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.5520 - val_loss: 90.5403\n",
      "Epoch 1709/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 45.6277 - val_loss: 92.3937\n",
      "Epoch 1710/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.1351 - val_loss: 89.5112\n",
      "Epoch 1711/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 45.6215 - val_loss: 91.0941\n",
      "Epoch 1712/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 45.4469 - val_loss: 92.8049\n",
      "Epoch 1713/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 45.4538 - val_loss: 91.0791\n",
      "Epoch 1714/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 45.9479 - val_loss: 89.7533\n",
      "Epoch 1715/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.1455 - val_loss: 91.9447\n",
      "Epoch 1716/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 45.8172 - val_loss: 92.4505\n",
      "Epoch 1717/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.5321 - val_loss: 93.5212\n",
      "Epoch 1718/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.3821 - val_loss: 90.6850\n",
      "Epoch 1719/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 45.2273 - val_loss: 94.9363\n",
      "Epoch 1720/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 44.7642 - val_loss: 90.7473\n",
      "Epoch 1721/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 44.5291 - val_loss: 91.3508\n",
      "Epoch 1722/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 45.2017 - val_loss: 94.4497\n",
      "Epoch 1723/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 45.1268 - val_loss: 92.4447\n",
      "Epoch 1724/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 45.8501 - val_loss: 92.3496\n",
      "Epoch 1725/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 45.4135 - val_loss: 90.9383\n",
      "Epoch 1726/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 45.1010 - val_loss: 90.9750\n",
      "Epoch 1727/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.0800 - val_loss: 93.6350\n",
      "Epoch 1728/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.8408 - val_loss: 90.0810\n",
      "Epoch 1729/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.3597 - val_loss: 93.5074\n",
      "Epoch 1730/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.0417 - val_loss: 92.1076\n",
      "Epoch 1731/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.6268 - val_loss: 92.5238\n",
      "Epoch 1732/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.5822 - val_loss: 91.7781\n",
      "Epoch 1733/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.1299 - val_loss: 91.7773\n",
      "Epoch 1734/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.1003 - val_loss: 90.8888\n",
      "Epoch 1735/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 46.4850 - val_loss: 97.0964\n",
      "Epoch 1736/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.9520 - val_loss: 93.7684\n",
      "Epoch 1737/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.7568 - val_loss: 90.0150\n",
      "Epoch 1738/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 45.2759 - val_loss: 91.4611\n",
      "Epoch 1739/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 45.0477 - val_loss: 94.6696\n",
      "Epoch 1740/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 47.0804 - val_loss: 94.1269\n",
      "Epoch 1741/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.1894 - val_loss: 91.4042\n",
      "Epoch 1742/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 47.0140 - val_loss: 92.2345\n",
      "Epoch 1743/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 48.8398 - val_loss: 97.6559\n",
      "Epoch 1744/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 49.4319 - val_loss: 95.2893\n",
      "Epoch 1745/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 47.9721 - val_loss: 92.7170\n",
      "Epoch 1746/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.3645 - val_loss: 93.1848\n",
      "Epoch 1747/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.8745 - val_loss: 90.5443\n",
      "Epoch 1748/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 47.0078 - val_loss: 93.5896\n",
      "Epoch 1749/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.0680 - val_loss: 93.2403\n",
      "Epoch 1750/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 45.7744 - val_loss: 92.6179\n",
      "Epoch 1751/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 45.8821 - val_loss: 91.0260\n",
      "Epoch 1752/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 45.8990 - val_loss: 95.2679\n",
      "Epoch 1753/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.2180 - val_loss: 93.1733\n",
      "Epoch 1754/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 45.5269 - val_loss: 90.2592\n",
      "Epoch 1755/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 45.8225 - val_loss: 91.3726\n",
      "Epoch 1756/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 45.9432 - val_loss: 89.8116\n",
      "Epoch 1757/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 46.5495 - val_loss: 96.1857\n",
      "Epoch 1758/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 47.6862 - val_loss: 90.1075\n",
      "Epoch 1759/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.3172 - val_loss: 92.1258\n",
      "Epoch 1760/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.3033 - val_loss: 91.4574\n",
      "Epoch 1761/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 45.7378 - val_loss: 91.7535\n",
      "Epoch 1762/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 45.9105 - val_loss: 94.5398\n",
      "Epoch 1763/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.6550 - val_loss: 92.8854\n",
      "Epoch 1764/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.5482 - val_loss: 90.7740\n",
      "Epoch 1765/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 45.6910 - val_loss: 90.3635\n",
      "Epoch 1766/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 45.5009 - val_loss: 93.4605\n",
      "Epoch 1767/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 45.5008 - val_loss: 94.1049\n",
      "Epoch 1768/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.3617 - val_loss: 92.9559\n",
      "Epoch 1769/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.5043 - val_loss: 90.5482\n",
      "Epoch 1770/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.6968 - val_loss: 91.4091\n",
      "Epoch 1771/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 47.7403 - val_loss: 93.7519\n",
      "Epoch 1772/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.2687 - val_loss: 90.3154\n",
      "Epoch 1773/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 44.9538 - val_loss: 92.5419\n",
      "Epoch 1774/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 45.2245 - val_loss: 91.8650\n",
      "Epoch 1775/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 45.6997 - val_loss: 95.8607\n",
      "Epoch 1776/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.6061 - val_loss: 92.0085\n",
      "Epoch 1777/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 45.2034 - val_loss: 92.8470\n",
      "Epoch 1778/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 45.0747 - val_loss: 94.5198\n",
      "Epoch 1779/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.2207 - val_loss: 90.7618\n",
      "Epoch 1780/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 47.1624 - val_loss: 94.9754\n",
      "Epoch 1781/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 45.9636 - val_loss: 90.7660\n",
      "Epoch 1782/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 45.2091 - val_loss: 91.0027\n",
      "Epoch 1783/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 44.6513 - val_loss: 91.9878\n",
      "Epoch 1784/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 44.8582 - val_loss: 89.9603\n",
      "Epoch 1785/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 45.3589 - val_loss: 93.1988\n",
      "Epoch 1786/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.2644 - val_loss: 94.2874\n",
      "Epoch 1787/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 45.9734 - val_loss: 93.5880\n",
      "Epoch 1788/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.9776 - val_loss: 92.4503\n",
      "Epoch 1789/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.0588 - val_loss: 93.9152\n",
      "Epoch 1790/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 45.6336 - val_loss: 90.2765\n",
      "Epoch 1791/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 45.1232 - val_loss: 92.5401\n",
      "Epoch 1792/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 45.1912 - val_loss: 93.8277\n",
      "Epoch 1793/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.1526 - val_loss: 91.6518\n",
      "Epoch 1794/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 44.6408 - val_loss: 91.9341\n",
      "Epoch 1795/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 45.1375 - val_loss: 94.1606\n",
      "Epoch 1796/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 45.9012 - val_loss: 91.2867\n",
      "Epoch 1797/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 45.3508 - val_loss: 93.2831\n",
      "Epoch 1798/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 45.2307 - val_loss: 91.7495\n",
      "Epoch 1799/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 44.5699 - val_loss: 91.4784\n",
      "Epoch 1800/2000\n",
      "6/6 [==============================] - ETA: 0s - loss: 44.6256\n",
      "Epoch 01800: saving model to saved_models/latent2/cp-1800.h5\n",
      "6/6 [==============================] - 1s 143ms/step - loss: 44.6256 - val_loss: 91.6328\n",
      "Epoch 1801/2000\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 43.8954 - val_loss: 90.3949\n",
      "Epoch 1802/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 44.2678 - val_loss: 93.6908\n",
      "Epoch 1803/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 44.9437 - val_loss: 90.7762\n",
      "Epoch 1804/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 45.4313 - val_loss: 92.9095\n",
      "Epoch 1805/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 44.8363 - val_loss: 90.5896\n",
      "Epoch 1806/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 44.6168 - val_loss: 90.7808\n",
      "Epoch 1807/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 45.0861 - val_loss: 92.5261\n",
      "Epoch 1808/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 45.1988 - val_loss: 92.2977\n",
      "Epoch 1809/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 45.4052 - val_loss: 91.8314\n",
      "Epoch 1810/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 45.2154 - val_loss: 93.1474\n",
      "Epoch 1811/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 45.0094 - val_loss: 91.4508\n",
      "Epoch 1812/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 44.5598 - val_loss: 91.2116\n",
      "Epoch 1813/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 44.5520 - val_loss: 91.6134\n",
      "Epoch 1814/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 44.7335 - val_loss: 92.9890\n",
      "Epoch 1815/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 44.6014 - val_loss: 91.2743\n",
      "Epoch 1816/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 44.6024 - val_loss: 93.1651\n",
      "Epoch 1817/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 44.4038 - val_loss: 92.1886\n",
      "Epoch 1818/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 45.8694 - val_loss: 92.5882\n",
      "Epoch 1819/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 44.5412 - val_loss: 92.5791\n",
      "Epoch 1820/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 44.7629 - val_loss: 91.1926\n",
      "Epoch 1821/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 45.7993 - val_loss: 93.1404\n",
      "Epoch 1822/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.3374 - val_loss: 91.5014\n",
      "Epoch 1823/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 44.8608 - val_loss: 93.0829\n",
      "Epoch 1824/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 44.9708 - val_loss: 91.0455\n",
      "Epoch 1825/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 44.6697 - val_loss: 91.2733\n",
      "Epoch 1826/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 44.8573 - val_loss: 91.3400\n",
      "Epoch 1827/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 44.6348 - val_loss: 92.4296\n",
      "Epoch 1828/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 44.6323 - val_loss: 91.0761\n",
      "Epoch 1829/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 44.5940 - val_loss: 92.5837\n",
      "Epoch 1830/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 45.4274 - val_loss: 92.3292\n",
      "Epoch 1831/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 44.8069 - val_loss: 92.5441\n",
      "Epoch 1832/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 45.2433 - val_loss: 96.7004\n",
      "Epoch 1833/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.4049 - val_loss: 92.3215\n",
      "Epoch 1834/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 46.2180 - val_loss: 91.6859\n",
      "Epoch 1835/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 45.8701 - val_loss: 92.7203\n",
      "Epoch 1836/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.1752 - val_loss: 91.9562\n",
      "Epoch 1837/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 45.8283 - val_loss: 93.3789\n",
      "Epoch 1838/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 45.4572 - val_loss: 92.3984\n",
      "Epoch 1839/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 45.3583 - val_loss: 90.2449\n",
      "Epoch 1840/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 45.2416 - val_loss: 93.4772\n",
      "Epoch 1841/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 45.0299 - val_loss: 91.9516\n",
      "Epoch 1842/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 45.3633 - val_loss: 90.5669\n",
      "Epoch 1843/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 45.4861 - val_loss: 91.1191\n",
      "Epoch 1844/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 45.1910 - val_loss: 91.1595\n",
      "Epoch 1845/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 45.1227 - val_loss: 90.8341\n",
      "Epoch 1846/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 44.8312 - val_loss: 90.8658\n",
      "Epoch 1847/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 45.0881 - val_loss: 93.4749\n",
      "Epoch 1848/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 45.8316 - val_loss: 91.3727\n",
      "Epoch 1849/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 45.6828 - val_loss: 94.7744\n",
      "Epoch 1850/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 44.9392 - val_loss: 91.7025\n",
      "Epoch 1851/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 44.3673 - val_loss: 91.5302\n",
      "Epoch 1852/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 44.0287 - val_loss: 91.9049\n",
      "Epoch 1853/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 43.8447 - val_loss: 91.4101\n",
      "Epoch 1854/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 44.2039 - val_loss: 91.4993\n",
      "Epoch 1855/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 44.8995 - val_loss: 92.5048\n",
      "Epoch 1856/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 44.7101 - val_loss: 93.8593\n",
      "Epoch 1857/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 45.2398 - val_loss: 95.5879\n",
      "Epoch 1858/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 45.9092 - val_loss: 90.5946\n",
      "Epoch 1859/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 45.6352 - val_loss: 91.5944\n",
      "Epoch 1860/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 45.3707 - val_loss: 90.4167\n",
      "Epoch 1861/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 45.4332 - val_loss: 93.4726\n",
      "Epoch 1862/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 44.7934 - val_loss: 92.2411\n",
      "Epoch 1863/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 44.1075 - val_loss: 91.1670\n",
      "Epoch 1864/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 44.5911 - val_loss: 92.9494\n",
      "Epoch 1865/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 44.3996 - val_loss: 93.0666\n",
      "Epoch 1866/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 45.0416 - val_loss: 91.3716\n",
      "Epoch 1867/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 44.8782 - val_loss: 91.3577\n",
      "Epoch 1868/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 45.1404 - val_loss: 90.5406\n",
      "Epoch 1869/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 45.1710 - val_loss: 89.9915\n",
      "Epoch 1870/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 44.5561 - val_loss: 94.1904\n",
      "Epoch 1871/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 45.4932 - val_loss: 92.5718\n",
      "Epoch 1872/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 45.4576 - val_loss: 94.0141\n",
      "Epoch 1873/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.8529 - val_loss: 92.2470\n",
      "Epoch 1874/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.9342 - val_loss: 92.9094\n",
      "Epoch 1875/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 45.7078 - val_loss: 92.8299\n",
      "Epoch 1876/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 44.9886 - val_loss: 92.8680\n",
      "Epoch 1877/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 44.6980 - val_loss: 89.7023\n",
      "Epoch 1878/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 43.8788 - val_loss: 90.9692\n",
      "Epoch 1879/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 44.1733 - val_loss: 90.5644\n",
      "Epoch 1880/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 44.2046 - val_loss: 91.4330\n",
      "Epoch 1881/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 44.1043 - val_loss: 92.0733\n",
      "Epoch 1882/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 44.8493 - val_loss: 92.1113\n",
      "Epoch 1883/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 44.4469 - val_loss: 92.1108\n",
      "Epoch 1884/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 44.8282 - val_loss: 94.5802\n",
      "Epoch 1885/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 45.6992 - val_loss: 93.6894\n",
      "Epoch 1886/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.2361 - val_loss: 94.3966\n",
      "Epoch 1887/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.6173 - val_loss: 94.7281\n",
      "Epoch 1888/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.2600 - val_loss: 94.2133\n",
      "Epoch 1889/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.6386 - val_loss: 93.2572\n",
      "Epoch 1890/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.5813 - val_loss: 91.5462\n",
      "Epoch 1891/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.4564 - val_loss: 94.9603\n",
      "Epoch 1892/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 47.1297 - val_loss: 92.1354\n",
      "Epoch 1893/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 47.6327 - val_loss: 93.2198\n",
      "Epoch 1894/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 49.5179 - val_loss: 91.8183\n",
      "Epoch 1895/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 48.1499 - val_loss: 90.7344\n",
      "Epoch 1896/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.1802 - val_loss: 93.0975\n",
      "Epoch 1897/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 45.9708 - val_loss: 91.4621\n",
      "Epoch 1898/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 45.5097 - val_loss: 92.8765\n",
      "Epoch 1899/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 44.9182 - val_loss: 90.7239\n",
      "Epoch 1900/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 44.9278 - val_loss: 92.7887\n",
      "Epoch 1901/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 44.4858 - val_loss: 89.6686\n",
      "Epoch 1902/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 44.2696 - val_loss: 89.9272\n",
      "Epoch 1903/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 44.8451 - val_loss: 91.6740\n",
      "Epoch 1904/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 44.9461 - val_loss: 91.1636\n",
      "Epoch 1905/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 44.7153 - val_loss: 93.2076\n",
      "Epoch 1906/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 44.7920 - val_loss: 91.3546\n",
      "Epoch 1907/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 44.7390 - val_loss: 91.5560\n",
      "Epoch 1908/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 45.0860 - val_loss: 91.8673\n",
      "Epoch 1909/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.1080 - val_loss: 92.8205\n",
      "Epoch 1910/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 44.7078 - val_loss: 90.9585\n",
      "Epoch 1911/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 44.7414 - val_loss: 92.8365\n",
      "Epoch 1912/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 44.2639 - val_loss: 93.7487\n",
      "Epoch 1913/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 45.1357 - val_loss: 95.9435\n",
      "Epoch 1914/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 44.9356 - val_loss: 90.9466\n",
      "Epoch 1915/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 44.4426 - val_loss: 92.9456\n",
      "Epoch 1916/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 44.0346 - val_loss: 92.4213\n",
      "Epoch 1917/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 44.3154 - val_loss: 95.5973\n",
      "Epoch 1918/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 45.3098 - val_loss: 93.4601\n",
      "Epoch 1919/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 45.6050 - val_loss: 92.0523\n",
      "Epoch 1920/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 44.8490 - val_loss: 91.5945\n",
      "Epoch 1921/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 44.5669 - val_loss: 92.0967\n",
      "Epoch 1922/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 45.6472 - val_loss: 93.3572\n",
      "Epoch 1923/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 44.4521 - val_loss: 92.7488\n",
      "Epoch 1924/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 45.6237 - val_loss: 91.0347\n",
      "Epoch 1925/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 45.0306 - val_loss: 96.2443\n",
      "Epoch 1926/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 45.5042 - val_loss: 91.7295\n",
      "Epoch 1927/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 45.5094 - val_loss: 93.6554\n",
      "Epoch 1928/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 45.1887 - val_loss: 93.6416\n",
      "Epoch 1929/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 44.6340 - val_loss: 90.6738\n",
      "Epoch 1930/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 44.4424 - val_loss: 90.2540\n",
      "Epoch 1931/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 44.9735 - val_loss: 90.6158\n",
      "Epoch 1932/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 44.7361 - val_loss: 92.8468\n",
      "Epoch 1933/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 44.3746 - val_loss: 91.6791\n",
      "Epoch 1934/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 44.1854 - val_loss: 92.3128\n",
      "Epoch 1935/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 44.2146 - val_loss: 90.3886\n",
      "Epoch 1936/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 44.1406 - val_loss: 93.1780\n",
      "Epoch 1937/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 44.6806 - val_loss: 92.6580\n",
      "Epoch 1938/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 44.9017 - val_loss: 94.7519\n",
      "Epoch 1939/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 45.5059 - val_loss: 91.1755\n",
      "Epoch 1940/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 45.5826 - val_loss: 93.2557\n",
      "Epoch 1941/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 44.8281 - val_loss: 94.3647\n",
      "Epoch 1942/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 45.2583 - val_loss: 92.4940\n",
      "Epoch 1943/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.0358 - val_loss: 94.7979\n",
      "Epoch 1944/2000\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 45.0510 - val_loss: 91.3894\n",
      "Epoch 1945/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 45.0000 - val_loss: 92.3700\n",
      "Epoch 1946/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 45.4106 - val_loss: 92.2385\n",
      "Epoch 1947/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 46.5980 - val_loss: 93.6175\n",
      "Epoch 1948/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 48.3078 - val_loss: 91.1076\n",
      "Epoch 1949/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 45.4391 - val_loss: 93.0412\n",
      "Epoch 1950/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 45.7543 - val_loss: 93.3285\n",
      "Epoch 1951/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 45.5223 - val_loss: 91.2101\n",
      "Epoch 1952/2000\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 44.6627 - val_loss: 96.9228\n",
      "Epoch 1953/2000\n",
      "6/6 [==============================] - ETA: 0s - loss: 46.6756Restoring model weights from the end of the best epoch.\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 46.6756 - val_loss: 96.8793\n",
      "Epoch 01953: early stopping\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Train VAE model with callbacks \"\"\"\n",
    "history = vae.fit(trainX, trainX, \n",
    "                  batch_size = batch_size, \n",
    "                  epochs = epochs, \n",
    "                  callbacks=[initial_checkpoint, checkpoint, EarlyStoppingAtMinLoss()],\n",
    "                  #callbacks=[initial_checkpoint, checkpoint],\n",
    "                  shuffle=True,\n",
    "                  validation_data=(valX, valX)\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step size: 6.0\n"
     ]
    }
   ],
   "source": [
    "# In this GPU implementation, it shows the number of batches/iterations for one epoch (and not the training samples), which is:\n",
    "print (\"Step size:\", np.ceil(trainX.shape[0]/batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Save the entire model \"\"\"\n",
    "# Save notes about hyperparameters used:\n",
    "with open(f'{SAVE_FOLDER}/notes.txt', 'w') as f:\n",
    "    f.write(f'Epochs: {epochs} \\n')\n",
    "    f.write(f'Batch size: {batch_size} \\n')\n",
    "    f.write(f'Latent dimension: {latent_dim} \\n')\n",
    "    f.write(f'Filter sizes: {filters} \\n')\n",
    "    f.write(f'img_width: {img_width} \\n')\n",
    "    f.write(f'img_height: {img_height} \\n')\n",
    "    f.write(f'num_channels: {num_channels}')\n",
    "    f.close()\n",
    "\n",
    "# Save weights for encoder model:\n",
    "encoder.save_weights(f'{SAVE_FOLDER}/ENCODER_WEIGHTS.h5')\n",
    "\n",
    "# Save weights for decoder model:\n",
    "decoder.save_weights(f'{SAVE_FOLDER}/DECODER_WEIGHTS.h5')\n",
    "\n",
    "# Save weights for total VAE:\n",
    "vae.save_weights(f'{SAVE_FOLDER}/VAE_WEIGHTS.h5')\n",
    "\n",
    "# Save the history at each epochs (cost of train and validation sets):\n",
    "np.save(f'{SAVE_FOLDER}/HISTORY.npy', history.history)\n",
    "\n",
    "# Save exact training, validation and testing data set (as numpy arrays):\n",
    "np.save(f'{SAVE_FOLDER}/trainX.npy', trainX)\n",
    "np.save(f'{SAVE_FOLDER}/valX.npy', valX)\n",
    "np.save(f'{SAVE_FOLDER}/testAllX.npy', testAllX)\n",
    "np.save(f'{SAVE_FOLDER}/testAllY.npy', testAllY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate training process\n",
    "\n",
    "Now that the training process is complete, lets evaluate the average loss of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 936x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set_style(\"darkgrid\")\n",
    "plt.figure(figsize=(13, 6))\n",
    "\n",
    "# summarize history for loss:\n",
    "plt.plot(history.history['loss'], color='blue', label='train')\n",
    "plt.plot(history.history['val_loss'], color='blue', alpha=0.4, label='validation')\n",
    "plt.xlabel('Epoch', fontsize=16)\n",
    "plt.ylabel('Cost', fontsize=16)\n",
    "plt.tick_params(axis='both', which='major', labelsize=14)\n",
    "plt.title('Average Loss During Training', fontsize=18)\n",
    "#plt.xticks(range(0,epochs))\n",
    "plt.legend(loc='upper right', fontsize=16)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
